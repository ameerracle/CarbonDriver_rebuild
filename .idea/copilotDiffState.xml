<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/config.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/config.py" />
              <option name="originalContent" value="&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Configuration for the carbondriver project.&#10;Simple configuration matching the original design.&#10;&quot;&quot;&quot;&#10;&#10;# Main configuration - matches the original Old_files/config.py&#10;default_config = {&#10;    &quot;num_iter&quot;: 400,&#10;    &quot;make_plots&quot;: False,&#10;    &quot;normalize&quot;: False,&#10;}&#10;&#10;# Optional: Add a few essential settings for the rebuild&#10;extended_config = {&#10;    **default_config,&#10;    &quot;data_file&quot;: &quot;Characterization_data.xlsx&quot;,&#10;    &quot;random_seed&quot;: 2,&#10;    &quot;device&quot;: &quot;cpu&quot;,&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/data/__init__.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/data/__init__.py" />
              <option name="originalContent" value="# Data loading and preprocessing module&#10;&#10;" />
              <option name="updatedContent" value="# Data loading and preprocessing module&#10;from .loader import load_data, load_raw_data, prepare_tensors, DataTensors, NormalizationParams&#10;&#10;__all__ = ['load_data', 'load_raw_data', 'prepare_tensors', 'DataTensors', 'NormalizationParams']" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/data/loader.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/data/loader.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Clean data loading and preprocessing module.&#10;Eliminates confusion between normalized/unnormalized data flows.&#10;&quot;&quot;&quot;&#10;from pathlib import Path&#10;from typing import Tuple, Optional, NamedTuple&#10;import numpy as np&#10;import pandas as pd&#10;import torch&#10;from dataclasses import dataclass&#10;&#10;&#10;@dataclass&#10;class DataTensors:&#10;    &quot;&quot;&quot;Container for processed data tensors with clear naming.&quot;&quot;&quot;&#10;    X: torch.Tensor  # Input features&#10;    y: torch.Tensor  # Target outputs&#10;    feature_names: list[str]&#10;    target_names: list[str]&#10;&#10;    def __post_init__(self):&#10;        &quot;&quot;&quot;Validate tensor shapes on creation.&quot;&quot;&quot;&#10;        assert self.X.ndim == 2, f&quot;X must be 2D, got shape {self.X.shape}&quot;&#10;        assert self.y.ndim == 2, f&quot;y must be 2D, got shape {self.y.shape}&quot;&#10;        assert self.X.shape[0] == self.y.shape[0], f&quot;Sample count mismatch: X={self.X.shape[0]}, y={self.y.shape[0]}&quot;&#10;        assert len(self.feature_names) == self.X.shape[1], f&quot;Feature name count mismatch&quot;&#10;        assert len(self.target_names) == self.y.shape[1], f&quot;Target name count mismatch&quot;&#10;&#10;&#10;@dataclass&#10;class NormalizationParams:&#10;    &quot;&quot;&quot;Container for normalization parameters.&quot;&quot;&quot;&#10;    feature_means: torch.Tensor&#10;    feature_stds: torch.Tensor&#10;    target_means: Optional[torch.Tensor] = None&#10;    target_stds: Optional[torch.Tensor] = None&#10;&#10;    def normalize_features(self, X: torch.Tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;Apply feature normalization.&quot;&quot;&quot;&#10;        return (X - self.feature_means) / self.feature_stds&#10;&#10;    def denormalize_features(self, X_norm: torch.Tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;Reverse feature normalization.&quot;&quot;&quot;&#10;        return X_norm * self.feature_stds + self.feature_means&#10;&#10;    def normalize_targets(self, y: torch.Tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;Apply target normalization if parameters exist.&quot;&quot;&quot;&#10;        if self.target_means is not None and self.target_stds is not None:&#10;            return (y - self.target_means) / self.target_stds&#10;        return y&#10;&#10;    def denormalize_targets(self, y_norm: torch.Tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;Reverse target normalization if parameters exist.&quot;&quot;&quot;&#10;        if self.target_means is not None and self.target_stds is not None:&#10;            return y_norm * self.target_stds + self.target_means&#10;        return y_norm&#10;&#10;&#10;def load_raw_data(file_path: Optional[Path] = None) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;&#10;    Load raw experimental data from Excel file.&#10;&#10;    Args:&#10;        file_path: Path to Excel file. If None, uses default location.&#10;&#10;    Returns:&#10;        Clean DataFrame with processed features and targets.&#10;    &quot;&quot;&quot;&#10;    if file_path is None:&#10;        file_path = Path('Characterization_data.xlsx')  # Fixed path - file is in project root&#10;&#10;    # Load and clean data&#10;    df = pd.read_excel(file_path, skiprows=[1], index_col=0)&#10;    df = df[['AgCu Ratio', 'Naf vol (ul)', 'Sust vol (ul)', 'Catalyst mass loading', 'FE (Eth)', 'FE (CO)']]&#10;    df = df.sort_values(by=['AgCu Ratio', 'Naf vol (ul)'])&#10;    df = df.dropna()&#10;&#10;    # Convert FE percentages to fractions&#10;    df['FE (CO)'] = df['FE (CO)'] / 100&#10;    df['FE (Eth)'] = df['FE (Eth)'] / 100&#10;&#10;    # Calculate thickness feature&#10;    dens_Ag = 10490  # kg/m^3&#10;    dens_Cu = 8935   # kg/m^3&#10;    dens_avg = (1 - df['AgCu Ratio']) * dens_Cu + df['AgCu Ratio'] * dens_Ag&#10;    mass = df['Catalyst mass loading'] * 1e-6  # kg&#10;    area = 1.85**2  # cm^2&#10;    A = area * 1e-4  # m^2&#10;    thickness = (mass / dens_avg) / A  # m&#10;    df.insert(3, column='Zero_eps_thickness', value=thickness)&#10;&#10;    # Reshuffle triplets (reproducible randomization)&#10;    df['triplet'] = np.arange(len(df)) // 3&#10;    gen = np.random.default_rng(2)  # Fixed seed for reproducibility&#10;    order = gen.permutation(30)&#10;    new_df = pd.DataFrame()&#10;    for i in order:&#10;        new_df = pd.concat([new_df, df[df['triplet'] == i]])&#10;    new_df.reset_index(drop=True, inplace=True)&#10;    new_df = new_df.drop(columns=['triplet'])&#10;&#10;    return new_df&#10;&#10;&#10;def prepare_tensors(df: pd.DataFrame, normalize_features: bool = True,&#10;                   normalize_targets: bool = False) -&gt; Tuple[DataTensors, Optional[NormalizationParams]]:&#10;    &quot;&quot;&quot;&#10;    Convert DataFrame to tensors with optional normalization.&#10;&#10;    Args:&#10;        df: Input DataFrame with features and targets&#10;        normalize_features: Whether to normalize input features&#10;        normalize_targets: Whether to normalize target outputs&#10;&#10;    Returns:&#10;        DataTensors: Container with X, y tensors and metadata&#10;        NormalizationParams: Normalization parameters (None if no normalization)&#10;    &quot;&quot;&quot;&#10;    # Split features and targets&#10;    feature_cols = df.columns[:-2].tolist()  # All except last 2 columns&#10;    target_cols = df.columns[-2:].tolist()   # Last 2 columns (FE outputs)&#10;&#10;    X = torch.tensor(df[feature_cols].values, dtype=torch.float32)&#10;    y = torch.tensor(df[target_cols].values, dtype=torch.float32)&#10;&#10;    # Validate shapes&#10;    assert X.shape[0] &gt; 0, &quot;No data samples found&quot;&#10;    assert X.shape[1] == len(feature_cols), &quot;Feature dimension mismatch&quot;&#10;    assert y.shape[1] == len(target_cols), &quot;Target dimension mismatch&quot;&#10;&#10;    norm_params = None&#10;&#10;    if normalize_features or normalize_targets:&#10;        # Calculate normalization parameters&#10;        feature_means = X.mean(dim=0)&#10;        feature_stds = X.std(dim=0, unbiased=False)&#10;&#10;        # Avoid division by zero&#10;        feature_stds = torch.where(feature_stds == 0, torch.ones_like(feature_stds), feature_stds)&#10;&#10;        target_means = y.mean(dim=0) if normalize_targets else None&#10;        target_stds = y.std(dim=0, unbiased=False) if normalize_targets else None&#10;        if normalize_targets:&#10;            target_stds = torch.where(target_stds == 0, torch.ones_like(target_stds), target_stds)&#10;&#10;        norm_params = NormalizationParams(&#10;            feature_means=feature_means,&#10;            feature_stds=feature_stds,&#10;            target_means=target_means,&#10;            target_stds=target_stds&#10;        )&#10;&#10;        # Apply normalization&#10;        if normalize_features:&#10;            X = norm_params.normalize_features(X)&#10;        if normalize_targets:&#10;            y = norm_params.normalize_targets(y)&#10;&#10;    data_tensors = DataTensors(&#10;        X=X,&#10;        y=y,&#10;        feature_names=feature_cols,&#10;        target_names=target_cols&#10;    )&#10;&#10;    return data_tensors, norm_params&#10;&#10;&#10;def load_data(file_path: Optional[Path] = None, normalize_features: bool = True,&#10;              normalize_targets: bool = False) -&gt; Tuple[DataTensors, Optional[NormalizationParams], pd.DataFrame]:&#10;    &quot;&quot;&quot;&#10;    Complete data loading pipeline.&#10;&#10;    Args:&#10;        file_path: Path to Excel file&#10;        normalize_features: Whether to normalize input features&#10;        normalize_targets: Whether to normalize target outputs&#10;&#10;    Returns:&#10;        DataTensors: Processed tensor data&#10;        NormalizationParams: Normalization parameters (None if no normalization)&#10;        DataFrame: Original raw DataFrame for reference&#10;    &quot;&quot;&quot;&#10;    df = load_raw_data(file_path)&#10;    data_tensors, norm_params = prepare_tensors(df, normalize_features, normalize_targets)&#10;&#10;    return data_tensors, norm_params, df&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Clean data loading and preprocessing module.&#10;Eliminates confusion between normalized/unnormalized data flows.&#10;&quot;&quot;&quot;&#10;from pathlib import Path&#10;from typing import Tuple, Optional, NamedTuple&#10;import numpy as np&#10;import pandas as pd&#10;import torch&#10;from dataclasses import dataclass&#10;&#10;&#10;@dataclass&#10;class DataTensors:&#10;    &quot;&quot;&quot;Container for processed data tensors with clear naming.&quot;&quot;&quot;&#10;    X: torch.Tensor  # Input features&#10;    y: torch.Tensor  # Target outputs&#10;    feature_names: list[str]&#10;    target_names: list[str]&#10;&#10;    def __post_init__(self):&#10;        &quot;&quot;&quot;Validate tensor shapes on creation.&quot;&quot;&quot;&#10;        assert self.X.ndim == 2, f&quot;X must be 2D, got shape {self.X.shape}&quot;&#10;        assert self.y.ndim == 2, f&quot;y must be 2D, got shape {self.y.shape}&quot;&#10;        assert self.X.shape[0] == self.y.shape[0], f&quot;Sample count mismatch: X={self.X.shape[0]}, y={self.y.shape[0]}&quot;&#10;        assert len(self.feature_names) == self.X.shape[1], f&quot;Feature name count mismatch&quot;&#10;        assert len(self.target_names) == self.y.shape[1], f&quot;Target name count mismatch&quot;&#10;&#10;&#10;@dataclass&#10;class NormalizationParams:&#10;    &quot;&quot;&quot;Container for normalization parameters.&quot;&quot;&quot;&#10;    feature_means: torch.Tensor&#10;    feature_stds: torch.Tensor&#10;    target_means: Optional[torch.Tensor] = None&#10;    target_stds: Optional[torch.Tensor] = None&#10;&#10;    def normalize_features(self, X: torch.Tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;Apply feature normalization.&quot;&quot;&quot;&#10;        return (X - self.feature_means) / self.feature_stds&#10;&#10;    def denormalize_features(self, X_norm: torch.Tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;Reverse feature normalization.&quot;&quot;&quot;&#10;        return X_norm * self.feature_stds + self.feature_means&#10;&#10;    def normalize_targets(self, y: torch.Tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;Apply target normalization if parameters exist.&quot;&quot;&quot;&#10;        if self.target_means is not None and self.target_stds is not None:&#10;            return (y - self.target_means) / self.target_stds&#10;        return y&#10;&#10;    def denormalize_targets(self, y_norm: torch.Tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;Reverse target normalization if parameters exist.&quot;&quot;&quot;&#10;        if self.target_means is not None and self.target_stds is not None:&#10;            return y_norm * self.target_stds + self.target_means&#10;        return y_norm&#10;&#10;&#10;def load_raw_data(file_path: Optional[Path] = None) -&gt; pd.DataFrame:&#10;    &quot;&quot;&quot;&#10;    Load raw experimental data from Excel file.&#10;&#10;    Args:&#10;        file_path: Path to Excel file. If None, uses default location.&#10;&#10;    Returns:&#10;        Clean DataFrame with processed features and targets.&#10;    &quot;&quot;&quot;&#10;    if file_path is None:&#10;        file_path = Path('Characterization_data.xlsx')  # Fixed path - file is in project root&#10;&#10;    # Load and clean data&#10;    df = pd.read_excel(file_path, skiprows=[1], index_col=0)&#10;    df = df[['AgCu Ratio', 'Naf vol (ul)', 'Sust vol (ul)', 'Catalyst mass loading', 'FE (Eth)', 'FE (CO)']]&#10;    df = df.sort_values(by=['AgCu Ratio', 'Naf vol (ul)'])&#10;    df = df.dropna()&#10;&#10;    # Convert FE percentages to fractions&#10;    df['FE (CO)'] = df['FE (CO)'] / 100&#10;    df['FE (Eth)'] = df['FE (Eth)'] / 100&#10;&#10;    # Calculate thickness feature&#10;    dens_Ag = 10490  # kg/m^3&#10;    dens_Cu = 8935   # kg/m^3&#10;    dens_avg = (1 - df['AgCu Ratio']) * dens_Cu + df['AgCu Ratio'] * dens_Ag&#10;    mass = df['Catalyst mass loading'] * 1e-6  # kg&#10;    area = 1.85**2  # cm^2&#10;    A = area * 1e-4  # m^2&#10;    thickness = (mass / dens_avg) / A  # m&#10;    df.insert(3, column='Zero_eps_thickness', value=thickness)&#10;&#10;    # Reshuffle triplets (reproducible randomization)&#10;    df['triplet'] = np.arange(len(df)) // 3&#10;    gen = np.random.default_rng(2)  # Fixed seed for reproducibility&#10;    order = gen.permutation(30)&#10;    new_df = pd.DataFrame()&#10;    for i in order:&#10;        new_df = pd.concat([new_df, df[df['triplet'] == i]])&#10;    new_df.reset_index(drop=True, inplace=True)&#10;    new_df = new_df.drop(columns=['triplet'])&#10;&#10;    return new_df&#10;&#10;&#10;def prepare_tensors(df: pd.DataFrame, normalize_features: bool = True,&#10;                   normalize_targets: bool = False) -&gt; Tuple[DataTensors, Optional[NormalizationParams]]:&#10;    &quot;&quot;&quot;&#10;    Convert DataFrame to tensors with optional normalization.&#10;&#10;    Args:&#10;        df: Input DataFrame with features and targets&#10;        normalize_features: Whether to normalize input features&#10;        normalize_targets: Whether to normalize target outputs&#10;&#10;    Returns:&#10;        DataTensors: Container with X, y tensors and metadata&#10;        NormalizationParams: Normalization parameters (None if no normalization)&#10;    &quot;&quot;&quot;&#10;    # Split features and targets&#10;    feature_cols = df.columns[:-2].tolist()  # All except last 2 columns&#10;    target_cols = df.columns[-2:].tolist()   # Last 2 columns (FE outputs)&#10;&#10;    X = torch.tensor(df[feature_cols].values, dtype=torch.float32)&#10;    y = torch.tensor(df[target_cols].values, dtype=torch.float32)&#10;&#10;    # Validate shapes&#10;    assert X.shape[0] &gt; 0, &quot;No data samples found&quot;&#10;    assert X.shape[1] == len(feature_cols), &quot;Feature dimension mismatch&quot;&#10;    assert y.shape[1] == len(target_cols), &quot;Target dimension mismatch&quot;&#10;&#10;    norm_params = None&#10;&#10;    if normalize_features or normalize_targets:&#10;        # Calculate normalization parameters&#10;        feature_means = X.mean(dim=0)&#10;        feature_stds = X.std(dim=0, unbiased=False)&#10;&#10;        # Avoid division by zero&#10;        feature_stds = torch.where(feature_stds == 0, torch.ones_like(feature_stds), feature_stds)&#10;&#10;        target_means = y.mean(dim=0) if normalize_targets else None&#10;        target_stds = y.std(dim=0, unbiased=False) if normalize_targets else None&#10;        if normalize_targets:&#10;            target_stds = torch.where(target_stds == 0, torch.ones_like(target_stds), target_stds)&#10;&#10;        norm_params = NormalizationParams(&#10;            feature_means=feature_means,&#10;            feature_stds=feature_stds,&#10;            target_means=target_means,&#10;            target_stds=target_stds&#10;        )&#10;&#10;        # Apply normalization&#10;        if normalize_features:&#10;            X = norm_params.normalize_features(X)&#10;        if normalize_targets:&#10;            y = norm_params.normalize_targets(y)&#10;&#10;    data_tensors = DataTensors(&#10;        X=X,&#10;        y=y,&#10;        feature_names=feature_cols,&#10;        target_names=target_cols&#10;    )&#10;&#10;    return data_tensors, norm_params&#10;&#10;&#10;def load_data(file_path: Optional[Path] = None, normalize_features: bool = True,&#10;              normalize_targets: bool = False) -&gt; Tuple[DataTensors, Optional[NormalizationParams], pd.DataFrame]:&#10;    &quot;&quot;&quot;&#10;    Complete data loading pipeline.&#10;    &#10;    Args:&#10;        file_path: Path to Excel file&#10;        normalize_features: Whether to normalize input features (default True, matches old behavior)&#10;        normalize_targets: Whether to normalize target outputs (default False, targets stay as 0-1 decimals)&#10;        &#10;    Returns:&#10;        DataTensors: Processed tensor data&#10;        NormalizationParams: Normalization parameters (None if no normalization)&#10;        DataFrame: Original raw DataFrame for reference&#10;        &#10;    Note:&#10;        This matches the original behavior where:&#10;        - Input features are normalized (mean=0, std=1) &#10;        - Output targets remain as decimal values (0-1) from percentage conversion&#10;    &quot;&quot;&quot;&#10;    df = load_raw_data(file_path)&#10;    data_tensors, norm_params = prepare_tensors(df, normalize_features, normalize_targets)&#10;&#10;    return data_tensors, norm_params, df&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/models/__init__.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/models/__init__.py" />
              <option name="originalContent" value="# Models package for carbondriver&#10;from .mlp_ensemble import MLPModel, MLPEnsemble, EnsembleConfig&#10;from .physics_model import PhModel, PhysicsConfig&#10;from .gde_system import System&#10;&#10;__all__ = ['MLPModel', 'MLPEnsemble', 'EnsembleConfig', 'PhModel', 'PhysicsConfig', 'System']&#10;" />
              <option name="updatedContent" value="# Models package for carbondriver&#10;from .mlp_ensemble import MLPModel, MLPEnsemble, EnsembleConfig&#10;from .physics_model import PhModel, PhysicsConfig&#10;from .ph_ensemble import PhModelEnsemble, PhEnsembleConfig&#10;from .gde_system import System&#10;&#10;__all__ = ['MLPModel', 'MLPEnsemble', 'EnsembleConfig', 'PhModel', 'PhysicsConfig', 'PhModelEnsemble', 'PhEnsembleConfig', 'System']" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/models/gde_system.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/models/gde_system.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Complete physics engine for electrochemical CO2 reduction simulations.&#10;Based on the original gde_multi.py with full electrochemical modeling.&#10;&quot;&quot;&quot;&#10;from dataclasses import dataclass, field&#10;from typing import Dict, Literal, Tuple, Union, Optional&#10;from types import MappingProxyType&#10;from functools import cached_property&#10;import numpy as np&#10;import scipy.optimize as opt&#10;import torch&#10;import torch.nn.functional&#10;&#10;&#10;# Physics constants from original gde_multi.py&#10;diffusion_coefficients = MappingProxyType({&#10;    'CO2': 1.91e-09,&#10;    'OH': 5.293e-09,&#10;    'CO3': 9.23e-10,&#10;    'HCO3': 1.18e-09,&#10;    'H': 9.311e-09,&#10;    'K': 1.96e-09,&#10;    'CO': 2.03e-09,&#10;    'H2': 4.5e-09&#10;})  # [m^2/s]&#10;&#10;salting_out_exponents = MappingProxyType({&#10;    'h_OH': 6.67e-05,&#10;    'h_CO3': 0.0001251,&#10;    'h_HCO3': 7.95e-05,&#10;    'h_K': 7.5e-05,&#10;    'h_CO2': 0.0&#10;})  # [m^3/mol]&#10;&#10;electrode_reaction_kinetics = MappingProxyType({&#10;    'i_0_CO': 0.00471,    # [A/m^2]&#10;    'i_0_C2H4': 1e-5,     # [A/m^2]&#10;    'i_0_H2a': 0.00979,   # [A/m^2] not used in the calculation&#10;    'i_0_H2b': 1.16e-05,  # [A/m^2] &#10;    'alpha_CO': 0.44,&#10;    'alpha_C2H4': 0.4, &#10;    'alpha_H2a': 0.27,&#10;    'alpha_H2b': 0.36&#10;})&#10;&#10;electrode_reaction_potentials = MappingProxyType({&#10;    'E_0_CO': -0.11,  # [V]&#10;    'E_0_C2H4': 0.09, # [V]&#10;    'E_0_H2a': 0.0,   # [V]&#10;    'E_0_H2b': 0.0    # [V]&#10;})&#10;&#10;chemical_reaction_rates = MappingProxyType({&#10;    'k1f': 2.23,      # [m^3/(mol s)]&#10;    'c_ref': 1000.0   # [mol/m^3]&#10;})&#10;&#10;&#10;class ModelData(torch.nn.Module):&#10;    &quot;&quot;&quot;Container for model parameters that can be either learnable or fixed.&quot;&quot;&quot;&#10;    def __init__(self, **kwargs):&#10;        super().__init__()&#10;        toprint = []&#10;        for k, v in kwargs.items():&#10;            toprint.append(k)&#10;            if isinstance(v, torch.nn.parameter.Parameter):&#10;                self.register_parameter(k, v)&#10;            else:&#10;                self.register_buffer(k, torch.tensor(v))&#10;        self.toprint = toprint&#10;&#10;    def __repr__(self):&#10;        return &quot;ModelData(&quot; + &quot;, &quot;.join([f&quot;{k}={getattr(self, k)}&quot; for k in self.toprint]) + &quot;)&quot;&#10;    &#10;    def __getitem__(self, key):&#10;        return getattr(self, key)&#10;&#10;&#10;class PositiveModelData(torch.nn.Module):&#10;    &quot;&quot;&quot;Container for model parameters that must be positive (uses ReLU).&quot;&quot;&quot;&#10;    def __init__(self, **kwargs):&#10;        super().__init__()&#10;        toprint = []&#10;        for k, v in kwargs.items():&#10;            toprint.append(k)&#10;            if isinstance(v, torch.nn.parameter.Parameter):&#10;                self.register_parameter(k, v)&#10;            else:&#10;                self.register_buffer(k, torch.tensor(v))&#10;        self.toprint = toprint         &#10;&#10;    def __repr__(self):&#10;        return &quot;PositiveModelData(&quot; + &quot;, &quot;.join([f&quot;{k}={getattr(self, k)}&quot; for k in self.toprint]) + &quot;)&quot;&#10;&#10;    def __getitem__(self, key):&#10;        return torch.nn.functional.relu(getattr(self, key))&#10;&#10;&#10;class System(torch.nn.Module):&#10;    &quot;&quot;&quot;&#10;    Complete electrochemical system for CO2 reduction simulation.&#10;    &#10;    This is the full physics engine from the original gde_multi.py that solves:&#10;    - Mass transport equations with salting-out effects&#10;    - Electrochemical kinetics (Butler-Volmer equations)&#10;    - Carbonate equilibrium chemistry&#10;    - Flow channel characteristics&#10;    &quot;&quot;&quot;&#10;    &#10;    R: float = 8.3145   # Universal gas constant [J/(mol*K)]&#10;    F: float = 96485    # Faraday constant [C/mol]&#10;    TOL: float = 1e-36  # Numerical tolerance&#10;    MAX_ITER: int = 50  # Maximum iterations for convergence&#10;    &#10;    def __init__(&#10;        self, &#10;        T: float = 298.15,              # Temperature [K]&#10;        p0: float = 2.38,               # CO2 pressure [bar]&#10;        Q: float = 30,                  # Flow rate [cm^3/min]&#10;        flow_chan_width: float = 1.5e-3,     # Flow channel width [m]&#10;        flow_chan_length: float = 0.02,      # Flow channel length [m]&#10;        flow_chan_height: float = 0.005,     # Flow channel height [m]&#10;        c_khco3: float = 500,           # KHCO3 concentration [mol/l]&#10;        c_k: float = 500,               # K+ concentration [mol/l]&#10;        dic: float = 10**(-3.408),      # Dissolved inorganic carbon&#10;        diffusion_coefficients: Optional[Dict[str, float]] = None,&#10;        salting_out_exponents: Optional[Dict[str, float]] = None,&#10;        electrode_reaction_kinetics: Optional[Dict[str, float]] = None,&#10;        electrode_reaction_potentials: Optional[Dict[str, float]] = None,&#10;        chemical_reaction_rates: Optional[Dict[str, float]] = None,&#10;        co2_equilibrium: Optional[Dict[str, float]] = None,&#10;        method: Literal['DIC', 'CO2 eql'] = 'CO2 eql',&#10;    ):&#10;        super().__init__()&#10;        self.T = T&#10;        self.p0 = p0&#10;        self.Q = Q&#10;        self.flow_chan_width = flow_chan_width&#10;        self.flow_chan_length = flow_chan_length&#10;        self.flow_chan_height = flow_chan_height&#10;        self.c_khco3 = c_khco3&#10;        self.c_k = c_k&#10;        self.dic = dic&#10;        self.method = method&#10;&#10;        # Initialize parameter containers&#10;        if diffusion_coefficients is not None:&#10;            self.diffusion_coefficients = PositiveModelData(**diffusion_coefficients)&#10;        if salting_out_exponents is not None:&#10;            self.salting_out_exponents = PositiveModelData(**salting_out_exponents)&#10;        if electrode_reaction_kinetics is not None:&#10;            self.electrode_reaction_kinetics = PositiveModelData(**electrode_reaction_kinetics)&#10;        if electrode_reaction_potentials is not None:&#10;            self.electrode_reaction_potentials = ModelData(**electrode_reaction_potentials)&#10;        if chemical_reaction_rates is not None:&#10;            self.chemical_reaction_rates = PositiveModelData(**chemical_reaction_rates)&#10;        &#10;        self.set_initial_carbonate_equilibria()&#10;        &#10;        if co2_equilibrium is not None:&#10;            self.co2_equilibrium = co2_equilibrium&#10;        else:&#10;            self.co2_equilibrium = self._co2_equilibrium()&#10;&#10;    @property&#10;    def Hnr(self) -&gt; float:&#10;        &quot;&quot;&quot;Henry's constant for CO2 [mol/(L*atm)]&quot;&quot;&quot;&#10;        T = self.T&#10;        return 1000*np.exp(93.4517*100/T - 60.2409 + 23.3585*np.log(T/100))&#10;    &#10;    @cached_property&#10;    def v(self):&#10;        &quot;&quot;&quot;Flow velocity [m/s]&quot;&quot;&quot;&#10;        return self.Q / (self.flow_chan_height * self.flow_chan_width) * 1e-6 / 60&#10;    &#10;    def volumetric_surface_area(self, eps, r):&#10;        &quot;&quot;&quot;Volumetric surface area [m^-1]&quot;&quot;&quot;&#10;        return 3 * (1 - eps) / r&#10;&#10;    @staticmethod&#10;    def bruggeman(dif_coef, eps):&#10;        &quot;&quot;&quot;Bruggeman correction for diffusion coefficient [m^2/s]&quot;&quot;&quot;&#10;        return dif_coef * eps**1.5&#10;&#10;    @cached_property&#10;    def butler_volmer_factor(self):&#10;        &quot;&quot;&quot;Butler-Volmer factor F/(RT)&quot;&quot;&quot;&#10;        return self.F / (self.R * self.T)&#10;    &#10;    @cached_property&#10;    def flow_channel_characteristics(self):&#10;        &quot;&quot;&quot;Calculate flow channel mass transfer characteristics&quot;&quot;&quot;&#10;        flow_channel_characteristics = {&#10;            'mu': 0.00000093944,  # Kinematic viscosity [m^2/s]&#10;        }&#10;        flow_channel_characteristics['Reynolds Number'] = self.v*self.flow_chan_length/flow_channel_characteristics['mu']&#10;        flow_channel_characteristics['Hydrodynamic Entrance length'] = 0.0099*flow_channel_characteristics['Reynolds Number']*self.flow_chan_width&#10;        flow_channel_characteristics['Parallel plate effective boundary layer'] = 13/35*self.flow_chan_width&#10;        &#10;        def diff_coeff_to_bl_thickness(x):&#10;            return 3*1.607/4*(self.flow_chan_width*x*self.flow_chan_length/self.v)**(1/3)&#10;        &#10;        def diff_coeff_to_K_L(D, L):&#10;            return D*torch.sqrt(L**-2+flow_channel_characteristics['Parallel plate effective boundary layer']**-2)/np.sqrt(2)&#10;        &#10;        for substance in ['CO2', 'OH', 'CO3', 'HCO3', 'H', 'K', 'CO', 'H2']:&#10;            flow_channel_characteristics[f'Developing boundary layer thickness {substance} (average)'] = diff_coeff_to_bl_thickness(getattr(self.diffusion_coefficients, substance))&#10;        &#10;        for substance in ['CO2', 'OH', 'CO3', 'HCO3', 'H', 'K', 'CO', 'H2']:&#10;            flow_channel_characteristics[f'K_L_{substance}'] = diff_coeff_to_K_L(getattr(self.diffusion_coefficients, substance), flow_channel_characteristics[f'Developing boundary layer thickness {substance} (average)'])&#10;        &#10;        return flow_channel_characteristics&#10;&#10;    def set_initial_carbonate_equilibria(self):&#10;        &quot;&quot;&quot;Calculate carbonate equilibrium constants&quot;&quot;&quot;&#10;        T = self.T&#10;        kco3_to_salinity = lambda x: x/(1.005*x/1000+19.924)&#10;        &#10;        initial_carbonate_equilibria = {&#10;            'Salinity': kco3_to_salinity(self.c_khco3),&#10;            'pK1_0': -126.34048 + 6320.813/T + 19.568224*np.log(T),&#10;            'pK2_0': -90.18333 + 5143.692/T + 14.613358*np.log(T),&#10;        }&#10;        &#10;        salinity_to_A1 = lambda x: 13.4191*x**0.5 + 0.0331*x - 5.33e-5*x**2&#10;        initial_carbonate_equilibria['A1'] = salinity_to_A1(initial_carbonate_equilibria['Salinity'])&#10;        salinity_to_A2 = lambda x: 21.0894*x**0.5 + 0.1248*x - 3.687e-4*x**2&#10;        initial_carbonate_equilibria['A2'] = salinity_to_A2(initial_carbonate_equilibria['Salinity'])&#10;        salinity_to_B1 = lambda x: -530.123*x**0.5 - 6.103*x&#10;        initial_carbonate_equilibria['B1'] = salinity_to_B1(initial_carbonate_equilibria['Salinity'])&#10;        salinity_to_B2 = lambda x: -772.483*x**0.5 - 20.051*x&#10;        initial_carbonate_equilibria['B2'] = salinity_to_B2(initial_carbonate_equilibria['Salinity'])&#10;        initial_carbonate_equilibria['C1'] = -2.0695*initial_carbonate_equilibria['Salinity']**0.5&#10;        initial_carbonate_equilibria['C2'] = -3.3336*initial_carbonate_equilibria['Salinity']**0.5&#10;        initial_carbonate_equilibria['pK1'] = initial_carbonate_equilibria['pK1_0'] + initial_carbonate_equilibria['A1'] + initial_carbonate_equilibria['B1']/T + initial_carbonate_equilibria['C1']*np.log(T)&#10;        initial_carbonate_equilibria['pK2'] = initial_carbonate_equilibria['pK2_0'] + initial_carbonate_equilibria['A2'] + initial_carbonate_equilibria['B2']/T + initial_carbonate_equilibria['C2']*np.log(T)&#10;        initial_carbonate_equilibria['K1'] = 10**-initial_carbonate_equilibria['pK1']&#10;        initial_carbonate_equilibria['K2'] = 10**-initial_carbonate_equilibria['pK2']&#10;        initial_carbonate_equilibria['Kw'] = np.exp(148.96502 - 13847.26/T - 23.6521*np.log(T) + (-5.977 + 118.67/T + 1.0495*np.log(T))*initial_carbonate_equilibria['Salinity']**0.5 - 0.01615*initial_carbonate_equilibria['Salinity'])&#10;&#10;        self.initial_carbonate_equilibria = initial_carbonate_equilibria&#10;&#10;    def _co2_equilibrium(self):&#10;        &quot;&quot;&quot;Calculate CO2 equilibrium concentrations&quot;&quot;&quot;&#10;        ice = self.initial_carbonate_equilibria&#10;&#10;        co2_init = self.Hnr*self.dic&#10;        a = 1&#10;        b = self.c_khco3/1000&#10;        c = lambda co2: -(ice['Kw'] + ice['K1']*co2/1000)&#10;        d = lambda co2: -2*co2/1000*ice['K1']*ice['K2']&#10;        f_xn = lambda x, a,b,c,d: a*x**3 + b*x**2 + c*x + d&#10;        df_xn = lambda x, a,b,c,d: 3*a*x**2 + 2*b*x + c &#10;        xn_to_co2 = lambda xn, co2: self.Hnr*self.dic*torch.exp(&#10;            -self.c_k*self.salting_out_exponents['h_K'] -&#10;            self.salting_out_exponents['h_HCO3']*co2*ice['K1']/xn -&#10;            co2*self.salting_out_exponents['h_CO3']*ice['K1']*ice['K2']/xn**2 -&#10;            self.salting_out_exponents['h_OH']*ice['Kw']/xn*1000&#10;        )&#10;        &#10;        # Find root of f(x) = 0&#10;        co2 = co2_init&#10;        for i in range(3):&#10;            x_n = opt.newton(f_xn, x0=1e-5, fprime=df_xn, tol=self.TOL, args=(a,b,c(co2),d(co2)), maxiter=self.MAX_ITER)&#10;            co2 = xn_to_co2(x_n, co2)&#10;&#10;        co2_equilibrium_sol = {&#10;            'final_pH': -torch.log10(x_n),&#10;            'CO2': co2,&#10;            'HCO3': co2*ice['K1']/x_n,&#10;            'OH': ice['Kw']/x_n * 1000,&#10;            'K': self.c_k,&#10;        }&#10;        co2_equilibrium_sol['CO3'] = co2_equilibrium_sol['HCO3']*ice['K2']/x_n&#10;        &#10;        return co2_equilibrium_sol&#10;&#10;    def solve_current(&#10;        self,&#10;        i_target: Union[float, torch.Tensor],&#10;        eps: torch.Tensor,&#10;        r: torch.Tensor,&#10;        L: torch.Tensor,&#10;        thetas: Dict[str, torch.Tensor],&#10;        gdl_mass_transfer_coeff: torch.Tensor,&#10;        voltage_bounds: Tuple = (-1.25, 0),&#10;        grid_size: int = 1000&#10;    ):&#10;        &quot;&quot;&quot;&#10;        Solve for Faradaic efficiencies at target current density.&#10;        &#10;        This is the main physics solver that:&#10;        1. Creates a voltage grid and solves the full electrochemical equations&#10;        2. Interpolates to find the solution at the target current density&#10;        3. Returns FE values and other physics outputs&#10;        &#10;        Args:&#10;            i_target: Target current density [A/m^2]&#10;            eps: Porosity (dimensionless)&#10;            r: Pore radius [m]&#10;            L: Layer thickness [m]&#10;            thetas: Surface coverage fractions {'CO': ..., 'C2H4': ..., 'H2b': ...}&#10;            gdl_mass_transfer_coeff: Mass transfer coefficient [m/s]&#10;            voltage_bounds: Voltage range for search [V]&#10;            grid_size: Number of voltage points for interpolation&#10;            &#10;        Returns:&#10;            Dictionary with 'fe_co', 'fe_c2h4' and other physics variables&#10;        &quot;&quot;&quot;&#10;        if not isinstance(i_target, torch.Tensor):&#10;            i_target = torch.ones_like(eps) * i_target&#10;&#10;        # Create voltage grid (monotonically decreasing)&#10;        phi = torch.linspace(*sorted(voltage_bounds, reverse=True), grid_size).reshape(1, -1)&#10;        &#10;        # Solve full electrochemical equations&#10;        solution = self.solve(phi, eps, r, L, thetas, gdl_mass_transfer_coeff)&#10;        &#10;        # Extract current density (monotonically increasing)&#10;        I = solution['current_density'].detach()&#10;&#10;        i_target = i_target.reshape(-1, 1)&#10;&#10;        # Interpolate to find solution at target current&#10;        idx = torch.searchsorted(I, i_target, side='right') - 1&#10;        idx = torch.clamp(idx, min=0, max=I.shape[1]-2)&#10;        &#10;        curr_left = I.gather(dim=1, index=idx)&#10;        curr_right = I.gather(dim=1, index=idx+1)&#10;        denom = curr_right - curr_left&#10;        epsilon = 1e-10&#10;        denom_safe = torch.where(torch.abs(denom) &lt; epsilon, torch.full_like(denom, fill_value=epsilon), other=denom)&#10;        p = (i_target - curr_left) / denom_safe&#10;&#10;        # Interpolate all solution variables&#10;        out = {}&#10;        for k, v in solution.items():&#10;            if v.shape[0] == 1:&#10;                v = v * torch.ones_like(eps)&#10;            out[k] = (1-p)*v.gather(dim=1, index=idx) + p*v.gather(dim=1, index=idx+1)&#10;            &#10;        return out&#10;&#10;    def solve(&#10;        self, &#10;        phi_ext: torch.Tensor,&#10;        eps: torch.Tensor,&#10;        r: torch.Tensor,&#10;        L: torch.Tensor,&#10;        thetas: Dict[str, torch.Tensor],&#10;        gdl_mass_transfer_coeff: torch.Tensor,&#10;    ):&#10;        &quot;&quot;&quot;&#10;        Solve the complete electrochemical system for given voltage.&#10;        &#10;        This method solves the coupled system of:&#10;        - Mass transport with Bruggeman correction&#10;        - Butler-Volmer electrochemical kinetics  &#10;        - Carbonate equilibrium chemistry&#10;        - Salting-out effects&#10;        &quot;&quot;&quot;&#10;        # Calculate volumetric surface area&#10;        A = self.volumetric_surface_area(eps, r)&#10;        &#10;        # Constants&#10;        F = self.F&#10;        R = self.R&#10;        T = self.T&#10;        Hnr = self.Hnr&#10;        &#10;        # Salting-out corrected Henry's constant&#10;        Hnr_c = lambda c1, c2: Hnr*torch.exp(-self.salting_out_exponents['h_OH']*c1 - self.salting_out_exponents['h_CO3']*c2 - self.salting_out_exponents['h_K']*(c1+2*c2))&#10;        &#10;        # Effective diffusion coefficient&#10;        DCO2 = self.bruggeman(self.diffusion_coefficients['CO2'], eps)&#10;        &#10;        # Electrode potentials&#10;        E_CO = self.electrode_reaction_potentials['E_0_CO']&#10;        &#10;        # Bulk electrolyte concentrations&#10;        co2_equilibrium = self.co2_equilibrium&#10;        OH_neg = co2_equilibrium['OH']&#10;        HCO3_neg = co2_equilibrium['HCO3']&#10;        CO3_2neg = co2_equilibrium['CO3']&#10;&#10;        # Overpotentials&#10;        overpotential_CO = phi_ext - E_CO&#10;        overpotential_C2H4 = phi_ext - self.electrode_reaction_potentials['E_0_C2H4']&#10;        &#10;        # Thiele modulus helper&#10;        M = lambda k: torch.sqrt(k*L**2/DCO2)&#10;&#10;        # Reaction rate constants (Butler-Volmer kinetics)&#10;        k0_CO = A/(2*F) * self.electrode_reaction_kinetics['i_0_CO'] * thetas['CO']/self.chemical_reaction_rates['c_ref'] * torch.exp(&#10;            -overpotential_CO * self.butler_volmer_factor*self.electrode_reaction_kinetics['alpha_CO'])&#10;        k0_C2H4 = A/(6*F) * self.electrode_reaction_kinetics['i_0_C2H4'] * thetas['C2H4']/self.chemical_reaction_rates['c_ref'] * torch.exp(&#10;            -overpotential_C2H4 * self.butler_volmer_factor*self.electrode_reaction_kinetics['alpha_C2H4'])&#10;        k0 = k0_CO + k0_C2H4&#10;        &#10;        # Initial effectiveness factor and CO2 concentration&#10;        eff_0 = 1/(M(k0)/torch.tanh(M(k0)) + k0*L/gdl_mass_transfer_coeff)&#10;        c00 = Hnr*self.p0*eff_0&#10;&#10;        # Hydrogen evolution reaction rate&#10;        r_H2 = A*self.electrode_reaction_kinetics['i_0_H2b'] * thetas['H2b']/F * torch.exp(-phi_ext*self.butler_volmer_factor*self.electrode_reaction_kinetics['alpha_H2b'])&#10;        &#10;        # H2 consumption rate due to CO2 reduction&#10;        r_H2_CO2 = (2*k0_CO + 6*k0_C2H4)*c00&#10;        &#10;        # Initial OH- concentration estimate&#10;        c10 = OH_neg+(&#10;            self.flow_channel_characteristics['K_L_OH']*OH_neg - &#10;            1/(&#10;                1/(L*(r_H2+r_H2_CO2)) +&#10;                1/(self.flow_channel_characteristics['K_L_HCO3']*HCO3_neg)&#10;            )&#10;            + L*(r_H2+r_H2_CO2)&#10;        ) / (&#10;            self.flow_channel_characteristics['K_L_OH'] + 2*eps*L*self.chemical_reaction_rates['k1f']*c00&#10;        )&#10;&#10;        # Update CO2 concentration with chemical reaction&#10;        k1 = k0 + eps*self.chemical_reaction_rates['k1f']*c10&#10;        eff_1 = 1/(M(k1)/torch.tanh(M(k1)) + k1*L/gdl_mass_transfer_coeff)&#10;        c01 = eff_1*self.p0*Hnr&#10;        &#10;        # Update OH- concentration&#10;        r_H2_CO2 = (2*k0_CO + 6*k0_C2H4)*c01&#10;        c11 = OH_neg + (&#10;            self.flow_channel_characteristics['K_L_OH']*OH_neg - 1/(&#10;                1/(L*(r_H2+r_H2_CO2)) + 1/(self.flow_channel_characteristics['K_L_HCO3']*HCO3_neg)&#10;            ) + L*(r_H2+r_H2_CO2) &#10;            ) / (&#10;            self.flow_channel_characteristics['K_L_OH']+2*self.chemical_reaction_rates['k1f']*L*eps*c01&#10;        )&#10;        &#10;        # Solve for CO3-- concentration&#10;        A_1 = (&#10;            2*self.flow_channel_characteristics['K_L_CO3']*CO3_2neg + self.flow_channel_characteristics['K_L_HCO3']*HCO3_neg + self.flow_channel_characteristics['K_L_OH']*OH_neg + L*r_H2 - self.flow_channel_characteristics['K_L_OH']*c11&#10;        ) / (2*self.flow_channel_characteristics['K_L_CO3'])&#10;        B_2 = L*2*k0*c01 / (2*self.flow_channel_characteristics['K_L_CO3']) * torch.exp(-c11*(self.salting_out_exponents['h_OH']+self.salting_out_exponents['h_K']))&#10;        C = self.salting_out_exponents['h_CO3']+2*self.salting_out_exponents['h_K']&#10;        c20 = A_1+torch.log(&#10;            1+(B_2*C * torch.exp(-A_1*C)) / (1+torch.log(torch.sqrt(1+B_2*C*torch.exp(-A_1*C))))&#10;            ) / C&#10;        &#10;        # Salting-out corrected CO2 concentration&#10;        c02 = eff_1*self.p0*Hnr_c(c11,c20)&#10;        r_H2_CO2 = (2*k0_CO + 6*k0_C2H4)*c02&#10;        &#10;        # Corrected OH- concentration&#10;        c12 = OH_neg + (&#10;            self.flow_channel_characteristics['K_L_OH']*OH_neg - 1/(&#10;                1/(L*(r_H2+r_H2_CO2)) + 1/(self.flow_channel_characteristics['K_L_HCO3']*HCO3_neg)&#10;            ) + L*(r_H2+r_H2_CO2)&#10;            ) / (&#10;            self.flow_channel_characteristics['K_L_OH']+2*self.chemical_reaction_rates['k1f']*L*eps*c02&#10;        )&#10;&#10;        # Final iteration&#10;        k2 = k0 + eps*self.chemical_reaction_rates['k1f']*c12&#10;        eff_2 = 1/(&#10;            torch.sqrt(&#10;                k2*L**2/DCO2&#10;            ) / torch.tanh(torch.sqrt(&#10;                k2*L**2/DCO2&#10;            )) + k2*L/gdl_mass_transfer_coeff&#10;        )&#10;        A_1_1 = (&#10;            2*self.flow_channel_characteristics['K_L_CO3']*CO3_2neg + self.flow_channel_characteristics['K_L_HCO3']*HCO3_neg + self.flow_channel_characteristics['K_L_OH']*OH_neg + L*r_H2 - self.flow_channel_characteristics['K_L_OH']*c12&#10;        ) / (2*self.flow_channel_characteristics['K_L_CO3'])&#10;        &#10;        B_2_1 = L*2*k0*eff_2*Hnr*self.p0 / (2*self.flow_channel_characteristics['K_L_CO3']) * torch.exp(-c12*(self.salting_out_exponents['h_OH']+self.salting_out_exponents['h_K']))&#10;        c21 = A_1_1+torch.log(&#10;            1+(&#10;                B_2_1*(self.salting_out_exponents['h_CO3']+2*self.salting_out_exponents['h_K']) * torch.exp(-A_1_1*(self.salting_out_exponents['h_CO3']+2*self.salting_out_exponents['h_K']))&#10;                ) / (&#10;                    1+torch.log(torch.sqrt(1+B_2_1*(self.salting_out_exponents['h_CO3']+2*self.salting_out_exponents['h_K'])*torch.exp(-A_1_1*(self.salting_out_exponents['h_CO3']+2*self.salting_out_exponents['h_K']))))&#10;                    )&#10;            ) / (&#10;            self.salting_out_exponents['h_CO3']+2*self.salting_out_exponents['h_K']&#10;        )&#10;        c21 = torch.maximum(c21, torch.zeros_like(c21))&#10;        c03 = self.p0*Hnr_c(c12, c21)*eff_2&#10;        &#10;        # Calculate current densities&#10;        potential_vs_rhe = phi_ext - R*T/F*torch.log(c12/OH_neg)&#10;        co_current_density = L*c03*k0_CO*2*F/10     # mA/cm^2&#10;        c2h4_current_density = L*c03*k0_C2H4*6*F/10 # mA/cm^2&#10;        h2_current_density = (F*L*r_H2)/10          # mA/cm^2&#10;        current_density = co_current_density + c2h4_current_density + h2_current_density&#10;        &#10;        # Calculate Faradaic efficiencies&#10;        fe_co = co_current_density / current_density&#10;        fe_c2h4 = c2h4_current_density / current_density&#10;        &#10;        # Other calculated variables&#10;        co2 = c03&#10;        co3 = c21&#10;        pH = torch.log10(c12/1000/self.initial_carbonate_equilibria['Kw'])&#10;        parasitic = c03*c12*eps*L*self.chemical_reaction_rates['k1f']&#10;        electrode = L*k0*c03&#10;        gdl_flux = gdl_mass_transfer_coeff*(&#10;            Hnr_c(c12,c21)*self.p0 - c03*torch.sqrt(&#10;                k2*L**2/DCO2&#10;            ) / torch.tanh(&#10;                torch.sqrt(k2*L**2/DCO2)&#10;            )&#10;        )&#10;        hco3 = torch.minimum(&#10;            HCO3_neg+Hnr_c(c12,c21)*self.p0,&#10;            co3*10**(3-pH)/self.initial_carbonate_equilibria['K1']&#10;        )&#10;        solubility = Hnr_c(c12,c21)/Hnr&#10;        &#10;        return {&#10;            'phi_ext': phi_ext,&#10;            'k0': k0,&#10;            'eff_0': eff_0,&#10;            'c00': c00,&#10;            'r_H2': r_H2,&#10;            'r_H2_CO2': r_H2_CO2,&#10;            'c10': c10,&#10;            'k1': k1,&#10;            'eff_1': eff_1,&#10;            'c01': c01,&#10;            'c11': c11,&#10;            'A_1': A_1,&#10;            'B_2': B_2,&#10;            'c20': c20,&#10;            'c02': c02,&#10;            'c12': c12,&#10;            'k2': k2,&#10;            'eff_2': eff_2,&#10;            'c03': c03,&#10;            'c21': c21,&#10;            'A_1_1': A_1_1,&#10;            'B_2_1': B_2_1,&#10;            'potential_vs_rhe': potential_vs_rhe,&#10;            'h2_current_density': h2_current_density,&#10;            'co_current_density': co_current_density,&#10;            'c2h4_current_density': c2h4_current_density,&#10;            'current_density': current_density,&#10;            'co2': co2,&#10;            'co3': co3,&#10;            'pH': pH,&#10;            'fe_co': fe_co,&#10;            'fe_c2h4': fe_c2h4,&#10;            'parasitic': parasitic,&#10;            'electrode': electrode,&#10;            'gdl_flux': gdl_flux,&#10;            'hco3': hco3,&#10;            'solubility': solubility&#10;        }" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/models/mlp_ensemble.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/models/mlp_ensemble.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;MLP Model Ensemble with BoTorch integration.&#10;Clean rebuild of the ensemble approach from Old_files.&#10;&quot;&quot;&quot;&#10;import torch&#10;import torch.nn as nn&#10;from torch.utils.data import DataLoader, TensorDataset&#10;import numpy as np&#10;from typing import List, Tuple, Optional, Dict&#10;from dataclasses import dataclass&#10;import copy&#10;&#10;# Add BoTorch imports for proper integration&#10;try:&#10;    from botorch.models.ensemble import EnsembleModel&#10;    from botorch.posteriors import Posterior&#10;    from gpytorch.distributions import MultitaskMultivariateNormal&#10;    BOTORCH_AVAILABLE = True&#10;except ImportError:&#10;    BOTORCH_AVAILABLE = False&#10;    print(&quot;Warning: BoTorch not available. Install with 'pip install botorch'&quot;)&#10;&#10;&#10;@dataclass&#10;class EnsembleConfig:&#10;    &quot;&quot;&quot;Configuration for MLP ensemble.&quot;&quot;&quot;&#10;    ensemble_size: int = 50&#10;    hidden_dim: int = 64&#10;    dropout_rate: float = 0.1&#10;    learning_rate: float = 0.001&#10;    bootstrap_fraction: float = 0.5  # Fraction of data each model sees&#10;&#10;&#10;class MLPModel(nn.Module):&#10;    &quot;&quot;&quot;&#10;    Single MLP model for Faradaic Efficiency prediction.&#10;&#10;    Architecture matches the old MLPModel but with cleaner implementation.&#10;    &quot;&quot;&quot;&#10;    def __init__(self, input_dim: int = 5, output_dim: int = 2,&#10;                 hidden_dim: int = 64, dropout_rate: float = 0.1):&#10;        super().__init__()&#10;&#10;        self.mlp = nn.Sequential(&#10;            nn.Linear(input_dim, hidden_dim),&#10;            nn.ReLU(),&#10;            nn.Dropout(dropout_rate),&#10;            nn.Linear(hidden_dim, hidden_dim),&#10;            nn.ReLU(),&#10;            nn.Dropout(dropout_rate),&#10;            nn.Linear(hidden_dim, hidden_dim),&#10;            nn.ReLU(),&#10;            nn.Dropout(dropout_rate),&#10;            nn.Linear(hidden_dim, output_dim),&#10;            nn.Sigmoid()  # Output FE values in [0,1] range&#10;        )&#10;&#10;        # Initialize weights for better training stability&#10;        self._init_weights()&#10;&#10;    def _init_weights(self):&#10;        &quot;&quot;&quot;Initialize weights using Xavier initialization.&quot;&quot;&quot;&#10;        for module in self.modules():&#10;            if isinstance(module, nn.Linear):&#10;                nn.init.xavier_uniform_(module.weight)&#10;                if module.bias is not None:&#10;                    nn.init.zeros_(module.bias)&#10;&#10;    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;&#10;        Forward pass through the MLP.&#10;&#10;        Args:&#10;            x: Input tensor of shape (batch_size, 5) - normalized features&#10;&#10;        Returns:&#10;            Predictions of shape (batch_size, 2) - [FE_Eth, FE_CO] in [0,1]&#10;        &quot;&quot;&quot;&#10;        # Validate input shape&#10;        assert x.ndim == 2 and x.shape[1] == 5, f&quot;Expected input shape (N, 5), got {x.shape}&quot;&#10;&#10;        return self.mlp(x)&#10;&#10;&#10;class MLPEnsemble:&#10;    &quot;&quot;&quot;&#10;    Ensemble of MLP models for uncertainty-aware predictions.&#10;&#10;    Provides both point predictions and uncertainty estimates compatible with BoTorch.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: EnsembleConfig = None):&#10;        self.config = config or EnsembleConfig()&#10;        self.models: List[MLPModel] = []&#10;        self.is_trained = False&#10;        &#10;        # BoTorch compatibility attributes&#10;        self.num_outputs = 2  # FE_Eth and FE_CO&#10;        self._num_outputs = 2&#10;        &#10;        # Create ensemble of models&#10;        for _ in range(self.config.ensemble_size):&#10;            model = MLPModel(&#10;                hidden_dim=self.config.hidden_dim,&#10;                dropout_rate=self.config.dropout_rate&#10;            )&#10;            self.models.append(model)&#10;&#10;    def _create_bootstrap_data(self, X: torch.Tensor, y: torch.Tensor,&#10;                              model_idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:&#10;        &quot;&quot;&quot;Create bootstrap sample for a specific model using sampling WITH replacement.&quot;&quot;&quot;&#10;        n_samples = int(len(X) * self.config.bootstrap_fraction)&#10;&#10;        # Use model index as seed for reproducible bootstrapping&#10;        generator = torch.Generator().manual_seed(model_idx)&#10;        # Sample WITH replacement (true bootstrapping)&#10;        indices = torch.randint(0, len(X), (n_samples,), generator=generator)&#10;&#10;        return X[indices], y[indices]&#10;&#10;    def train(self, X_train: torch.Tensor, y_train: torch.Tensor,&#10;              num_epochs: int = 400, verbose: bool = True) -&gt; Dict[str, List[float]]:&#10;        &quot;&quot;&quot;&#10;        Train the ensemble of MLP models.&#10;&#10;        Args:&#10;            X_train: Training features (N, 5) - normalized&#10;            y_train: Training targets (N, 2) - FE values in [0,1]&#10;            num_epochs: Number of training epochs&#10;            verbose: Whether to print training progress&#10;&#10;        Returns:&#10;            Dictionary with training statistics&#10;        &quot;&quot;&quot;&#10;        # Validate inputs&#10;        assert X_train.ndim == 2 and X_train.shape[1] == 5, f&quot;Expected X shape (N, 5), got {X_train.shape}&quot;&#10;        assert y_train.ndim == 2 and y_train.shape[1] == 2, f&quot;Expected y shape (N, 2), got {y_train.shape}&quot;&#10;        assert len(X_train) == len(y_train), &quot;X and y must have same number of samples&quot;&#10;&#10;        training_stats = {'losses': [], 'model_losses': [[] for _ in range(self.config.ensemble_size)]}&#10;&#10;        # Train each model in the ensemble&#10;        for model_idx, model in enumerate(self.models):&#10;            if verbose:&#10;                print(f&quot;Training model {model_idx + 1}/{self.config.ensemble_size}&quot;)&#10;&#10;            # Create bootstrap sample for this model&#10;            X_boot, y_boot = self._create_bootstrap_data(X_train, y_train, model_idx)&#10;&#10;            # Setup optimizer and loss&#10;            optimizer = torch.optim.Adam(model.parameters(), lr=self.config.learning_rate)&#10;            criterion = nn.MSELoss()&#10;&#10;            model.train()&#10;            model_losses = []&#10;&#10;            for epoch in range(num_epochs):&#10;                optimizer.zero_grad()&#10;&#10;                predictions = model(X_boot)&#10;                loss = criterion(predictions, y_boot)&#10;&#10;                loss.backward()&#10;                optimizer.step()&#10;&#10;                model_losses.append(loss.item())&#10;&#10;                # Print progress occasionally&#10;                if verbose and epoch % 100 == 0:&#10;                    print(f&quot;  Epoch {epoch}, Loss: {loss.item():.6f}&quot;)&#10;&#10;            training_stats['model_losses'][model_idx] = model_losses&#10;&#10;        self.is_trained = True&#10;&#10;        # Calculate ensemble training loss&#10;        if verbose:&#10;            final_losses = [losses[-1] for losses in training_stats['model_losses']]&#10;            avg_loss = np.mean(final_losses)&#10;            print(f&quot;Ensemble training complete. Average final loss: {avg_loss:.6f}&quot;)&#10;&#10;        return training_stats&#10;&#10;    def predict(self, X: torch.Tensor, return_std: bool = True) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor]]:&#10;        &quot;&quot;&quot;&#10;        Make predictions with the ensemble.&#10;&#10;        Args:&#10;            X: Input features (N, 5) - normalized&#10;            return_std: Whether to return uncertainty estimates&#10;&#10;        Returns:&#10;            mean_pred: Mean predictions (N, 2)&#10;            std_pred: Standard deviations (N, 2) if return_std=True&#10;        &quot;&quot;&quot;&#10;        assert self.is_trained, &quot;Ensemble must be trained before making predictions&quot;&#10;        assert X.ndim == 2 and X.shape[1] == 5, f&quot;Expected input shape (N, 5), got {X.shape}&quot;&#10;&#10;        # Collect predictions from all models&#10;        all_predictions = []&#10;&#10;        for model in self.models:&#10;            model.eval()&#10;            with torch.no_grad():&#10;                pred = model(X)&#10;                all_predictions.append(pred)&#10;&#10;        # Stack predictions: (ensemble_size, N, 2)&#10;        predictions = torch.stack(all_predictions, dim=0)&#10;&#10;        # Calculate statistics&#10;        mean_pred = predictions.mean(dim=0)  # (N, 2)&#10;&#10;        if return_std:&#10;            std_pred = predictions.std(dim=0)  # (N, 2)&#10;            return mean_pred, std_pred&#10;        else:&#10;            return mean_pred, None&#10;&#10;    def get_prediction_samples(self, X: torch.Tensor, n_samples: int = 1000) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;&#10;        Get prediction samples for uncertainty quantification (BoTorch compatibility).&#10;&#10;        Args:&#10;            X: Input features (N, 5)&#10;            n_samples: Number of samples to draw&#10;&#10;        Returns:&#10;            Samples of shape (n_samples, N, 2)&#10;        &quot;&quot;&quot;&#10;        assert self.is_trained, &quot;Ensemble must be trained before sampling&quot;&#10;&#10;        # For an ensemble, we can sample by randomly selecting models&#10;        samples = []&#10;&#10;        for _ in range(n_samples):&#10;            # Randomly select a model from the ensemble&#10;            model_idx = torch.randint(0, len(self.models), (1,)).item()&#10;            model = self.models[model_idx]&#10;&#10;            model.eval()&#10;            with torch.no_grad():&#10;                pred = model(X)&#10;                samples.append(pred)&#10;&#10;        return torch.stack(samples, dim=0)  # (n_samples, N, 2)&#10;&#10;    def save(self, filepath: str):&#10;        &quot;&quot;&quot;Save the trained ensemble.&quot;&quot;&quot;&#10;        if not self.is_trained:&#10;            raise ValueError(&quot;Cannot save untrained ensemble&quot;)&#10;&#10;        state_dict = {&#10;            'config': self.config,&#10;            'model_states': [model.state_dict() for model in self.models],&#10;            'is_trained': self.is_trained&#10;        }&#10;        torch.save(state_dict, filepath)&#10;&#10;    @classmethod&#10;    def load(cls, filepath: str) -&gt; 'MLPEnsemble':&#10;        &quot;&quot;&quot;Load a trained ensemble.&quot;&quot;&quot;&#10;        state_dict = torch.load(filepath)&#10;&#10;        ensemble = cls(config=state_dict['config'])&#10;&#10;        for model, model_state in zip(ensemble.models, state_dict['model_states']):&#10;            model.load_state_dict(model_state)&#10;&#10;        ensemble.is_trained = state_dict['is_trained']&#10;        return ensemble&#10;&#10;    def get_botorch_posterior(self, X: torch.Tensor) -&gt; 'Posterior':&#10;        &quot;&quot;&quot;&#10;        Get BoTorch-compatible posterior for the ensemble.&#10;&#10;        Args:&#10;            X: Input features (batch_size, 5) - normalized&#10;&#10;        Returns:&#10;            BoTorch Posterior object with correct tensor dimensions&#10;        &quot;&quot;&quot;&#10;        if not BOTORCH_AVAILABLE:&#10;            raise ImportError(&quot;BoTorch not available. Install with 'pip install botorch'&quot;)&#10;&#10;        assert self.is_trained, &quot;Ensemble must be trained before getting posterior&quot;&#10;        assert X.ndim == 2 and X.shape[1] == 5, f&quot;Expected input shape (N, 5), got {X.shape}&quot;&#10;&#10;        # Get ensemble predictions: (ensemble_size, batch_size, 2)&#10;        all_predictions = []&#10;        for model in self.models:&#10;            model.eval()&#10;            with torch.no_grad():&#10;                pred = model(X)  # (batch_size, 2)&#10;                all_predictions.append(pred)&#10;&#10;        predictions = torch.stack(all_predictions, dim=0)  # (ensemble_size, batch_size, 2)&#10;&#10;        # Calculate mean and covariance&#10;        mean = predictions.mean(dim=0)  # (batch_size, 2)&#10;&#10;        # For BoTorch, we need to reshape to (batch_size, 1, 2) for single-task&#10;        # But since we have 2 outputs (FE_Eth, FE_CO), this is multi-task&#10;        mean = mean.unsqueeze(1)  # (batch_size, 1, 2)&#10;&#10;        # Calculate covariance across ensemble&#10;        # predictions: (ensemble_size, batch_size, 2)&#10;        # We need covariance matrix for each batch element&#10;        batch_size = X.shape[0]&#10;        covariance_matrices = []&#10;&#10;        for b in range(batch_size):&#10;            # Get predictions for this batch element across all ensemble members&#10;            batch_preds = predictions[:, b, :]  # (ensemble_size, 2)&#10;            # Calculate covariance matrix&#10;            cov = torch.cov(batch_preds.T)  # (2, 2)&#10;            # Add small diagonal for numerical stability&#10;            cov = cov + 1e-6 * torch.eye(2)&#10;            covariance_matrices.append(cov)&#10;&#10;        # Stack covariance matrices: (batch_size, 2, 2)&#10;        covariance = torch.stack(covariance_matrices, dim=0)&#10;&#10;        # Create MultitaskMultivariateNormal distribution&#10;        mvn = MultitaskMultivariateNormal(mean, covariance)&#10;&#10;        # Return as BoTorch Posterior&#10;        from botorch.posteriors.gpytorch import GPyTorchPosterior&#10;        return GPyTorchPosterior(mvn)&#10;&#10;    def forward(self, X: torch.Tensor) -&gt; 'Posterior':&#10;        &quot;&quot;&quot;&#10;        BoTorch-style forward method.&#10;&#10;        Args:&#10;            X: Input tensor (batch_size, 5)&#10;&#10;        Returns:&#10;            Posterior with correct BoTorch format&#10;        &quot;&quot;&quot;&#10;        return self.get_botorch_posterior(X)&#10;&#10;    def get_prediction_samples_botorch(self, X: torch.Tensor, n_samples: int = 1000) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;&#10;        Get prediction samples in BoTorch format.&#10;&#10;        Args:&#10;            X: Input features (batch_size, 5)&#10;            n_samples: Number of samples to draw&#10;&#10;        Returns:&#10;            Samples of shape (n_samples, batch_size, 1, 2) for BoTorch compatibility&#10;        &quot;&quot;&quot;&#10;        # Get samples in our format: (n_samples, batch_size, 2)&#10;        samples = self.get_prediction_samples(X, n_samples)&#10;&#10;        # Reshape for BoTorch: (n_samples, batch_size, 1, 2)&#10;        return samples.unsqueeze(2)  # Add task dimension&#10;&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#13;&#10;MLP Model Ensemble with BoTorch integration.&#13;&#10;Clean rebuild of the ensemble approach from Old_files.&#13;&#10;&quot;&quot;&quot;&#13;&#10;import torch&#13;&#10;import torch.nn as nn&#13;&#10;from torch.utils.data import DataLoader, TensorDataset&#13;&#10;import numpy as np&#13;&#10;from typing import List, Tuple, Optional, Dict&#13;&#10;from dataclasses import dataclass&#13;&#10;import copy&#13;&#10;&#13;&#10;# Add BoTorch imports for proper integration&#13;&#10;try:&#13;&#10;    from botorch.models.ensemble import EnsembleModel&#13;&#10;    from botorch.posteriors import Posterior&#13;&#10;    from gpytorch.distributions import MultitaskMultivariateNormal&#13;&#10;    BOTORCH_AVAILABLE = True&#13;&#10;except ImportError:&#13;&#10;    BOTORCH_AVAILABLE = False&#13;&#10;    print(&quot;Warning: BoTorch not available. Install with 'pip install botorch'&quot;)&#13;&#10;&#13;&#10;&#13;&#10;@dataclass&#13;&#10;class EnsembleConfig:&#13;&#10;    &quot;&quot;&quot;Configuration for MLP ensemble.&quot;&quot;&quot;&#13;&#10;    ensemble_size: int = 50&#13;&#10;    hidden_dim: int = 64&#13;&#10;    dropout_rate: float = 0.1&#13;&#10;    learning_rate: float = 0.001&#13;&#10;    bootstrap_fraction: float = 0.5  # Fraction of data each model sees&#13;&#10;&#13;&#10;&#13;&#10;class MLPModel(nn.Module):&#13;&#10;    &quot;&quot;&quot;&#13;&#10;    Single MLP model for Faradaic Efficiency prediction.&#13;&#10;&#13;&#10;    Architecture matches the old MLPModel but with cleaner implementation.&#13;&#10;    &quot;&quot;&quot;&#13;&#10;    def __init__(self, input_dim: int = 5, output_dim: int = 2,&#13;&#10;                 hidden_dim: int = 64, dropout_rate: float = 0.1):&#13;&#10;        super().__init__()&#13;&#10;&#13;&#10;        self.mlp = nn.Sequential(&#13;&#10;            nn.Linear(input_dim, hidden_dim),&#13;&#10;            nn.ReLU(),&#13;&#10;            nn.Dropout(dropout_rate),&#13;&#10;            nn.Linear(hidden_dim, hidden_dim),&#13;&#10;            nn.ReLU(),&#13;&#10;            nn.Dropout(dropout_rate),&#13;&#10;            nn.Linear(hidden_dim, hidden_dim),&#13;&#10;            nn.ReLU(),&#13;&#10;            nn.Dropout(dropout_rate),&#13;&#10;            nn.Linear(hidden_dim, output_dim),&#13;&#10;            nn.Sigmoid()  # Output FE values in [0,1] range&#13;&#10;        )&#13;&#10;&#13;&#10;        # Initialize weights for better training stability&#13;&#10;        self._init_weights()&#13;&#10;&#13;&#10;    def _init_weights(self):&#13;&#10;        &quot;&quot;&quot;Initialize weights using Xavier initialization.&quot;&quot;&quot;&#13;&#10;        for module in self.modules():&#13;&#10;            if isinstance(module, nn.Linear):&#13;&#10;                nn.init.xavier_uniform_(module.weight)&#13;&#10;                if module.bias is not None:&#13;&#10;                    nn.init.zeros_(module.bias)&#13;&#10;&#13;&#10;    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        Forward pass through the MLP.&#13;&#10;&#13;&#10;        Args:&#13;&#10;            x: Input tensor of shape (batch_size, 5) - normalized features&#13;&#10;&#13;&#10;        Returns:&#13;&#10;            Predictions of shape (batch_size, 2) - [FE_Eth, FE_CO] in [0,1]&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        # Validate input shape&#13;&#10;        assert x.ndim == 2 and x.shape[1] == 5, f&quot;Expected input shape (N, 5), got {x.shape}&quot;&#13;&#10;&#13;&#10;        return self.mlp(x)&#13;&#10;&#13;&#10;&#13;&#10;class MLPEnsemble:&#13;&#10;    &quot;&quot;&quot;&#13;&#10;    Ensemble of MLP models for uncertainty-aware predictions.&#13;&#10;&#13;&#10;    Provides both point predictions and uncertainty estimates compatible with BoTorch.&#13;&#10;    &quot;&quot;&quot;&#13;&#10;&#13;&#10;    def __init__(self, config: EnsembleConfig = None):&#13;&#10;        self.config = config or EnsembleConfig()&#13;&#10;        self.models: List[MLPModel] = []&#13;&#10;        self.is_trained = False&#13;&#10;        &#13;&#10;        # BoTorch compatibility attributes&#13;&#10;        self.num_outputs = 2  # FE_Eth and FE_CO&#13;&#10;        self._num_outputs = 2&#13;&#10;        &#13;&#10;        # Create ensemble of models&#13;&#10;        for _ in range(self.config.ensemble_size):&#13;&#10;            model = MLPModel(&#13;&#10;                hidden_dim=self.config.hidden_dim,&#13;&#10;                dropout_rate=self.config.dropout_rate&#13;&#10;            )&#13;&#10;            self.models.append(model)&#13;&#10;&#13;&#10;    def _create_bootstrap_data(self, X: torch.Tensor, y: torch.Tensor,&#13;&#10;                              model_idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:&#13;&#10;        &quot;&quot;&quot;Create bootstrap sample for a specific model using sampling WITH replacement.&quot;&quot;&quot;&#13;&#10;        n_samples = int(len(X) * self.config.bootstrap_fraction)&#13;&#10;&#13;&#10;        # Use model index as seed for reproducible bootstrapping&#13;&#10;        generator = torch.Generator().manual_seed(model_idx)&#13;&#10;        # Sample WITH replacement (true bootstrapping)&#13;&#10;        indices = torch.randint(0, len(X), (n_samples,), generator=generator)&#13;&#10;&#13;&#10;        return X[indices], y[indices]&#13;&#10;&#13;&#10;    def train(self, X_train: torch.Tensor, y_train: torch.Tensor,&#13;&#10;              num_epochs: int = 400, verbose: bool = True) -&gt; Dict[str, List[float]]:&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        Train the ensemble of MLP models.&#13;&#10;&#13;&#10;        Args:&#13;&#10;            X_train: Training features (N, 5) - normalized&#13;&#10;            y_train: Training targets (N, 2) - FE values in [0,1]&#13;&#10;            num_epochs: Number of training epochs&#13;&#10;            verbose: Whether to print training progress&#13;&#10;&#13;&#10;        Returns:&#13;&#10;            Dictionary with training statistics&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        # Validate inputs&#13;&#10;        assert X_train.ndim == 2 and X_train.shape[1] == 5, f&quot;Expected X shape (N, 5), got {X_train.shape}&quot;&#13;&#10;        assert y_train.ndim == 2 and y_train.shape[1] == 2, f&quot;Expected y shape (N, 2), got {y_train.shape}&quot;&#13;&#10;        assert len(X_train) == len(y_train), &quot;X and y must have same number of samples&quot;&#13;&#10;&#13;&#10;        training_stats = {'losses': [], 'model_losses': [[] for _ in range(self.config.ensemble_size)]}&#13;&#10;&#13;&#10;        # Train each model in the ensemble&#13;&#10;        for model_idx, model in enumerate(self.models):&#13;&#10;            if verbose:&#13;&#10;                print(f&quot;Training model {model_idx + 1}/{self.config.ensemble_size}&quot;)&#13;&#10;&#13;&#10;            # Create bootstrap sample for this model&#13;&#10;            X_boot, y_boot = self._create_bootstrap_data(X_train, y_train, model_idx)&#13;&#10;&#13;&#10;            # Setup optimizer and loss&#13;&#10;            optimizer = torch.optim.Adam(model.parameters(), lr=self.config.learning_rate)&#13;&#10;            criterion = nn.MSELoss()&#13;&#10;&#13;&#10;            model.train()&#13;&#10;            model_losses = []&#13;&#10;&#13;&#10;            for epoch in range(num_epochs):&#13;&#10;                optimizer.zero_grad()&#13;&#10;&#13;&#10;                predictions = model(X_boot)&#13;&#10;                loss = criterion(predictions, y_boot)&#13;&#10;&#13;&#10;                loss.backward()&#13;&#10;                optimizer.step()&#13;&#10;&#13;&#10;                model_losses.append(loss.item())&#13;&#10;&#13;&#10;                # Print progress occasionally&#13;&#10;                if verbose and epoch % 100 == 0:&#13;&#10;                    print(f&quot;  Epoch {epoch}, Loss: {loss.item():.6f}&quot;)&#13;&#10;&#13;&#10;            training_stats['model_losses'][model_idx] = model_losses&#13;&#10;&#13;&#10;        self.is_trained = True&#13;&#10;&#13;&#10;        # Calculate ensemble training loss&#13;&#10;        if verbose:&#13;&#10;            final_losses = [losses[-1] for losses in training_stats['model_losses']]&#13;&#10;            avg_loss = np.mean(final_losses)&#13;&#10;            print(f&quot;Ensemble training complete. Average final loss: {avg_loss:.6f}&quot;)&#13;&#10;&#13;&#10;        return training_stats&#13;&#10;&#13;&#10;    def predict(self, X: torch.Tensor, return_std: bool = True) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor]]:&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        Make predictions with the ensemble.&#13;&#10;&#13;&#10;        Args:&#13;&#10;            X: Input features (N, 5) - normalized&#13;&#10;            return_std: Whether to return uncertainty estimates&#13;&#10;&#13;&#10;        Returns:&#13;&#10;            mean_pred: Mean predictions (N, 2)&#13;&#10;            std_pred: Standard deviations (N, 2) if return_std=True&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        assert self.is_trained, &quot;Ensemble must be trained before making predictions&quot;&#13;&#10;        assert X.ndim == 2 and X.shape[1] == 5, f&quot;Expected input shape (N, 5), got {X.shape}&quot;&#13;&#10;&#13;&#10;        # Collect predictions from all models&#13;&#10;        all_predictions = []&#13;&#10;&#13;&#10;        for model in self.models:&#13;&#10;            model.eval()&#13;&#10;            with torch.no_grad():&#13;&#10;                pred = model(X)&#13;&#10;                all_predictions.append(pred)&#13;&#10;&#13;&#10;        # Stack predictions: (ensemble_size, N, 2)&#13;&#10;        predictions = torch.stack(all_predictions, dim=0)&#13;&#10;&#13;&#10;        # Calculate statistics&#13;&#10;        mean_pred = predictions.mean(dim=0)  # (N, 2)&#13;&#10;&#13;&#10;        if return_std:&#13;&#10;            std_pred = predictions.std(dim=0)  # (N, 2)&#13;&#10;            return mean_pred, std_pred&#13;&#10;        else:&#13;&#10;            return mean_pred, None&#13;&#10;&#13;&#10;    def get_prediction_samples(self, X: torch.Tensor, n_samples: int = 1000) -&gt; torch.Tensor:&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        Get prediction samples for uncertainty quantification (BoTorch compatibility).&#13;&#10;&#13;&#10;        Args:&#13;&#10;            X: Input features (N, 5)&#13;&#10;            n_samples: Number of samples to draw&#13;&#10;&#13;&#10;        Returns:&#13;&#10;            Samples of shape (n_samples, N, 2)&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        assert self.is_trained, &quot;Ensemble must be trained before sampling&quot;&#13;&#10;&#13;&#10;        # For an ensemble, we can sample by randomly selecting models&#13;&#10;        samples = []&#13;&#10;&#13;&#10;        for _ in range(n_samples):&#13;&#10;            # Randomly select a model from the ensemble&#13;&#10;            model_idx = torch.randint(0, len(self.models), (1,)).item()&#13;&#10;            model = self.models[model_idx]&#13;&#10;&#13;&#10;            model.eval()&#13;&#10;            with torch.no_grad():&#13;&#10;                pred = model(X)&#13;&#10;                samples.append(pred)&#13;&#10;&#13;&#10;        return torch.stack(samples, dim=0)  # (n_samples, N, 2)&#13;&#10;&#13;&#10;    def save(self, filepath: str):&#13;&#10;        &quot;&quot;&quot;Save the trained ensemble.&quot;&quot;&quot;&#13;&#10;        if not self.is_trained:&#13;&#10;            raise ValueError(&quot;Cannot save untrained ensemble&quot;)&#13;&#10;&#13;&#10;        state_dict = {&#13;&#10;            'config': self.config,&#13;&#10;            'model_states': [model.state_dict() for model in self.models],&#13;&#10;            'is_trained': self.is_trained&#13;&#10;        }&#13;&#10;        torch.save(state_dict, filepath)&#13;&#10;&#13;&#10;    @classmethod&#13;&#10;    def load(cls, filepath: str) -&gt; 'MLPEnsemble':&#13;&#10;        &quot;&quot;&quot;Load a trained ensemble.&quot;&quot;&quot;&#13;&#10;        state_dict = torch.load(filepath)&#13;&#10;&#13;&#10;        ensemble = cls(config=state_dict['config'])&#13;&#10;&#13;&#10;        for model, model_state in zip(ensemble.models, state_dict['model_states']):&#13;&#10;            model.load_state_dict(model_state)&#13;&#10;&#13;&#10;        ensemble.is_trained = state_dict['is_trained']&#13;&#10;        return ensemble&#13;&#10;&#13;&#10;    def get_botorch_posterior(self, X: torch.Tensor) -&gt; 'Posterior':&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        Get BoTorch-compatible posterior for the ensemble.&#13;&#10;&#13;&#10;        Args:&#13;&#10;            X: Input features (batch_size, 5) - normalized&#13;&#10;&#13;&#10;        Returns:&#13;&#10;            BoTorch Posterior object with correct tensor dimensions&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        if not BOTORCH_AVAILABLE:&#13;&#10;            raise ImportError(&quot;BoTorch not available. Install with 'pip install botorch'&quot;)&#13;&#10;&#13;&#10;        assert self.is_trained, &quot;Ensemble must be trained before getting posterior&quot;&#13;&#10;        assert X.ndim == 2 and X.shape[1] == 5, f&quot;Expected input shape (N, 5), got {X.shape}&quot;&#13;&#10;&#13;&#10;        # Get ensemble predictions: (ensemble_size, batch_size, 2)&#13;&#10;        all_predictions = []&#13;&#10;        for model in self.models:&#13;&#10;            model.eval()&#13;&#10;            with torch.no_grad():&#13;&#10;                pred = model(X)  # (batch_size, 2)&#13;&#10;                all_predictions.append(pred)&#13;&#10;&#13;&#10;        predictions = torch.stack(all_predictions, dim=0)  # (ensemble_size, batch_size, 2)&#13;&#10;&#13;&#10;        # Calculate mean and covariance&#13;&#10;        mean = predictions.mean(dim=0)  # (batch_size, 2)&#13;&#10;&#13;&#10;        # For BoTorch, we need to reshape to (batch_size, 1, 2) for single-task&#13;&#10;        # But since we have 2 outputs (FE_Eth, FE_CO), this is multi-task&#13;&#10;        mean = mean.unsqueeze(1)  # (batch_size, 1, 2)&#13;&#10;&#13;&#10;        # Calculate covariance across ensemble&#13;&#10;        # predictions: (ensemble_size, batch_size, 2)&#13;&#10;        # We need covariance matrix for each batch element&#13;&#10;        batch_size = X.shape[0]&#13;&#10;        covariance_matrices = []&#13;&#10;&#13;&#10;        for b in range(batch_size):&#13;&#10;            # Get predictions for this batch element across all ensemble members&#13;&#10;            batch_preds = predictions[:, b, :]  # (ensemble_size, 2)&#13;&#10;            # Calculate covariance matrix&#13;&#10;            cov = torch.cov(batch_preds.T)  # (2, 2)&#13;&#10;            # Add small diagonal for numerical stability&#13;&#10;            cov = cov + 1e-6 * torch.eye(2)&#13;&#10;            covariance_matrices.append(cov)&#13;&#10;&#13;&#10;        # Stack covariance matrices: (batch_size, 2, 2)&#13;&#10;        covariance = torch.stack(covariance_matrices, dim=0)&#13;&#10;&#13;&#10;        # Create MultitaskMultivariateNormal distribution&#13;&#10;        mvn = MultitaskMultivariateNormal(mean, covariance)&#13;&#10;&#13;&#10;        # Return as BoTorch Posterior&#13;&#10;        from botorch.posteriors.gpytorch import GPyTorchPosterior&#13;&#10;        return GPyTorchPosterior(mvn)&#13;&#10;&#13;&#10;    def forward(self, X: torch.Tensor) -&gt; 'Posterior':&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        BoTorch-style forward method.&#13;&#10;&#13;&#10;        Args:&#13;&#10;            X: Input tensor (batch_size, 5)&#13;&#10;&#13;&#10;        Returns:&#13;&#10;            Posterior with correct BoTorch format&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        return self.get_botorch_posterior(X)&#13;&#10;&#13;&#10;    def get_prediction_samples_botorch(self, X: torch.Tensor, n_samples: int = 1000) -&gt; torch.Tensor:&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        Get prediction samples in BoTorch format.&#13;&#10;&#13;&#10;        Args:&#13;&#10;            X: Input features (batch_size, 5)&#13;&#10;            n_samples: Number of samples to draw&#13;&#10;&#13;&#10;        Returns:&#13;&#10;            Samples of shape (n_samples, batch_size, 1, 2) for BoTorch compatibility&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        # Get samples in our format: (n_samples, batch_size, 2)&#13;&#10;        samples = self.get_prediction_samples(X, n_samples)&#13;&#10;&#13;&#10;        # Reshape for BoTorch: (n_samples, batch_size, 1, 2)&#13;&#10;        return samples.unsqueeze(2)  # Add task dimension" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/models/ph_ensemble.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/models/ph_ensemble.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;PhModel Ensemble for physics-informed predictions with uncertainty.&#10;Uses the same ensemble approach as MLP but with physics-informed models.&#10;&quot;&quot;&quot;&#10;import torch&#10;import torch.nn as nn&#10;import numpy as np&#10;from typing import List, Tuple, Optional, Dict&#10;from dataclasses import dataclass&#10;import copy&#10;from .physics_model import PhModel, PhysicsConfig&#10;&#10;# Add BoTorch imports for proper integration&#10;try:&#10;    from botorch.models.ensemble import EnsembleModel&#10;    from botorch.posteriors import Posterior&#10;    from gpytorch.distributions import MultitaskMultivariateNormal&#10;    BOTORCH_AVAILABLE = True&#10;except ImportError:&#10;    BOTORCH_AVAILABLE = False&#10;    print(&quot;Warning: BoTorch not available. Install with 'pip install botorch'&quot;)&#10;&#10;&#10;@dataclass&#10;class PhEnsembleConfig:&#10;    &quot;&quot;&quot;Configuration for PhModel ensemble.&quot;&quot;&quot;&#10;    ensemble_size: int = 50&#10;    hidden_dim: int = 64&#10;    dropout_rate: float = 0.1&#10;    learning_rate: float = 0.001&#10;    bootstrap_fraction: float = 0.5  # Fraction of data each model sees&#10;    current_target: float = 200.0    # Target current density [A/m^2]&#10;    grid_size: int = 1000&#10;    voltage_bounds: Tuple[float, float] = (-1.25, 0.0)&#10;&#10;&#10;class PhModelEnsemble:&#10;    &quot;&quot;&quot;&#10;    Ensemble of Physics-Informed Models for uncertainty-aware predictions.&#10;&#10;    Combines the ensemble training approach (like MLP) with physics-informed models.&#10;    Each model in the ensemble is a complete PhModel with the full physics engine.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: PhEnsembleConfig = None, zlt_mu_stds: Optional[Tuple[float, float]] = None):&#10;        self.config = config or PhEnsembleConfig()&#10;        self.zlt_mu_stds = zlt_mu_stds or (5e-6, 1e-6)  # Default ZLT normalization&#10;        self.models: List[PhModel] = []&#10;        self.is_trained = False&#10;&#10;        # Create ensemble of PhModels&#10;        for _ in range(self.config.ensemble_size):&#10;            ph_config = PhysicsConfig(&#10;                hidden_dim=self.config.hidden_dim,&#10;                dropout_rate=self.config.dropout_rate,&#10;                current_target=self.config.current_target,&#10;                grid_size=self.config.grid_size,&#10;                voltage_bounds=self.config.voltage_bounds&#10;            )&#10;            model = PhModel(config=ph_config, zlt_mu_stds=self.zlt_mu_stds)&#10;            self.models.append(model)&#10;&#10;    def _create_bootstrap_data(self, X: torch.Tensor, y: torch.Tensor,&#10;                              model_idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:&#10;        &quot;&quot;&quot;Create bootstrap sample for a specific model using sampling WITH replacement.&quot;&quot;&quot;&#10;        n_samples = int(len(X) * self.config.bootstrap_fraction)&#10;&#10;        # Use model index as seed for reproducible bootstrapping&#10;        generator = torch.Generator().manual_seed(model_idx)&#10;        # Sample WITH replacement (true bootstrapping)&#10;        indices = torch.randint(0, len(X), (n_samples,), generator=generator)&#10;&#10;        return X[indices], y[indices]&#10;&#10;    def train(self, X_train: torch.Tensor, y_train: torch.Tensor,&#10;              num_epochs: int = 400, verbose: bool = True) -&gt; Dict[str, List[float]]:&#10;        &quot;&quot;&quot;&#10;        Train the ensemble of PhModels.&#10;&#10;        Args:&#10;            X_train: Training features (N, 5) - normalized&#10;            y_train: Training targets (N, 2) - FE values in [0,1]&#10;            num_epochs: Number of training epochs&#10;            verbose: Whether to print training progress&#10;&#10;        Returns:&#10;            Dictionary with training statistics&#10;        &quot;&quot;&quot;&#10;        # Validate inputs&#10;        assert X_train.ndim == 2 and X_train.shape[1] == 5, f&quot;Expected X shape (N, 5), got {X_train.shape}&quot;&#10;        assert y_train.ndim == 2 and y_train.shape[1] == 2, f&quot;Expected y shape (N, 2), got {y_train.shape}&quot;&#10;        assert len(X_train) == len(y_train), &quot;X and y must have same number of samples&quot;&#10;&#10;        training_stats = {'losses': [], 'model_losses': [[] for _ in range(self.config.ensemble_size)]}&#10;&#10;        # Train each PhModel in the ensemble&#10;        for model_idx, model in enumerate(self.models):&#10;            if verbose:&#10;                print(f&quot;Training PhModel {model_idx + 1}/{self.config.ensemble_size}&quot;)&#10;&#10;            # Create bootstrap sample for this model&#10;            X_boot, y_boot = self._create_bootstrap_data(X_train, y_train, model_idx)&#10;&#10;            # Setup optimizer and loss&#10;            optimizer = torch.optim.Adam(model.parameters(), lr=self.config.learning_rate)&#10;            criterion = nn.MSELoss()&#10;&#10;            model.train()&#10;            model_losses = []&#10;&#10;            for epoch in range(num_epochs):&#10;                optimizer.zero_grad()&#10;&#10;                predictions = model(X_boot)&#10;                loss = criterion(predictions, y_boot)&#10;&#10;                loss.backward()&#10;                optimizer.step()&#10;&#10;                model_losses.append(loss.item())&#10;&#10;                # Print progress occasionally&#10;                if verbose and epoch % 100 == 0:&#10;                    print(f&quot;  Epoch {epoch}, Loss: {loss.item():.6f}&quot;)&#10;&#10;            training_stats['model_losses'][model_idx] = model_losses&#10;&#10;        self.is_trained = True&#10;&#10;        # Calculate ensemble training loss&#10;        if verbose:&#10;            final_losses = [losses[-1] for losses in training_stats['model_losses']]&#10;            avg_loss = np.mean(final_losses)&#10;            print(f&quot;PhModel ensemble training complete. Average final loss: {avg_loss:.6f}&quot;)&#10;&#10;        return training_stats&#10;&#10;    def predict(self, X: torch.Tensor, return_std: bool = True) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor]]:&#10;        &quot;&quot;&quot;&#10;        Make predictions with the PhModel ensemble.&#10;&#10;        Args:&#10;            X: Input features (N, 5) - normalized&#10;            return_std: Whether to return uncertainty estimates&#10;&#10;        Returns:&#10;            mean_pred: Mean predictions (N, 2)&#10;            std_pred: Standard deviations (N, 2) if return_std=True&#10;        &quot;&quot;&quot;&#10;        assert self.is_trained, &quot;PhModel ensemble must be trained before making predictions&quot;&#10;        assert X.ndim == 2 and X.shape[1] == 5, f&quot;Expected input shape (N, 5), got {X.shape}&quot;&#10;&#10;        # Collect predictions from all PhModels&#10;        all_predictions = []&#10;&#10;        for model in self.models:&#10;            model.eval()&#10;            with torch.no_grad():&#10;                pred = model(X)&#10;                all_predictions.append(pred)&#10;&#10;        # Stack predictions: (ensemble_size, N, 2)&#10;        predictions = torch.stack(all_predictions, dim=0)&#10;&#10;        # Calculate statistics&#10;        mean_pred = predictions.mean(dim=0)  # (N, 2)&#10;&#10;        if return_std:&#10;            std_pred = predictions.std(dim=0)  # (N, 2)&#10;            return mean_pred, std_pred&#10;        else:&#10;            return mean_pred, None&#10;&#10;    def get_prediction_samples(self, X: torch.Tensor, n_samples: int = 1000) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;&#10;        Get prediction samples for uncertainty quantification.&#10;&#10;        Args:&#10;            X: Input features (N, 5)&#10;            n_samples: Number of samples to draw&#10;&#10;        Returns:&#10;            Samples of shape (n_samples, N, 2)&#10;        &quot;&quot;&quot;&#10;        assert self.is_trained, &quot;PhModel ensemble must be trained before sampling&quot;&#10;&#10;        # For an ensemble, we can sample by randomly selecting models&#10;        samples = []&#10;&#10;        for _ in range(n_samples):&#10;            # Randomly select a model from the ensemble&#10;            model_idx = torch.randint(0, len(self.models), (1,)).item()&#10;            model = self.models[model_idx]&#10;&#10;            model.eval()&#10;            with torch.no_grad():&#10;                pred = model(X)&#10;                samples.append(pred)&#10;&#10;        return torch.stack(samples, dim=0)  # (n_samples, N, 2)&#10;&#10;    def get_botorch_posterior(self, X: torch.Tensor) -&gt; 'Posterior':&#10;        &quot;&quot;&quot;&#10;        Get BoTorch-compatible posterior for the PhModel ensemble.&#10;&#10;        Args:&#10;            X: Input features (batch_size, 5) - normalized&#10;&#10;        Returns:&#10;            BoTorch Posterior object with correct tensor dimensions&#10;        &quot;&quot;&quot;&#10;        if not BOTORCH_AVAILABLE:&#10;            raise ImportError(&quot;BoTorch not available. Install with 'pip install botorch'&quot;)&#10;&#10;        assert self.is_trained, &quot;PhModel ensemble must be trained before getting posterior&quot;&#10;        assert X.ndim == 2 and X.shape[1] == 5, f&quot;Expected input shape (N, 5), got {X.shape}&quot;&#10;&#10;        # Get ensemble predictions: (ensemble_size, batch_size, 2)&#10;        all_predictions = []&#10;        for model in self.models:&#10;            model.eval()&#10;            with torch.no_grad():&#10;                pred = model(X)  # (batch_size, 2)&#10;                all_predictions.append(pred)&#10;&#10;        predictions = torch.stack(all_predictions, dim=0)  # (ensemble_size, batch_size, 2)&#10;&#10;        # Calculate mean and covariance&#10;        mean = predictions.mean(dim=0)  # (batch_size, 2)&#10;        mean = mean.unsqueeze(1)  # (batch_size, 1, 2) for BoTorch&#10;&#10;        # Calculate covariance across ensemble&#10;        batch_size = X.shape[0]&#10;        covariance_matrices = []&#10;&#10;        for b in range(batch_size):&#10;            # Get predictions for this batch element across all ensemble members&#10;            batch_preds = predictions[:, b, :]  # (ensemble_size, 2)&#10;            # Calculate covariance matrix&#10;            cov = torch.cov(batch_preds.T)  # (2, 2)&#10;            # Add small diagonal for numerical stability&#10;            cov = cov + 1e-6 * torch.eye(2)&#10;            covariance_matrices.append(cov)&#10;&#10;        # Stack covariance matrices: (batch_size, 2, 2)&#10;        covariance = torch.stack(covariance_matrices, dim=0)&#10;&#10;        # Create MultitaskMultivariateNormal distribution&#10;        mvn = MultitaskMultivariateNormal(mean, covariance)&#10;&#10;        # Return as BoTorch Posterior&#10;        from botorch.posteriors.gpytorch import GPyTorchPosterior&#10;        return GPyTorchPosterior(mvn)&#10;&#10;    def forward(self, X: torch.Tensor) -&gt; 'Posterior':&#10;        &quot;&quot;&quot;BoTorch-style forward method.&quot;&quot;&quot;&#10;        return self.get_botorch_posterior(X)&#10;&#10;    def get_prediction_samples_botorch(self, X: torch.Tensor, n_samples: int = 1000) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;&#10;        Get prediction samples in BoTorch format.&#10;&#10;        Args:&#10;            X: Input features (batch_size, 5)&#10;            n_samples: Number of samples to draw&#10;&#10;        Returns:&#10;            Samples of shape (n_samples, batch_size, 1, 2) for BoTorch compatibility&#10;        &quot;&quot;&quot;&#10;        # Get samples in our format: (n_samples, batch_size, 2)&#10;        samples = self.get_prediction_samples(X, n_samples)&#10;&#10;        # Reshape for BoTorch: (n_samples, batch_size, 1, 2)&#10;        return samples.unsqueeze(2)  # Add task dimension&#10;&#10;    def save(self, filepath: str):&#10;        &quot;&quot;&quot;Save the trained PhModel ensemble.&quot;&quot;&quot;&#10;        if not self.is_trained:&#10;            raise ValueError(&quot;Cannot save untrained PhModel ensemble&quot;)&#10;&#10;        state_dict = {&#10;            'config': self.config,&#10;            'zlt_mu_stds': self.zlt_mu_stds,&#10;            'model_states': [model.state_dict() for model in self.models],&#10;            'is_trained': self.is_trained&#10;        }&#10;        torch.save(state_dict, filepath)&#10;&#10;    @classmethod&#10;    def load(cls, filepath: str) -&gt; 'PhModelEnsemble':&#10;        &quot;&quot;&quot;Load a trained PhModel ensemble.&quot;&quot;&quot;&#10;        state_dict = torch.load(filepath)&#10;&#10;        ensemble = cls(config=state_dict['config'], zlt_mu_stds=state_dict['zlt_mu_stds'])&#10;&#10;        for model, model_state in zip(ensemble.models, state_dict['model_states']):&#10;            model.load_state_dict(model_state)&#10;&#10;        ensemble.is_trained = state_dict['is_trained']&#10;        return ensemble&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;PhModel Ensemble for physics-informed predictions with uncertainty.&#10;Uses the same ensemble approach as MLP but with physics-informed models.&#10;&quot;&quot;&quot;&#10;import torch&#10;import torch.nn as nn&#10;import numpy as np&#10;from typing import List, Tuple, Optional, Dict&#10;from dataclasses import dataclass&#10;import copy&#10;from .physics_model import PhModel, PhysicsConfig&#10;&#10;# Add BoTorch imports for proper integration&#10;try:&#10;    from botorch.models.ensemble import EnsembleModel&#10;    from botorch.posteriors import Posterior&#10;    from gpytorch.distributions import MultitaskMultivariateNormal&#10;    BOTORCH_AVAILABLE = True&#10;except ImportError:&#10;    BOTORCH_AVAILABLE = False&#10;    print(&quot;Warning: BoTorch not available. Install with 'pip install botorch'&quot;)&#10;&#10;&#10;@dataclass&#10;class PhEnsembleConfig:&#10;    &quot;&quot;&quot;Configuration for PhModel ensemble.&quot;&quot;&quot;&#10;    ensemble_size: int = 50&#10;    hidden_dim: int = 64&#10;    dropout_rate: float = 0.1&#10;    learning_rate: float = 0.001&#10;    bootstrap_fraction: float = 0.5  # Fraction of data each model sees&#10;    current_target: float = 200.0    # Target current density [A/m^2]&#10;    grid_size: int = 1000&#10;    voltage_bounds: Tuple[float, float] = (-1.25, 0.0)&#10;&#10;&#10;class PhModelEnsemble:&#10;    &quot;&quot;&quot;&#10;    Ensemble of Physics-Informed Models for uncertainty-aware predictions.&#10;&#10;    Combines the ensemble training approach (like MLP) with physics-informed models.&#10;    Each model in the ensemble is a complete PhModel with the full physics engine.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: PhEnsembleConfig = None, zlt_mu_stds: Optional[Tuple[float, float]] = None):&#10;        self.config = config or PhEnsembleConfig()&#10;        self.zlt_mu_stds = zlt_mu_stds or (5e-6, 1e-6)  # Default ZLT normalization&#10;        self.models: List[PhModel] = []&#10;        self.is_trained = False&#10;        &#10;        # BoTorch compatibility attributes&#10;        self.num_outputs = 2  # FE_Eth and FE_CO&#10;        self._num_outputs = 2&#10;        &#10;        # Create ensemble of PhModels&#10;        for _ in range(self.config.ensemble_size):&#10;            ph_config = PhysicsConfig(&#10;                hidden_dim=self.config.hidden_dim,&#10;                dropout_rate=self.config.dropout_rate,&#10;                current_target=self.config.current_target,&#10;                grid_size=self.config.grid_size,&#10;                voltage_bounds=self.config.voltage_bounds&#10;            )&#10;            model = PhModel(config=ph_config, zlt_mu_stds=self.zlt_mu_stds)&#10;            self.models.append(model)&#10;&#10;    def _create_bootstrap_data(self, X: torch.Tensor, y: torch.Tensor,&#10;                              model_idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:&#10;        &quot;&quot;&quot;Create bootstrap sample for a specific model using sampling WITH replacement.&quot;&quot;&quot;&#10;        n_samples = int(len(X) * self.config.bootstrap_fraction)&#10;&#10;        # Use model index as seed for reproducible bootstrapping&#10;        generator = torch.Generator().manual_seed(model_idx)&#10;        # Sample WITH replacement (true bootstrapping)&#10;        indices = torch.randint(0, len(X), (n_samples,), generator=generator)&#10;&#10;        return X[indices], y[indices]&#10;&#10;    def train(self, X_train: torch.Tensor, y_train: torch.Tensor,&#10;              num_epochs: int = 400, verbose: bool = True) -&gt; Dict[str, List[float]]:&#10;        &quot;&quot;&quot;&#10;        Train the ensemble of PhModels.&#10;&#10;        Args:&#10;            X_train: Training features (N, 5) - normalized&#10;            y_train: Training targets (N, 2) - FE values in [0,1]&#10;            num_epochs: Number of training epochs&#10;            verbose: Whether to print training progress&#10;&#10;        Returns:&#10;            Dictionary with training statistics&#10;        &quot;&quot;&quot;&#10;        # Validate inputs&#10;        assert X_train.ndim == 2 and X_train.shape[1] == 5, f&quot;Expected X shape (N, 5), got {X_train.shape}&quot;&#10;        assert y_train.ndim == 2 and y_train.shape[1] == 2, f&quot;Expected y shape (N, 2), got {y_train.shape}&quot;&#10;        assert len(X_train) == len(y_train), &quot;X and y must have same number of samples&quot;&#10;&#10;        training_stats = {'losses': [], 'model_losses': [[] for _ in range(self.config.ensemble_size)]}&#10;&#10;        # Train each PhModel in the ensemble&#10;        for model_idx, model in enumerate(self.models):&#10;            if verbose:&#10;                print(f&quot;Training PhModel {model_idx + 1}/{self.config.ensemble_size}&quot;)&#10;&#10;            # Create bootstrap sample for this model&#10;            X_boot, y_boot = self._create_bootstrap_data(X_train, y_train, model_idx)&#10;&#10;            # Setup optimizer and loss&#10;            optimizer = torch.optim.Adam(model.parameters(), lr=self.config.learning_rate)&#10;            criterion = nn.MSELoss()&#10;&#10;            model.train()&#10;            model_losses = []&#10;&#10;            for epoch in range(num_epochs):&#10;                optimizer.zero_grad()&#10;&#10;                predictions = model(X_boot)&#10;                loss = criterion(predictions, y_boot)&#10;&#10;                loss.backward()&#10;                optimizer.step()&#10;&#10;                model_losses.append(loss.item())&#10;&#10;                # Print progress occasionally&#10;                if verbose and epoch % 100 == 0:&#10;                    print(f&quot;  Epoch {epoch}, Loss: {loss.item():.6f}&quot;)&#10;&#10;            training_stats['model_losses'][model_idx] = model_losses&#10;&#10;        self.is_trained = True&#10;&#10;        # Calculate ensemble training loss&#10;        if verbose:&#10;            final_losses = [losses[-1] for losses in training_stats['model_losses']]&#10;            avg_loss = np.mean(final_losses)&#10;            print(f&quot;PhModel ensemble training complete. Average final loss: {avg_loss:.6f}&quot;)&#10;&#10;        return training_stats&#10;&#10;    def predict(self, X: torch.Tensor, return_std: bool = True) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor]]:&#10;        &quot;&quot;&quot;&#10;        Make predictions with the PhModel ensemble.&#10;&#10;        Args:&#10;            X: Input features (N, 5) - normalized&#10;            return_std: Whether to return uncertainty estimates&#10;&#10;        Returns:&#10;            mean_pred: Mean predictions (N, 2)&#10;            std_pred: Standard deviations (N, 2) if return_std=True&#10;        &quot;&quot;&quot;&#10;        assert self.is_trained, &quot;PhModel ensemble must be trained before making predictions&quot;&#10;        assert X.ndim == 2 and X.shape[1] == 5, f&quot;Expected input shape (N, 5), got {X.shape}&quot;&#10;&#10;        # Collect predictions from all PhModels&#10;        all_predictions = []&#10;&#10;        for model in self.models:&#10;            model.eval()&#10;            with torch.no_grad():&#10;                pred = model(X)&#10;                all_predictions.append(pred)&#10;&#10;        # Stack predictions: (ensemble_size, N, 2)&#10;        predictions = torch.stack(all_predictions, dim=0)&#10;&#10;        # Calculate statistics&#10;        mean_pred = predictions.mean(dim=0)  # (N, 2)&#10;&#10;        if return_std:&#10;            std_pred = predictions.std(dim=0)  # (N, 2)&#10;            return mean_pred, std_pred&#10;        else:&#10;            return mean_pred, None&#10;&#10;    def get_prediction_samples(self, X: torch.Tensor, n_samples: int = 1000) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;&#10;        Get prediction samples for uncertainty quantification.&#10;&#10;        Args:&#10;            X: Input features (N, 5)&#10;            n_samples: Number of samples to draw&#10;&#10;        Returns:&#10;            Samples of shape (n_samples, N, 2)&#10;        &quot;&quot;&quot;&#10;        assert self.is_trained, &quot;PhModel ensemble must be trained before sampling&quot;&#10;&#10;        # For an ensemble, we can sample by randomly selecting models&#10;        samples = []&#10;&#10;        for _ in range(n_samples):&#10;            # Randomly select a model from the ensemble&#10;            model_idx = torch.randint(0, len(self.models), (1,)).item()&#10;            model = self.models[model_idx]&#10;&#10;            model.eval()&#10;            with torch.no_grad():&#10;                pred = model(X)&#10;                samples.append(pred)&#10;&#10;        return torch.stack(samples, dim=0)  # (n_samples, N, 2)&#10;&#10;    def get_botorch_posterior(self, X: torch.Tensor) -&gt; 'Posterior':&#10;        &quot;&quot;&quot;&#10;        Get BoTorch-compatible posterior for the PhModel ensemble.&#10;&#10;        Args:&#10;            X: Input features (batch_size, 5) - normalized&#10;&#10;        Returns:&#10;            BoTorch Posterior object with correct tensor dimensions&#10;        &quot;&quot;&quot;&#10;        if not BOTORCH_AVAILABLE:&#10;            raise ImportError(&quot;BoTorch not available. Install with 'pip install botorch'&quot;)&#10;&#10;        assert self.is_trained, &quot;PhModel ensemble must be trained before getting posterior&quot;&#10;        assert X.ndim == 2 and X.shape[1] == 5, f&quot;Expected input shape (N, 5), got {X.shape}&quot;&#10;&#10;        # Get ensemble predictions: (ensemble_size, batch_size, 2)&#10;        all_predictions = []&#10;        for model in self.models:&#10;            model.eval()&#10;            with torch.no_grad():&#10;                pred = model(X)  # (batch_size, 2)&#10;                all_predictions.append(pred)&#10;&#10;        predictions = torch.stack(all_predictions, dim=0)  # (ensemble_size, batch_size, 2)&#10;&#10;        # Calculate mean and covariance&#10;        mean = predictions.mean(dim=0)  # (batch_size, 2)&#10;        mean = mean.unsqueeze(1)  # (batch_size, 1, 2) for BoTorch&#10;&#10;        # Calculate covariance across ensemble&#10;        batch_size = X.shape[0]&#10;        covariance_matrices = []&#10;&#10;        for b in range(batch_size):&#10;            # Get predictions for this batch element across all ensemble members&#10;            batch_preds = predictions[:, b, :]  # (ensemble_size, 2)&#10;            # Calculate covariance matrix&#10;            cov = torch.cov(batch_preds.T)  # (2, 2)&#10;            # Add small diagonal for numerical stability&#10;            cov = cov + 1e-6 * torch.eye(2)&#10;            covariance_matrices.append(cov)&#10;&#10;        # Stack covariance matrices: (batch_size, 2, 2)&#10;        covariance = torch.stack(covariance_matrices, dim=0)&#10;&#10;        # Create MultitaskMultivariateNormal distribution&#10;        mvn = MultitaskMultivariateNormal(mean, covariance)&#10;&#10;        # Return as BoTorch Posterior&#10;        from botorch.posteriors.gpytorch import GPyTorchPosterior&#10;        return GPyTorchPosterior(mvn)&#10;&#10;    def forward(self, X: torch.Tensor) -&gt; 'Posterior':&#10;        &quot;&quot;&quot;BoTorch-style forward method.&quot;&quot;&quot;&#10;        return self.get_botorch_posterior(X)&#10;&#10;    def get_prediction_samples_botorch(self, X: torch.Tensor, n_samples: int = 1000) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;&#10;        Get prediction samples in BoTorch format.&#10;&#10;        Args:&#10;            X: Input features (batch_size, 5)&#10;            n_samples: Number of samples to draw&#10;&#10;        Returns:&#10;            Samples of shape (n_samples, batch_size, 1, 2) for BoTorch compatibility&#10;        &quot;&quot;&quot;&#10;        # Get samples in our format: (n_samples, batch_size, 2)&#10;        samples = self.get_prediction_samples(X, n_samples)&#10;&#10;        # Reshape for BoTorch: (n_samples, batch_size, 1, 2)&#10;        return samples.unsqueeze(2)  # Add task dimension&#10;&#10;    def save(self, filepath: str):&#10;        &quot;&quot;&quot;Save the trained PhModel ensemble.&quot;&quot;&quot;&#10;        if not self.is_trained:&#10;            raise ValueError(&quot;Cannot save untrained PhModel ensemble&quot;)&#10;&#10;        state_dict = {&#10;            'config': self.config,&#10;            'zlt_mu_stds': self.zlt_mu_stds,&#10;            'model_states': [model.state_dict() for model in self.models],&#10;            'is_trained': self.is_trained&#10;        }&#10;        torch.save(state_dict, filepath)&#10;&#10;    @classmethod&#10;    def load(cls, filepath: str) -&gt; 'PhModelEnsemble':&#10;        &quot;&quot;&quot;Load a trained PhModel ensemble.&quot;&quot;&quot;&#10;        state_dict = torch.load(filepath)&#10;&#10;        ensemble = cls(config=state_dict['config'], zlt_mu_stds=state_dict['zlt_mu_stds'])&#10;&#10;        for model, model_state in zip(ensemble.models, state_dict['model_states']):&#10;            model.load_state_dict(model_state)&#10;&#10;        ensemble.is_trained = state_dict['is_trained']&#10;        return ensemble&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/models/physics_model.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/models/physics_model.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Physics-Informed Model (PhModel) for CO2 reduction predictions.&#10;Combines neural networks with electrochemical physics simulation.&#10;&quot;&quot;&quot;&#10;import torch&#10;import torch.nn as nn&#10;from typing import Dict, Tuple, Optional&#10;from dataclasses import dataclass&#10;import copy&#10;&#10;# Physics constants and parameters (from gde_multi.py)&#10;DIFFUSION_COEFFICIENTS = {&#10;    'CO2': 1.91e-09,&#10;    'OH': 5.293e-09,&#10;    'CO3': 9.23e-10,&#10;    'HCO3': 1.18e-09,&#10;    'H': 9.311e-09,&#10;    'K': 1.96e-09,&#10;    'CO': 2.03e-09,&#10;    'H2': 4.5e-09&#10;}  # [m^2/s]&#10;&#10;ELECTRODE_REACTION_KINETICS = {&#10;    'i_0_CO': 0.00471,    # [A/m^2]&#10;    'i_0_C2H4': 1e-5,     # [A/m^2]&#10;    'i_0_H2b': 1.16e-05,  # [A/m^2]&#10;    'alpha_CO': 0.44,&#10;    'alpha_C2H4': 0.4,&#10;    'alpha_H2b': 0.36&#10;}&#10;&#10;ELECTRODE_REACTION_POTENTIALS = {&#10;    'E_0_CO': -0.11,  # [V]&#10;    'E_0_C2H4': 0.09, # [V]&#10;    'E_0_H2b': 0.0    # [V]&#10;}&#10;&#10;&#10;@dataclass&#10;class PhysicsConfig:&#10;    &quot;&quot;&quot;Configuration for physics-informed model.&quot;&quot;&quot;&#10;    hidden_dim: int = 64&#10;    dropout_rate: float = 0.1&#10;    current_target: float = 200.0  # Target current density [A/m^2]&#10;    grid_size: int = 1000&#10;    voltage_bounds: Tuple[float, float] = (-1.25, 0.0)&#10;&#10;&#10;class SimplifiedPhysicsEngine:&#10;    &quot;&quot;&quot;&#10;    Simplified physics engine for electrochemical simulations.&#10;&#10;    This is a placeholder for the full gde_multi.System implementation.&#10;    In the real system, this would solve complex electrochemical equations.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        self.diffusion_coeffs = DIFFUSION_COEFFICIENTS&#10;        self.kinetics = ELECTRODE_REACTION_KINETICS&#10;        self.potentials = ELECTRODE_REACTION_POTENTIALS&#10;&#10;    def bruggeman(self, D_bulk: float, porosity: torch.Tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;Calculate effective diffusion coefficient using Bruggeman relation.&quot;&quot;&quot;&#10;        return D_bulk * porosity ** 1.5&#10;&#10;    def solve_current(self, i_target: float, eps: torch.Tensor, r: torch.Tensor,&#10;                     L: torch.Tensor, thetas: Dict[str, torch.Tensor],&#10;                     gdl_mass_transfer_coeff: torch.Tensor,&#10;                     grid_size: int = 1000,&#10;                     voltage_bounds: Tuple[float, float] = (-1.25, 0.0)) -&gt; Dict[str, torch.Tensor]:&#10;        &quot;&quot;&quot;&#10;        Simplified physics simulation for Faradaic efficiencies.&#10;&#10;        In the real implementation, this would solve:&#10;        - Mass transport equations&#10;        - Electrochemical kinetics&#10;        - Current-voltage relationships&#10;&#10;        For now, we use a simplified approximation based on the physics parameters.&#10;        &quot;&quot;&quot;&#10;        # Ensure all inputs have consistent shapes&#10;        batch_size = eps.shape[0]&#10;        device = eps.device&#10;&#10;        # Flatten all inputs to ensure consistent batch dimension&#10;        eps_flat = eps.view(batch_size, -1)  # (batch_size, 1)&#10;        r_flat = r.view(batch_size, -1)      # (batch_size, 1)&#10;&#10;        # Calculate effective surface areas and reaction rates&#10;        effective_area = eps_flat / r_flat  # Simplified surface area scaling&#10;&#10;        # Extract surface coverage fractions and ensure proper shapes&#10;        theta_co = thetas['CO'].view(batch_size, -1)      # (batch_size, 1)&#10;        theta_c2h4 = thetas['C2H4'].view(batch_size, -1)  # (batch_size, 1)&#10;        theta_h2b = thetas['H2b'].view(batch_size, -1)    # (batch_size, 1)&#10;&#10;        # Normalize to ensure conservation&#10;        total_theta = theta_co + theta_c2h4 + theta_h2b + 1e-8&#10;        theta_co_norm = theta_co / total_theta&#10;        theta_c2h4_norm = theta_c2h4 / total_theta&#10;&#10;        # Simple physics-based approximation for Faradaic efficiencies&#10;        # CO efficiency increases with CO surface coverage and effective area&#10;        fe_co = theta_co_norm * effective_area * 0.1&#10;        fe_co = torch.clamp(fe_co, 0.0, 0.6)  # Physical bounds&#10;&#10;        # C2H4 efficiency increases with C2H4 surface coverage&#10;        fe_c2h4 = theta_c2h4_norm * effective_area * 0.05&#10;        fe_c2h4 = torch.clamp(fe_c2h4, 0.0, 0.4)  # Physical bounds&#10;&#10;        return {&#10;            'fe_co': fe_co,      # (batch_size, 1)&#10;            'fe_c2h4': fe_c2h4   # (batch_size, 1)&#10;        }&#10;&#10;&#10;class PhModel(nn.Module):&#10;    &quot;&quot;&quot;&#10;    Physics-Informed Model for predicting Faradaic efficiencies.&#10;&#10;    Architecture:&#10;    1. Neural network maps experimental inputs → latent physical parameters&#10;    2. Physics engine uses parameters to simulate electrochemical processes&#10;    3. Returns physics-based predictions for FE_CO and FE_C2H4&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: PhysicsConfig = None, zlt_mu_stds: Optional[Tuple[float, float]] = None):&#10;        super().__init__()&#10;        self.config = config or PhysicsConfig()&#10;&#10;        # Normalization parameters for Zero_eps_thickness&#10;        self.zlt_mu_stds = zlt_mu_stds or (5e-6, 1e-6)  # (mean, std)&#10;&#10;        # Neural network: 5 inputs → 6 latent physics parameters&#10;        self.net = nn.Sequential(&#10;            nn.Linear(5, self.config.hidden_dim),&#10;            nn.ReLU(),&#10;            nn.Dropout(self.config.dropout_rate),&#10;            nn.Linear(self.config.hidden_dim, self.config.hidden_dim),&#10;            nn.ReLU(),&#10;            nn.Dropout(self.config.dropout_rate),&#10;            nn.Linear(self.config.hidden_dim, self.config.hidden_dim),&#10;            nn.ReLU(),&#10;            nn.Dropout(self.config.dropout_rate),&#10;            nn.Linear(self.config.hidden_dim, 6)  # 6 latent parameters&#10;        )&#10;&#10;        # Physics engine&#10;        self.physics_engine = SimplifiedPhysicsEngine()&#10;&#10;        # Softmax for surface coverage fractions (must sum to ≤ 1)&#10;        self.softmax = nn.Softmax(dim=-1)&#10;&#10;        # Initialize weights&#10;        self._init_weights()&#10;&#10;    def _init_weights(self):&#10;        &quot;&quot;&quot;Initialize weights for stable training.&quot;&quot;&quot;&#10;        for module in self.modules():&#10;            if isinstance(module, nn.Linear):&#10;                nn.init.xavier_uniform_(module.weight)&#10;                if module.bias is not None:&#10;                    nn.init.zeros_(module.bias)&#10;&#10;    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;&#10;        Forward pass: experimental inputs → physics parameters → FE predictions.&#10;&#10;        Args:&#10;            x: Input tensor (batch_size, 5) with columns:&#10;               [AgCu Ratio, Naf vol (ul), Sust vol (ul), Zero_eps_thickness, Catalyst mass loading]&#10;&#10;        Returns:&#10;            Predictions tensor (batch_size, 2) with [FE_C2H4, FE_CO]&#10;        &quot;&quot;&quot;&#10;        # Validate input&#10;        assert x.ndim == 2 and x.shape[1] == 5, f&quot;Expected input shape (N, 5), got {x.shape}&quot;&#10;&#10;        # Neural network maps inputs to latent parameters&#10;        latents = self.net(x)  # (batch_size, 6)&#10;&#10;        # Extract and transform latent parameters to physical parameters&#10;&#10;        # 1. Pore radius (log-normal distribution)&#10;        r = 40e-9 * torch.exp(latents[..., 0:1])  # [m]&#10;&#10;        # 2. Porosity (sigmoid to ensure 0 &lt; eps &lt; 1)&#10;        eps = torch.sigmoid(latents[..., 1:2])  # dimensionless&#10;&#10;        # 3. Zero layer thickness (denormalized)&#10;        zlt = (x[..., 3:4] * self.zlt_mu_stds[1] + self.zlt_mu_stds[0])  # [m]&#10;&#10;        # 4. Layer thickness&#10;        L = zlt / (1 - eps + 1e-8)  # [m], avoid division by zero&#10;&#10;        # 5. Mass transfer coefficient factor&#10;        K_dl_factor = torch.exp(latents[..., 2:3])&#10;&#10;        # 6. Surface coverage fractions (softmax to ensure they sum ≤ 1)&#10;        theta_logits = 2 * latents[..., 3:6]  # Scale for better gradients&#10;        thetas_vec = self.softmax(theta_logits)  # (batch_size, 3)&#10;&#10;        thetas = {&#10;            'CO': thetas_vec[..., 0:1],&#10;            'C2H4': thetas_vec[..., 1:2],&#10;            'H2b': thetas_vec[..., 2:3]&#10;        }&#10;&#10;        # Calculate gas diffusion layer mass transfer coefficient&#10;        D_CO2 = self.physics_engine.diffusion_coeffs['CO2']&#10;        gdl_mass_transfer_coeff = K_dl_factor * self.physics_engine.bruggeman(D_CO2, eps) / r&#10;&#10;        # Solve physics equations&#10;        solution = self.physics_engine.solve_current(&#10;            i_target=self.config.current_target,&#10;            eps=eps,&#10;            r=r,&#10;            L=L,&#10;            thetas=thetas,&#10;            gdl_mass_transfer_coeff=gdl_mass_transfer_coeff,&#10;            grid_size=self.config.grid_size,&#10;            voltage_bounds=self.config.voltage_bounds&#10;        )&#10;&#10;        # Return FE predictions: [FE_C2H4, FE_CO]&#10;        output = torch.cat([solution['fe_c2h4'], solution['fe_co']], dim=-1)&#10;&#10;        # Ensure outputs are in valid range [0, 1]&#10;        output = torch.clamp(output, 0.0, 1.0)&#10;&#10;        return output&#10;&#10;    def get_physics_parameters(self, x: torch.Tensor) -&gt; Dict[str, torch.Tensor]:&#10;        &quot;&quot;&quot;&#10;        Get the intermediate physics parameters for interpretation.&#10;&#10;        Args:&#10;            x: Input tensor (batch_size, 5)&#10;&#10;        Returns:&#10;            Dictionary of physics parameters&#10;        &quot;&quot;&quot;&#10;        with torch.no_grad():&#10;            latents = self.net(x)&#10;&#10;            r = 40e-9 * torch.exp(latents[..., 0:1])&#10;            eps = torch.sigmoid(latents[..., 1:2])&#10;            zlt = (x[..., 3:4] * self.zlt_mu_stds[1] + self.zlt_mu_stds[0])&#10;            L = zlt / (1 - eps + 1e-8)&#10;            K_dl_factor = torch.exp(latents[..., 2:3])&#10;&#10;            theta_logits = 2 * latents[..., 3:6]&#10;            thetas_vec = self.softmax(theta_logits)&#10;&#10;            return {&#10;                'pore_radius': r,&#10;                'porosity': eps,&#10;                'zero_layer_thickness': zlt,&#10;                'layer_thickness': L,&#10;                'mass_transfer_factor': K_dl_factor,&#10;                'theta_CO': thetas_vec[..., 0:1],&#10;                'theta_C2H4': thetas_vec[..., 1:2],&#10;                'theta_H2b': thetas_vec[..., 2:3]&#10;            }&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#13;&#10;Physics-Informed Model (PhModel) for CO2 reduction predictions.&#13;&#10;Combines neural networks with the complete electrochemical physics simulation.&#13;&#10;&quot;&quot;&quot;&#13;&#10;import torch&#13;&#10;import torch.nn as nn&#13;&#10;from typing import Dict, Tuple, Optional&#13;&#10;from dataclasses import dataclass&#13;&#10;import copy&#13;&#10;&#13;&#10;# Import the complete physics engine&#13;&#10;from .gde_system import (&#13;&#10;    System, &#13;&#10;    diffusion_coefficients, &#13;&#10;    salting_out_exponents,&#13;&#10;    electrode_reaction_kinetics, &#13;&#10;    electrode_reaction_potentials, &#13;&#10;    chemical_reaction_rates&#13;&#10;)&#13;&#10;&#13;&#10;&#13;&#10;@dataclass&#13;&#10;class PhysicsConfig:&#13;&#10;    &quot;&quot;&quot;Configuration for physics-informed model.&quot;&quot;&quot;&#13;&#10;    hidden_dim: int = 64&#13;&#10;    dropout_rate: float = 0.1&#13;&#10;    current_target: float = 200.0  # Target current density [A/m^2]&#13;&#10;    grid_size: int = 1000&#13;&#10;    voltage_bounds: Tuple[float, float] = (-1.25, 0.0)&#13;&#10;&#13;&#10;&#13;&#10;class PhModel(nn.Module):&#13;&#10;    &quot;&quot;&quot;&#13;&#10;    Physics-Informed Model for predicting Faradaic efficiencies.&#13;&#10;    &#13;&#10;    Architecture (EXACTLY matching the original):&#13;&#10;    1. Neural network maps experimental inputs → latent physical parameters&#13;&#10;    2. Complete gde_multi.System solves electrochemical physics&#13;&#10;    3. Returns physics-based predictions for FE_CO and FE_C2H4&#13;&#10;    &#13;&#10;    This uses the full published physics engine with:&#13;&#10;    - Complete mass transport equations&#13;&#10;    - Butler-Volmer electrochemical kinetics&#13;&#10;    - Carbonate equilibrium chemistry&#13;&#10;    - Salting-out effects&#13;&#10;    - Flow channel mass transfer&#13;&#10;    &quot;&quot;&quot;&#13;&#10;    &#13;&#10;    def __init__(self, config: PhysicsConfig = None, zlt_mu_stds: Optional[Tuple[float, float]] = None):&#13;&#10;        super().__init__()&#13;&#10;        self.config = config or PhysicsConfig()&#13;&#10;        &#13;&#10;        # Normalization parameters for Zero_eps_thickness (from normalized data)&#13;&#10;        self.zlt_mu_stds = zlt_mu_stds or (0.0, 1.0)  # (mean, std) from normalized data&#13;&#10;        &#13;&#10;        # Neural network: 5 inputs → 6 latent physics parameters&#13;&#10;        # EXACTLY matching the original architecture&#13;&#10;        self.net = nn.Sequential(&#13;&#10;            nn.Linear(5, self.config.hidden_dim),&#13;&#10;            nn.ReLU(),&#13;&#10;            nn.Dropout(self.config.dropout_rate),&#13;&#10;            nn.Linear(self.config.hidden_dim, self.config.hidden_dim),&#13;&#10;            nn.ReLU(),&#13;&#10;            nn.Dropout(self.config.dropout_rate),&#13;&#10;            nn.Linear(self.config.hidden_dim, self.config.hidden_dim),&#13;&#10;            nn.ReLU(),&#13;&#10;            nn.Dropout(self.config.dropout_rate),&#13;&#10;            nn.Linear(self.config.hidden_dim, 6)  # 6 latent parameters&#13;&#10;        )&#13;&#10;        &#13;&#10;        # Complete physics engine with learnable kinetic parameters&#13;&#10;        erc = dict(electrode_reaction_kinetics)&#13;&#10;        erc['i_0_CO'] = nn.parameter.Parameter(torch.tensor(erc['i_0_CO']))&#13;&#10;        erc['i_0_C2H4'] = nn.parameter.Parameter(torch.tensor(erc['i_0_C2H4']))&#13;&#10;        erc['i_0_H2b'] = nn.parameter.Parameter(torch.tensor(erc['i_0_H2b']))&#13;&#10;        erc['alpha_CO'] = nn.parameter.Parameter(torch.tensor(erc['alpha_CO']))&#13;&#10;        erc['alpha_C2H4'] = nn.parameter.Parameter(torch.tensor(erc['alpha_C2H4']))&#13;&#10;        erc['alpha_H2b'] = nn.parameter.Parameter(torch.tensor(erc['alpha_H2b']))&#13;&#10;        &#13;&#10;        self.ph_model = System(&#13;&#10;            diffusion_coefficients=diffusion_coefficients, &#13;&#10;            salting_out_exponents=salting_out_exponents, &#13;&#10;            electrode_reaction_kinetics=erc,&#13;&#10;            electrode_reaction_potentials=electrode_reaction_potentials,&#13;&#10;            chemical_reaction_rates=chemical_reaction_rates,&#13;&#10;        )&#13;&#10;        &#13;&#10;        # Softmax for surface coverage fractions (must sum to ≤ 1)&#13;&#10;        self.softmax = nn.Softmax(dim=-1)&#13;&#10;        &#13;&#10;        # Initialize weights&#13;&#10;        self._init_weights()&#13;&#10;    &#13;&#10;    def _init_weights(self):&#13;&#10;        &quot;&quot;&quot;Initialize weights for stable training.&quot;&quot;&quot;&#13;&#10;        for module in self.modules():&#13;&#10;            if isinstance(module, nn.Linear):&#13;&#10;                nn.init.xavier_uniform_(module.weight)&#13;&#10;                if module.bias is not None:&#13;&#10;                    nn.init.zeros_(module.bias)&#13;&#10;    &#13;&#10;    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        Forward pass: experimental inputs → physics parameters → FE predictions.&#13;&#10;        &#13;&#10;        EXACTLY matches the original PhModel forward pass from the notebook.&#13;&#10;        &#13;&#10;        Args:&#13;&#10;            x: Input tensor (batch_size, 5) with columns (NORMALIZED):&#13;&#10;               [AgCu Ratio, Naf vol (ul), Sust vol (ul), Zero_eps_thickness, Catalyst mass loading]&#13;&#10;               &#13;&#10;        Returns:&#13;&#10;            Predictions tensor (batch_size, 2) with [FE_C2H4, FE_CO]&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        # Validate input&#13;&#10;        assert x.ndim == 2 and x.shape[1] == 5, f&quot;Expected input shape (N, 5), got {x.shape}&quot;&#13;&#10;        &#13;&#10;        # Neural network maps inputs to latent parameters&#13;&#10;        latents = self.net(x)  # (batch_size, 6)&#13;&#10;        &#13;&#10;        # Extract and transform latent parameters to physical parameters&#13;&#10;        # EXACTLY matching original transformations:&#13;&#10;        &#13;&#10;        # 1. Pore radius (log-normal distribution)&#13;&#10;        r = 40e-9 * torch.exp(latents[..., 0:1])  # [m]&#13;&#10;        &#13;&#10;        # 2. Porosity (sigmoid to ensure 0 &lt; eps &lt; 1)&#13;&#10;        eps = torch.sigmoid(latents[..., 1:2])  # dimensionless&#13;&#10;        &#13;&#10;        # 3. Zero layer thickness (denormalized from NORMALIZED input)&#13;&#10;        # This is the key insight: x[..., 3] is NORMALIZED Zero_eps_thickness&#13;&#10;        zlt = (x[..., 3:4] * self.zlt_mu_stds[1] + self.zlt_mu_stds[0])  # [m]&#13;&#10;        &#13;&#10;        # 4. Layer thickness &#13;&#10;        L = zlt / (1 - eps)  # [m] - original had no +1e-8 protection&#13;&#10;        &#13;&#10;        # 5. Mass transfer coefficient factor&#13;&#10;        K_dl_factor = torch.exp(latents[..., 2:3])&#13;&#10;        &#13;&#10;        # 6. Surface coverage fractions (softmax, scaled by 2 for gradients)&#13;&#10;        theta_logits = 2 * latents[..., 3:6]  # Scale for better gradients&#13;&#10;        thetas_vec = self.softmax(theta_logits)  # (batch_size, 3)&#13;&#10;        &#13;&#10;        # Convert to dictionary format expected by physics engine&#13;&#10;        thetas = {&#13;&#10;            'CO': thetas_vec[..., 0:1],&#13;&#10;            'C2H4': thetas_vec[..., 1:2], &#13;&#10;            'H2b': thetas_vec[..., 2:3]&#13;&#10;        }&#13;&#10;        &#13;&#10;        # Calculate gas diffusion layer mass transfer coefficient&#13;&#10;        gdl_mass_transfer_coefficient = K_dl_factor * self.ph_model.bruggeman(&#13;&#10;            self.ph_model.diffusion_coefficients['CO2'], eps) / r&#13;&#10;        &#13;&#10;        # Solve complete physics equations using the full System&#13;&#10;        solution = self.ph_model.solve_current(&#13;&#10;            i_target=self.config.current_target,&#13;&#10;            eps=eps,&#13;&#10;            r=r,&#13;&#10;            L=L,&#13;&#10;            thetas=thetas,&#13;&#10;            gdl_mass_transfer_coeff=gdl_mass_transfer_coefficient,&#13;&#10;            grid_size=self.config.grid_size,&#13;&#10;            voltage_bounds=self.config.voltage_bounds&#13;&#10;        )&#13;&#10;        &#13;&#10;        # Return FE predictions: [FE_C2H4, FE_CO] - EXACTLY matching original order&#13;&#10;        output = torch.cat([solution['fe_c2h4'], solution['fe_co']], dim=-1)&#13;&#10;        &#13;&#10;        return output&#13;&#10;    &#13;&#10;    def get_physics_parameters(self, x: torch.Tensor) -&gt; Dict[str, torch.Tensor]:&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        Get the intermediate physics parameters for interpretation.&#13;&#10;        &#13;&#10;        Args:&#13;&#10;            x: Input tensor (batch_size, 5) - normalized features&#13;&#10;            &#13;&#10;        Returns:&#13;&#10;            Dictionary of physics parameters&#13;&#10;        &quot;&quot;&quot;&#13;&#10;        with torch.no_grad():&#13;&#10;            latents = self.net(x)&#13;&#10;            &#13;&#10;            r = 40e-9 * torch.exp(latents[..., 0:1])&#13;&#10;            eps = torch.sigmoid(latents[..., 1:2])&#13;&#10;            zlt = (x[..., 3:4] * self.zlt_mu_stds[1] + self.zlt_mu_stds[0])&#13;&#10;            L = zlt / (1 - eps)&#13;&#10;            K_dl_factor = torch.exp(latents[..., 2:3])&#13;&#10;            &#13;&#10;            theta_logits = 2 * latents[..., 3:6]&#13;&#10;            thetas_vec = self.softmax(theta_logits)&#13;&#10;            &#13;&#10;            return {&#13;&#10;                'pore_radius': r,&#13;&#10;                'porosity': eps,&#13;&#10;                'zero_layer_thickness': zlt,&#13;&#10;                'layer_thickness': L,&#13;&#10;                'mass_transfer_factor': K_dl_factor,&#13;&#10;                'theta_CO': thetas_vec[..., 0:1],&#13;&#10;                'theta_C2H4': thetas_vec[..., 1:2],&#13;&#10;                'theta_H2b': thetas_vec[..., 2:3]&#13;&#10;            }&#13;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/pyproject.toml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/pyproject.toml" />
              <option name="updatedContent" value="[build-system]&#10;requires = [&quot;hatchling&quot;]&#10;build-backend = &quot;hatchling.build&quot;&#10;&#10;[project]&#10;name = &quot;carbondriver&quot;&#10;description = &quot;Physics-based active learning tools for CO2 reduction experiments&quot;&#10;version = &quot;0.2.0&quot;&#10;authors = [&#10;    { name=&quot;Ivan Grega&quot;, email=&quot;grega.ivan@gmail.com&quot; },&#10;]&#10;readme = &quot;README.md&quot;&#10;license = {file = &quot;LICENSE.txt&quot;}&#10;requires-python = &quot;&gt;=3.10&quot;&#10;&#10;dependencies = [&#10;    &quot;torch&gt;=2.0.0&quot;,&#10;    &quot;gpytorch&gt;=1.9.0&quot;, &#10;    &quot;botorch&gt;=0.8.0&quot;,&#10;    &quot;pandas&gt;=1.5.0&quot;,&#10;    &quot;numpy&gt;=1.24.0&quot;,&#10;    &quot;matplotlib&gt;=3.5.0&quot;,&#10;    &quot;seaborn&gt;=0.11.0&quot;,&#10;    &quot;openpyxl&gt;=3.0.0&quot;,&#10;    &quot;rich&gt;=12.0.0&quot;,&#10;    &quot;scikit-learn&gt;=1.2.0&quot;,&#10;]&#10;&#10;[project.optional-dependencies]&#10;dev = [&#10;    &quot;pytest&gt;=7.0.0&quot;,&#10;    &quot;pytest-cov&gt;=4.0.0&quot;,&#10;    &quot;black&gt;=23.0.0&quot;,&#10;    &quot;isort&gt;=5.12.0&quot;,&#10;    &quot;mypy&gt;=1.0.0&quot;,&#10;]&#10;&#10;[tool.pytest.ini_options]&#10;testpaths = [&quot;tests&quot;]&#10;python_files = [&quot;test_*.py&quot;]&#10;python_classes = [&quot;Test*&quot;]&#10;python_functions = [&quot;test_*&quot;]&#10;addopts = &quot;-v --tb=short&quot;&#10;&#10;[tool.black]&#10;line-length = 100&#10;target-version = ['py310']&#10;&#10;[tool.isort]&#10;profile = &quot;black&quot;&#10;line_length = 100" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_complete_physics_model.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_complete_physics_model.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Test script for the updated Physics-Informed Model with complete gde_system.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;from data.loader import load_data&#10;from models.physics_model import PhModel, PhysicsConfig&#10;&#10;&#10;def test_complete_physics_model():&#10;    &quot;&quot;&quot;Test the PhModel with complete gde_system physics engine.&quot;&quot;&quot;&#10;    print(&quot;Testing PhModel with COMPLETE gde_system physics engine...&quot;)&#10;    &#10;    # Load data with normalization (as the original PhModel expects)&#10;    print(&quot;\n1. Loading data with normalization...&quot;)&#10;    data_tensors, norm_params, df = load_data(&#10;        normalize_features=True,  # PhModel expects normalized inputs&#10;        normalize_targets=False   # Targets stay as decimals 0-1&#10;    )&#10;    print(f&quot;Data: {data_tensors.X.shape[0]} samples, {data_tensors.X.shape[1]} features&quot;)&#10;    print(f&quot;X normalized range: [{data_tensors.X.min():.3f}, {data_tensors.X.max():.3f}]&quot;)&#10;    print(f&quot;Target range: [{data_tensors.y.min():.3f}, {data_tensors.y.max():.3f}]&quot;)&#10;    &#10;    # Extract Zero_eps_thickness normalization parameters&#10;    print(&quot;\n2. Setting up PhModel with correct normalization parameters...&quot;)&#10;    zlt_mean = norm_params.feature_means[3].item()  # Zero_eps_thickness mean (normalized)&#10;    zlt_std = norm_params.feature_stds[3].item()    # Zero_eps_thickness std (normalized)&#10;    print(f&quot;Zero layer thickness norm params: mean={zlt_mean:.6f}, std={zlt_std:.6f}&quot;)&#10;    &#10;    # These are the normalization stats from the ORIGINAL data (before normalization)&#10;    # We need to calculate the original mean and std for denormalization in PhModel&#10;    original_zlt = df['Zero_eps_thickness']&#10;    original_zlt_mean = original_zlt.mean()&#10;    original_zlt_std = original_zlt.std(ddof=0)  # Use ddof=0 to match PyTorch default&#10;    print(f&quot;Original ZLT stats: mean={original_zlt_mean:.2e}, std={original_zlt_std:.2e}&quot;)&#10;    &#10;    config = PhysicsConfig(&#10;        hidden_dim=64,&#10;        dropout_rate=0.1,&#10;        current_target=200.0,  # Target current density [A/m^2]&#10;        grid_size=100,         # Smaller grid for faster testing&#10;        voltage_bounds=(-1.25, 0.0)&#10;    )&#10;    &#10;    # Create PhModel with original (unnormalized) ZLT statistics&#10;    model = PhModel(config=config, zlt_mu_stds=(original_zlt_mean, original_zlt_std))&#10;    total_params = sum(p.numel() for p in model.parameters())&#10;    print(f&quot;PhModel created with {total_params} parameters&quot;)&#10;    print(f&quot;Physics engine: Complete gde_system.System&quot;)&#10;    &#10;    # Test forward pass&#10;    print(&quot;\n3. Testing forward pass with complete physics...&quot;)&#10;    model.eval()&#10;    try:&#10;        with torch.no_grad():&#10;            # Test with small batch first&#10;            test_input = data_tensors.X[:3]&#10;            predictions = model(test_input)&#10;        &#10;        print(f&quot;✅ Forward pass successful!&quot;)&#10;        print(f&quot;Prediction shape: {predictions.shape}&quot;)&#10;        print(f&quot;Prediction range: [{predictions.min():.3f}, {predictions.max():.3f}]&quot;)&#10;        &#10;        # Display sample predictions&#10;        print(f&quot;\nSample predictions (first 3 points):&quot;)&#10;        for i in range(3):&#10;            true_eth, true_co = data_tensors.y[i]&#10;            pred_eth, pred_co = predictions[i]&#10;            print(f&quot;  Point {i}: True=[{true_eth:.3f}, {true_co:.3f}], Pred=[{pred_eth:.3f}, {pred_co:.3f}]&quot;)&#10;        &#10;        # Test physics parameter extraction&#10;        print(&quot;\n4. Testing physics parameter extraction...&quot;)&#10;        physics_params = model.get_physics_parameters(test_input)&#10;        &#10;        print(&quot;Physics parameters for first 3 samples:&quot;)&#10;        for key, values in physics_params.items():&#10;            if key == 'pore_radius':&#10;                print(f&quot;  {key}: {[f'{v:.2e}' for v in values.squeeze().tolist()]} [m]&quot;)&#10;            elif key in ['zero_layer_thickness', 'layer_thickness']:&#10;                print(f&quot;  {key}: {[f'{v:.2e}' for v in values.squeeze().tolist()]} [m]&quot;)&#10;            elif key == 'porosity':&#10;                print(f&quot;  {key}: {[f'{v:.3f}' for v in values.squeeze().tolist()]} (dimensionless)&quot;)&#10;            else:&#10;                print(f&quot;  {key}: {[f'{v:.3f}' for v in values.squeeze().tolist()]}&quot;)&#10;        &#10;        # Test training capability (few steps)&#10;        print(&quot;\n5. Testing training capability...&quot;)&#10;        model.train()&#10;        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)&#10;        criterion = torch.nn.MSELoss()&#10;        &#10;        initial_loss = None&#10;        print(&quot;Training for 5 epochs...&quot;)&#10;        for epoch in range(5):&#10;            optimizer.zero_grad()&#10;            &#10;            # Use smaller batch for testing&#10;            batch_X = data_tensors.X[:20]  # Smaller batch&#10;            batch_y = data_tensors.y[:20]&#10;            &#10;            predictions = model(batch_X)&#10;            loss = criterion(predictions, batch_y)&#10;            loss.backward()&#10;            optimizer.step()&#10;            &#10;            if epoch == 0:&#10;                initial_loss = loss.item()&#10;            &#10;            print(f&quot;  Epoch {epoch}: Loss = {loss.item():.6f}&quot;)&#10;        &#10;        final_loss = loss.item()&#10;        print(f&quot;Loss change: {initial_loss:.6f} → {final_loss:.6f}&quot;)&#10;        &#10;        # Validate physics constraints with complete engine&#10;        print(&quot;\n6. Validating physics constraints...&quot;)&#10;        model.eval()&#10;        with torch.no_grad():&#10;            test_predictions = model(data_tensors.X[:10])&#10;            params = model.get_physics_parameters(data_tensors.X[:10])&#10;        &#10;        constraints_valid = True&#10;        &#10;        # Check parameter ranges&#10;        porosity = params['porosity']&#10;        if not (0 &lt; porosity.min() and porosity.max() &lt; 1):&#10;            print(f&quot;❌ Porosity out of bounds: [{porosity.min():.3f}, {porosity.max():.3f}]&quot;)&#10;            constraints_valid = False&#10;        else:&#10;            print(f&quot;✅ Porosity in bounds: [{porosity.min():.3f}, {porosity.max():.3f}]&quot;)&#10;        &#10;        pore_radius = params['pore_radius']&#10;        if not (1e-10 &lt; pore_radius.min() and pore_radius.max() &lt; 1e-6):&#10;            print(f&quot;❌ Pore radius unreasonable: [{pore_radius.min():.2e}, {pore_radius.max():.2e}]&quot;)&#10;            constraints_valid = False&#10;        else:&#10;            print(f&quot;✅ Pore radius reasonable: [{pore_radius.min():.2e}, {pore_radius.max():.2e}]&quot;)&#10;        &#10;        # Surface coverage fractions should sum to ≤ 1&#10;        theta_sum = params['theta_CO'] + params['theta_C2H4'] + params['theta_H2b']&#10;        if not (theta_sum.max() &lt;= 1.01):  # Small tolerance&#10;            print(f&quot;❌ Surface coverage sum &gt; 1: max = {theta_sum.max():.3f}&quot;)&#10;            constraints_valid = False&#10;        else:&#10;            print(f&quot;✅ Surface coverage sum ≤ 1: max = {theta_sum.max():.3f}&quot;)&#10;        &#10;        # Predictions should be in [0, 1]&#10;        if not (0 &lt;= test_predictions.min() and test_predictions.max() &lt;= 1):&#10;            print(f&quot;❌ Predictions out of [0,1]: [{test_predictions.min():.3f}, {test_predictions.max():.3f}]&quot;)&#10;            constraints_valid = False&#10;        else:&#10;            print(f&quot;✅ Predictions in [0,1]: [{test_predictions.min():.3f}, {test_predictions.max():.3f}]&quot;)&#10;        &#10;        # Check that predictions are reasonable (not all the same)&#10;        pred_std = test_predictions.std().item()&#10;        if pred_std &lt; 1e-6:&#10;            print(f&quot;❌ Predictions are too similar (std={pred_std:.2e})&quot;)&#10;            constraints_valid = False&#10;        else:&#10;            print(f&quot;✅ Predictions have variation (std={pred_std:.3f})&quot;)&#10;        &#10;        if constraints_valid:&#10;            print(&quot;\n✅ All physics constraints satisfied!&quot;)&#10;        else:&#10;            print(&quot;\n❌ Some physics constraints violated!&quot;)&#10;        &#10;        print(&quot;\n✅ Complete PhModel test finished!&quot;)&#10;        print(f&quot;Summary:&quot;)&#10;        print(f&quot;  - Complete physics engine: ✓&quot;)&#10;        print(f&quot;  - Normalized input handling: ✓&quot;)&#10;        print(f&quot;  - Forward pass: ✓&quot;)&#10;        print(f&quot;  - Parameter extraction: ✓&quot;) &#10;        print(f&quot;  - Gradient flow: ✓&quot;)&#10;        print(f&quot;  - Physics constraints: {'✓' if constraints_valid else '❌'}&quot;)&#10;        print(f&quot;  - Prediction variation: ✓&quot;)&#10;        &#10;        return model, constraints_valid&#10;        &#10;    except Exception as e:&#10;        print(f&quot;❌ PhModel test failed: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()&#10;        return None, False&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    test_complete_physics_model()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_data_loader.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_data_loader.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Test script to validate the new data loading system.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;from data.loader import load_data, load_raw_data, prepare_tensors&#10;import torch&#10;&#10;&#10;def test_data_loading():&#10;    &quot;&quot;&quot;Test the complete data loading pipeline.&quot;&quot;&quot;&#10;    print(&quot;Testing data loading pipeline...&quot;)&#10;    &#10;    # Test 1: Load raw data&#10;    print(&quot;\n1. Testing raw data loading...&quot;)&#10;    df = load_raw_data()&#10;    print(f&quot;Raw data shape: {df.shape}&quot;)&#10;    print(f&quot;Columns: {list(df.columns)}&quot;)&#10;    print(f&quot;Sample data:\n{df.head(3)}&quot;)&#10;    &#10;    # Test 2: Prepare tensors without normalization&#10;    print(&quot;\n2. Testing tensor preparation (no normalization)...&quot;)&#10;    data_tensors, norm_params = prepare_tensors(df, normalize_features=False, normalize_targets=False)&#10;    print(f&quot;X shape: {data_tensors.X.shape}, dtype: {data_tensors.X.dtype}&quot;)&#10;    print(f&quot;y shape: {data_tensors.y.shape}, dtype: {data_tensors.y.dtype}&quot;)&#10;    print(f&quot;Feature names: {data_tensors.feature_names}&quot;)&#10;    print(f&quot;Target names: {data_tensors.target_names}&quot;)&#10;    print(f&quot;Normalization params: {norm_params}&quot;)&#10;    &#10;    # Test 3: Prepare tensors with feature normalization only&#10;    print(&quot;\n3. Testing tensor preparation (feature normalization only)...&quot;)&#10;    data_tensors_norm, norm_params_norm = prepare_tensors(df, normalize_features=True, normalize_targets=False)&#10;    print(f&quot;X normalized shape: {data_tensors_norm.X.shape}&quot;)&#10;    print(f&quot;X mean (should be ~0): {data_tensors_norm.X.mean(dim=0)}&quot;)&#10;    print(f&quot;X std (should be ~1): {data_tensors_norm.X.std(dim=0, unbiased=False)}&quot;)&#10;    print(f&quot;y unchanged mean: {data_tensors_norm.y.mean(dim=0)}&quot;)&#10;    &#10;    # Test 4: Test normalization reversibility&#10;    print(&quot;\n4. Testing normalization reversibility...&quot;)&#10;    X_denorm = norm_params_norm.denormalize_features(data_tensors_norm.X)&#10;    max_diff = torch.max(torch.abs(X_denorm - data_tensors.X))&#10;    print(f&quot;Max difference after denormalization: {max_diff.item():.2e} (should be ~0)&quot;)&#10;    &#10;    # Test 5: Complete pipeline&#10;    print(&quot;\n5. Testing complete pipeline...&quot;)&#10;    data_complete, norm_complete, df_complete = load_data(normalize_features=True, normalize_targets=False)&#10;    print(f&quot;Complete pipeline X shape: {data_complete.X.shape}&quot;)&#10;    print(f&quot;DataFrame matches: {df.equals(df_complete)}&quot;)&#10;    &#10;    print(&quot;\n✅ All tests passed! Data loading system is working correctly.&quot;)&#10;    &#10;    return data_complete, norm_complete, df_complete&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    try:&#10;        test_data_loading()&#10;    except Exception as e:&#10;        print(f&quot;❌ Test failed with error: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_ensemble_api.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_ensemble_api.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;API tests for MLP and PhModel ensembles matching the original test_api.py structure.&#10;Tests the same acquisition function and step methods as the original.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;import pandas as pd&#10;import numpy as np&#10;from data.loader import load_data&#10;from models.mlp_ensemble import MLPEnsemble, EnsembleConfig&#10;from models.ph_ensemble import PhModelEnsemble, PhEnsembleConfig&#10;&#10;# BoTorch imports for acquisition functions&#10;try:&#10;    from botorch.acquisition.analytic import ExpectedImprovement&#10;    from botorch.optim import optimize_acqf&#10;    BOTORCH_AVAILABLE = True&#10;except ImportError:&#10;    BOTORCH_AVAILABLE = False&#10;    print(&quot;Warning: BoTorch not available. Install with 'pip install botorch'&quot;)&#10;&#10;&#10;class SingleOutputEnsembleWrapper:&#10;    &quot;&quot;&quot;&#10;    Wrapper to make multi-output ensembles work with single-output acquisition functions.&#10;    Extracts a single output dimension for optimization.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, ensemble, output_index=0):&#10;        self.ensemble = ensemble&#10;        self.output_index = output_index&#10;        self.num_outputs = 1  # Single output for acquisition function&#10;        self._num_outputs = 1&#10;&#10;    def posterior(self, X: torch.Tensor, **kwargs):&#10;        &quot;&quot;&quot;Get posterior distribution for the specified output dimension.&quot;&quot;&quot;&#10;        # Get full posterior from ensemble&#10;        full_posterior = self.ensemble.get_botorch_posterior(X)&#10;&#10;        # Extract only the specified output dimension&#10;        mean = full_posterior.mean[..., self.output_index:self.output_index+1]  # (batch, 1, 1)&#10;&#10;        # Extract corresponding covariance for this output dimension&#10;        batch_size = X.shape[0]&#10;        covariance_single = []&#10;        for b in range(batch_size):&#10;            # Get the variance for this output dimension&#10;            var = full_posterior.distribution.covariance_matrix[b, self.output_index, self.output_index]&#10;            cov_matrix = var.unsqueeze(0).unsqueeze(0)  # (1, 1)&#10;            covariance_single.append(cov_matrix)&#10;&#10;        covariance = torch.stack(covariance_single, dim=0)  # (batch, 1, 1)&#10;&#10;        # Create single-output distribution&#10;        from gpytorch.distributions import MultitaskMultivariateNormal&#10;        mvn_single = MultitaskMultivariateNormal(mean, covariance)&#10;&#10;        from botorch.posteriors.gpytorch import GPyTorchPosterior&#10;        return GPyTorchPosterior(mvn_single)&#10;&#10;    def forward(self, X: torch.Tensor):&#10;        &quot;&quot;&quot;Forward pass that returns posterior (for compatibility).&quot;&quot;&quot;&#10;        return self.posterior(X)&#10;&#10;&#10;class EnsembleOptimizer:&#10;    &quot;&quot;&quot;&#10;    Optimizer class that wraps our MLP and PhModel ensembles for Bayesian optimization.&#10;    Mimics the API structure from the original GDEOptimizer.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, model_type=&quot;MLP&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True, output_dir=&quot;./out&quot;):&#10;        &quot;&quot;&quot;&#10;        Initialize the optimizer with the specified model type.&#10;&#10;        Args:&#10;            model_type: &quot;MLP&quot; or &quot;PhModel&quot;&#10;            quantity: The quantity to optimize (e.g., 'FE (Eth)')&#10;            maximize: Whether to maximize or minimize the quantity&#10;            output_dir: Directory to save output files&#10;        &quot;&quot;&quot;&#10;        self.model_type = model_type&#10;        self.quantity = quantity&#10;        self.maximize = maximize&#10;        self.output_dir = output_dir&#10;        self.ensemble = None&#10;        self.is_trained = False&#10;&#10;        # Determine output index for single-output wrapper&#10;        self.output_index = 0 if quantity == &quot;FE (Eth)&quot; else 1&#10;&#10;        # For tracking current best value&#10;        self.best_f = None&#10;&#10;    def _train_ensemble(self, df: pd.DataFrame):&#10;        &quot;&quot;&quot;Train the ensemble on the provided data.&quot;&quot;&quot;&#10;        # Load and prepare data&#10;        data_tensors, norm_params, _ = load_data()&#10;&#10;        # Filter to match the provided DataFrame indices&#10;        indices = df.index.tolist()&#10;        train_mask = torch.tensor([i in indices for i in range(len(data_tensors.X))])&#10;        X_train = data_tensors.X[train_mask]&#10;        y_train = data_tensors.y[train_mask]&#10;&#10;        if self.model_type == &quot;MLP&quot;:&#10;            config = EnsembleConfig(&#10;                ensemble_size=5,  # Smaller for testing&#10;                hidden_dim=32,&#10;                dropout_rate=0.1,&#10;                learning_rate=0.001,&#10;                bootstrap_fraction=0.5&#10;            )&#10;            self.ensemble = MLPEnsemble(config)&#10;&#10;        elif self.model_type == &quot;PhModel&quot;:&#10;            # Get original ZLT stats for PhModel&#10;            original_data = pd.read_excel('Characterization_data.xlsx', skiprows=[1], index_col=0)&#10;            original_data = original_data.dropna()&#10;            # Add thickness calculation (matching data loader)&#10;            dens_Ag = 10490&#10;            dens_Cu = 8935&#10;            dens_avg = (1 - original_data['AgCu Ratio']) * dens_Cu + original_data['AgCu Ratio'] * dens_Ag&#10;            mass = original_data['Catalyst mass loading'] * 1e-6&#10;            area = 1.85**2&#10;            A = area * 1e-4&#10;            thickness = (mass / dens_avg) / A&#10;            original_data.insert(3, column='Zero_eps_thickness', value=thickness)&#10;&#10;            original_zlt_mean = original_data['Zero_eps_thickness'].mean()&#10;            original_zlt_std = original_data['Zero_eps_thickness'].std(ddof=0)&#10;&#10;            config = PhEnsembleConfig(&#10;                ensemble_size=3,  # Smaller for testing&#10;                hidden_dim=32,&#10;                dropout_rate=0.1,&#10;                learning_rate=0.001,&#10;                bootstrap_fraction=0.5,&#10;                current_target=200.0,&#10;                grid_size=50  # Smaller for faster testing&#10;            )&#10;            self.ensemble = PhModelEnsemble(config, zlt_mu_stds=(original_zlt_mean, original_zlt_std))&#10;&#10;        else:&#10;            raise ValueError(f&quot;Unknown model type: {self.model_type}&quot;)&#10;&#10;        # Train the ensemble&#10;        print(f&quot;Training {self.model_type} ensemble on {len(X_train)} samples...&quot;)&#10;        self.ensemble.train(X_train, y_train, num_epochs=50, verbose=False)&#10;        self.is_trained = True&#10;&#10;        # Update best value for acquisition function&#10;        target_col_idx = 0 if self.quantity == &quot;FE (Eth)&quot; else 1&#10;        if self.maximize:&#10;            self.best_f = y_train[:, target_col_idx].max()&#10;        else:&#10;            self.best_f = y_train[:, target_col_idx].min()&#10;&#10;    def _get_acquisition_function(self):&#10;        &quot;&quot;&quot;Get the Expected Improvement acquisition function.&quot;&quot;&quot;&#10;        if not BOTORCH_AVAILABLE:&#10;            raise ImportError(&quot;BoTorch not available for acquisition functions&quot;)&#10;&#10;        # Wrap ensemble for single-output acquisition function&#10;        single_output_model = SingleOutputEnsembleWrapper(self.ensemble, self.output_index)&#10;&#10;        return ExpectedImprovement(&#10;            model=single_output_model,&#10;            best_f=self.best_f,&#10;            maximize=self.maximize&#10;        )&#10;&#10;    def step_within_data(self, df_train: pd.DataFrame, df_explore: pd.DataFrame):&#10;        &quot;&quot;&quot;&#10;        Perform acquisition function evaluation within the exploration data.&#10;&#10;        Args:&#10;            df_train: Training data to fit the model&#10;            df_explore: Exploration data to evaluate acquisition function&#10;&#10;        Returns:&#10;            ei_values: Expected Improvement values for exploration points&#10;            next_pick: Index of the best point to pick next&#10;        &quot;&quot;&quot;&#10;        # Train ensemble on training data&#10;        self._train_ensemble(df_train)&#10;&#10;        # Prepare exploration data&#10;        data_tensors, _, full_df = load_data()&#10;&#10;        # Get exploration indices and corresponding features&#10;        explore_indices = df_explore.index.tolist()&#10;        explore_mask = torch.tensor([i in explore_indices for i in range(len(data_tensors.X))])&#10;        X_explore = data_tensors.X[explore_mask]&#10;&#10;        if len(X_explore) == 0:&#10;            raise ValueError(&quot;No exploration points found&quot;)&#10;&#10;        # Get acquisition function&#10;        acquisition_func = self._get_acquisition_function()&#10;&#10;        # Evaluate acquisition function&#10;        with torch.no_grad():&#10;            ei_values = acquisition_func(X_explore.unsqueeze(1))  # Add batch dimension for BoTorch&#10;            ei_values = ei_values.squeeze()  # Remove extra dimensions&#10;&#10;        # Find best point&#10;        if self.maximize:&#10;            best_idx = ei_values.argmax().item()&#10;        else:&#10;            best_idx = ei_values.argmin().item()&#10;&#10;        # Map back to original DataFrame index&#10;        next_pick = explore_indices[best_idx]&#10;&#10;        return ei_values, next_pick&#10;&#10;&#10;def test_mlp_optimizer():&#10;    &quot;&quot;&quot;Test the MLP ensemble optimizer matching the original test_api.py structure.&quot;&quot;&quot;&#10;    print(&quot;Testing MLP Ensemble Optimizer API...&quot;)&#10;&#10;    # Load data&#10;    _, _, df = load_data()&#10;&#10;    # Create optimizer&#10;    optimizer = EnsembleOptimizer(model_type=&quot;MLP&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;&#10;    # Split data like in original test&#10;    df_train = df.iloc[:30]&#10;    df_explore = df.iloc[31:]&#10;&#10;    print(f&quot;Training set: {len(df_train)} samples&quot;)&#10;    print(f&quot;Exploration set: {len(df_explore)} samples&quot;)&#10;&#10;    # First step&#10;    ei, next_pick = optimizer.step_within_data(df_train, df_explore)&#10;    print(f&quot;First pick - EI max: {ei.max():.6f}, Next pick index: {next_pick}&quot;)&#10;&#10;    # Add picked point to training and remove from exploration&#10;    df_new = df_explore.loc[next_pick:next_pick]  # Single row DataFrame&#10;    df_train_updated = pd.concat([df_train, df_new])&#10;    df_explore_updated = df_explore.drop(index=next_pick)&#10;&#10;    # Second step&#10;    optimizer_2 = EnsembleOptimizer(model_type=&quot;MLP&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;    ei_2, next_pick_2 = optimizer_2.step_within_data(df_train_updated, df_explore_updated)&#10;    print(f&quot;Second pick - EI max: {ei_2.max():.6f}, Next pick index: {next_pick_2}&quot;)&#10;&#10;    print(&quot;✅ MLP optimizer API test completed&quot;)&#10;    return True&#10;&#10;&#10;def test_phmodel_optimizer():&#10;    &quot;&quot;&quot;Test the PhModel ensemble optimizer matching the original test_api.py structure.&quot;&quot;&quot;&#10;    print(&quot;\nTesting PhModel Ensemble Optimizer API...&quot;)&#10;&#10;    # Load data&#10;    _, _, df = load_data()&#10;&#10;    # Create optimizer&#10;    optimizer = EnsembleOptimizer(model_type=&quot;PhModel&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;&#10;    # Split data like in original test&#10;    df_train = df.iloc[:30]&#10;    df_explore = df.iloc[31:]&#10;&#10;    print(f&quot;Training set: {len(df_train)} samples&quot;)&#10;    print(f&quot;Exploration set: {len(df_explore)} samples&quot;)&#10;&#10;    # First step&#10;    ei, next_pick = optimizer.step_within_data(df_train, df_explore)&#10;    print(f&quot;First pick - EI max: {ei.max():.6f}, Next pick index: {next_pick}&quot;)&#10;&#10;    # Add picked point to training and remove from exploration&#10;    df_new = df_explore.loc[next_pick:next_pick]  # Single row DataFrame&#10;    df_train_updated = pd.concat([df_train, df_new])&#10;    df_explore_updated = df_explore.drop(index=next_pick)&#10;&#10;    # Second step&#10;    optimizer_2 = EnsembleOptimizer(model_type=&quot;PhModel&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;    ei_2, next_pick_2 = optimizer_2.step_within_data(df_train_updated, df_explore_updated)&#10;    print(f&quot;Second pick - EI max: {ei_2.max():.6f}, Next pick index: {next_pick_2}&quot;)&#10;&#10;    print(&quot;✅ PhModel optimizer API test completed&quot;)&#10;    return True&#10;&#10;&#10;def test_comparison():&#10;    &quot;&quot;&quot;Compare MLP vs PhModel acquisition function behavior.&quot;&quot;&quot;&#10;    print(&quot;\nComparing MLP vs PhModel Acquisition Functions...&quot;)&#10;&#10;    # Load data&#10;    _, _, df = load_data()&#10;    df_train = df.iloc[:20]  # Smaller training set for comparison&#10;    df_explore = df.iloc[21:30]  # Smaller exploration set&#10;&#10;    # Test both optimizers on the same data&#10;    mlp_opt = EnsembleOptimizer(model_type=&quot;MLP&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;    ph_opt = EnsembleOptimizer(model_type=&quot;PhModel&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;&#10;    ei_mlp, pick_mlp = mlp_opt.step_within_data(df_train, df_explore)&#10;    ei_ph, pick_ph = ph_opt.step_within_data(df_train, df_explore)&#10;&#10;    print(f&quot;MLP - Best EI: {ei_mlp.max():.6f}, Pick: {pick_mlp}&quot;)&#10;    print(f&quot;PhModel - Best EI: {ei_ph.max():.6f}, Pick: {pick_ph}&quot;)&#10;    print(f&quot;Same pick: {pick_mlp == pick_ph}&quot;)&#10;&#10;    return pick_mlp, pick_ph&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    if not BOTORCH_AVAILABLE:&#10;        print(&quot;❌ BoTorch not available - cannot run acquisition function tests&quot;)&#10;        print(&quot;Install with: pip install botorch&quot;)&#10;    else:&#10;        try:&#10;            test_mlp_optimizer()&#10;            test_phmodel_optimizer()&#10;            test_comparison()&#10;            print(&quot;\n✅ All API tests completed successfully!&quot;)&#10;        except Exception as e:&#10;            print(f&quot;❌ API test failed with error: {e}&quot;)&#10;            import traceback&#10;            traceback.print_exc()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;API tests for MLP and PhModel ensembles matching the original test_api.py structure.&#10;Tests the same acquisition function and step methods as the original.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;import pandas as pd&#10;import numpy as np&#10;from data.loader import load_data&#10;from models.mlp_ensemble import MLPEnsemble, EnsembleConfig&#10;from models.ph_ensemble import PhModelEnsemble, PhEnsembleConfig&#10;&#10;# BoTorch imports for acquisition functions&#10;try:&#10;    from botorch.acquisition.analytic import ExpectedImprovement&#10;    from botorch.optim import optimize_acqf&#10;    BOTORCH_AVAILABLE = True&#10;except ImportError:&#10;    BOTORCH_AVAILABLE = False&#10;    print(&quot;Warning: BoTorch not available. Install with 'pip install botorch'&quot;)&#10;&#10;&#10;class SingleOutputEnsembleWrapper:&#10;    &quot;&quot;&quot;&#10;    Wrapper to make multi-output ensembles work with single-output acquisition functions.&#10;    Extracts a single output dimension for optimization.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, ensemble, output_index=0):&#10;        self.ensemble = ensemble&#10;        self.output_index = output_index&#10;        self.num_outputs = 1  # Single output for acquisition function&#10;        self._num_outputs = 1&#10;&#10;    def posterior(self, X: torch.Tensor, **kwargs):&#10;        &quot;&quot;&quot;Get posterior distribution for the specified output dimension.&quot;&quot;&quot;&#10;        # Handle BoTorch's batch dimension - reshape (batch, 1, features) to (batch, features)&#10;        if X.ndim == 3 and X.shape[1] == 1:&#10;            X = X.squeeze(1)  # Remove the extra dimension&#10;&#10;        # Get full posterior from ensemble&#10;        full_posterior = self.ensemble.get_botorch_posterior(X)&#10;&#10;        # Extract only the specified output dimension&#10;        mean = full_posterior.mean[..., self.output_index:self.output_index+1]  # (batch, 1, 1)&#10;&#10;        # Extract corresponding covariance for this output dimension&#10;        batch_size = X.shape[0]&#10;        covariance_single = []&#10;        for b in range(batch_size):&#10;            # Get the variance for this output dimension&#10;            var = full_posterior.distribution.covariance_matrix[b, self.output_index, self.output_index]&#10;            cov_matrix = var.unsqueeze(0).unsqueeze(0)  # (1, 1)&#10;            covariance_single.append(cov_matrix)&#10;&#10;        covariance = torch.stack(covariance_single, dim=0)  # (batch, 1, 1)&#10;&#10;        # Create single-output distribution&#10;        from gpytorch.distributions import MultitaskMultivariateNormal&#10;        mvn_single = MultitaskMultivariateNormal(mean, covariance)&#10;&#10;        from botorch.posteriors.gpytorch import GPyTorchPosterior&#10;        return GPyTorchPosterior(mvn_single)&#10;&#10;    def forward(self, X: torch.Tensor):&#10;        &quot;&quot;&quot;Forward pass that returns posterior (for compatibility).&quot;&quot;&quot;&#10;        return self.posterior(X)&#10;&#10;&#10;class EnsembleOptimizer:&#10;    &quot;&quot;&quot;&#10;    Optimizer class that wraps our MLP and PhModel ensembles for Bayesian optimization.&#10;    Mimics the API structure from the original GDEOptimizer.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, model_type=&quot;MLP&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True, output_dir=&quot;./out&quot;):&#10;        &quot;&quot;&quot;&#10;        Initialize the optimizer with the specified model type.&#10;&#10;        Args:&#10;            model_type: &quot;MLP&quot; or &quot;PhModel&quot;&#10;            quantity: The quantity to optimize (e.g., 'FE (Eth)')&#10;            maximize: Whether to maximize or minimize the quantity&#10;            output_dir: Directory to save output files&#10;        &quot;&quot;&quot;&#10;        self.model_type = model_type&#10;        self.quantity = quantity&#10;        self.maximize = maximize&#10;        self.output_dir = output_dir&#10;        self.ensemble = None&#10;        self.is_trained = False&#10;&#10;        # Determine output index for single-output wrapper&#10;        self.output_index = 0 if quantity == &quot;FE (Eth)&quot; else 1&#10;&#10;        # For tracking current best value&#10;        self.best_f = None&#10;&#10;    def _train_ensemble(self, df: pd.DataFrame):&#10;        &quot;&quot;&quot;Train the ensemble on the provided data.&quot;&quot;&quot;&#10;        # Load and prepare data&#10;        data_tensors, norm_params, _ = load_data()&#10;&#10;        # Filter to match the provided DataFrame indices&#10;        indices = df.index.tolist()&#10;        train_mask = torch.tensor([i in indices for i in range(len(data_tensors.X))])&#10;        X_train = data_tensors.X[train_mask]&#10;        y_train = data_tensors.y[train_mask]&#10;&#10;        if self.model_type == &quot;MLP&quot;:&#10;            config = EnsembleConfig(&#10;                ensemble_size=5,  # Smaller for testing&#10;                hidden_dim=32,&#10;                dropout_rate=0.1,&#10;                learning_rate=0.001,&#10;                bootstrap_fraction=0.5&#10;            )&#10;            self.ensemble = MLPEnsemble(config)&#10;&#10;        elif self.model_type == &quot;PhModel&quot;:&#10;            # Get original ZLT stats for PhModel&#10;            original_data = pd.read_excel('Characterization_data.xlsx', skiprows=[1], index_col=0)&#10;            original_data = original_data.dropna()&#10;            # Add thickness calculation (matching data loader)&#10;            dens_Ag = 10490&#10;            dens_Cu = 8935&#10;            dens_avg = (1 - original_data['AgCu Ratio']) * dens_Cu + original_data['AgCu Ratio'] * dens_Ag&#10;            mass = original_data['Catalyst mass loading'] * 1e-6&#10;            area = 1.85**2&#10;            A = area * 1e-4&#10;            thickness = (mass / dens_avg) / A&#10;            original_data.insert(3, column='Zero_eps_thickness', value=thickness)&#10;&#10;            original_zlt_mean = original_data['Zero_eps_thickness'].mean()&#10;            original_zlt_std = original_data['Zero_eps_thickness'].std(ddof=0)&#10;&#10;            config = PhEnsembleConfig(&#10;                ensemble_size=3,  # Smaller for testing&#10;                hidden_dim=32,&#10;                dropout_rate=0.1,&#10;                learning_rate=0.001,&#10;                bootstrap_fraction=0.5,&#10;                current_target=200.0,&#10;                grid_size=50  # Smaller for faster testing&#10;            )&#10;            self.ensemble = PhModelEnsemble(config, zlt_mu_stds=(original_zlt_mean, original_zlt_std))&#10;&#10;        else:&#10;            raise ValueError(f&quot;Unknown model type: {self.model_type}&quot;)&#10;&#10;        # Train the ensemble&#10;        print(f&quot;Training {self.model_type} ensemble on {len(X_train)} samples...&quot;)&#10;        self.ensemble.train(X_train, y_train, num_epochs=50, verbose=False)&#10;        self.is_trained = True&#10;&#10;        # Update best value for acquisition function&#10;        target_col_idx = 0 if self.quantity == &quot;FE (Eth)&quot; else 1&#10;        if self.maximize:&#10;            self.best_f = y_train[:, target_col_idx].max()&#10;        else:&#10;            self.best_f = y_train[:, target_col_idx].min()&#10;&#10;    def _get_acquisition_function(self):&#10;        &quot;&quot;&quot;Get the Expected Improvement acquisition function.&quot;&quot;&quot;&#10;        if not BOTORCH_AVAILABLE:&#10;            raise ImportError(&quot;BoTorch not available for acquisition functions&quot;)&#10;&#10;        # Wrap ensemble for single-output acquisition function&#10;        single_output_model = SingleOutputEnsembleWrapper(self.ensemble, self.output_index)&#10;&#10;        return ExpectedImprovement(&#10;            model=single_output_model,&#10;            best_f=self.best_f,&#10;            maximize=self.maximize&#10;        )&#10;&#10;    def step_within_data(self, df_train: pd.DataFrame, df_explore: pd.DataFrame):&#10;        &quot;&quot;&quot;&#10;        Perform acquisition function evaluation within the exploration data.&#10;&#10;        Args:&#10;            df_train: Training data to fit the model&#10;            df_explore: Exploration data to evaluate acquisition function&#10;&#10;        Returns:&#10;            ei_values: Expected Improvement values for exploration points&#10;            next_pick: Index of the best point to pick next&#10;        &quot;&quot;&quot;&#10;        # Train ensemble on training data&#10;        self._train_ensemble(df_train)&#10;&#10;        # Prepare exploration data&#10;        data_tensors, _, full_df = load_data()&#10;&#10;        # Get exploration indices and corresponding features&#10;        explore_indices = df_explore.index.tolist()&#10;        explore_mask = torch.tensor([i in explore_indices for i in range(len(data_tensors.X))])&#10;        X_explore = data_tensors.X[explore_mask]&#10;&#10;        if len(X_explore) == 0:&#10;            raise ValueError(&quot;No exploration points found&quot;)&#10;&#10;        # Get acquisition function&#10;        acquisition_func = self._get_acquisition_function()&#10;&#10;        # Evaluate acquisition function&#10;        with torch.no_grad():&#10;            ei_values = acquisition_func(X_explore.unsqueeze(1))  # Add batch dimension for BoTorch&#10;            ei_values = ei_values.squeeze()  # Remove extra dimensions&#10;&#10;        # Find best point&#10;        if self.maximize:&#10;            best_idx = ei_values.argmax().item()&#10;        else:&#10;            best_idx = ei_values.argmin().item()&#10;&#10;        # Map back to original DataFrame index&#10;        next_pick = explore_indices[best_idx]&#10;&#10;        return ei_values, next_pick&#10;&#10;&#10;def test_mlp_optimizer():&#10;    &quot;&quot;&quot;Test the MLP ensemble optimizer matching the original test_api.py structure.&quot;&quot;&quot;&#10;    print(&quot;Testing MLP Ensemble Optimizer API...&quot;)&#10;&#10;    # Load data&#10;    _, _, df = load_data()&#10;&#10;    # Create optimizer&#10;    optimizer = EnsembleOptimizer(model_type=&quot;MLP&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;&#10;    # Split data like in original test&#10;    df_train = df.iloc[:30]&#10;    df_explore = df.iloc[31:]&#10;&#10;    print(f&quot;Training set: {len(df_train)} samples&quot;)&#10;    print(f&quot;Exploration set: {len(df_explore)} samples&quot;)&#10;&#10;    # First step&#10;    ei, next_pick = optimizer.step_within_data(df_train, df_explore)&#10;    print(f&quot;First pick - EI max: {ei.max():.6f}, Next pick index: {next_pick}&quot;)&#10;&#10;    # Add picked point to training and remove from exploration&#10;    df_new = df_explore.loc[next_pick:next_pick]  # Single row DataFrame&#10;    df_train_updated = pd.concat([df_train, df_new])&#10;    df_explore_updated = df_explore.drop(index=next_pick)&#10;&#10;    # Second step&#10;    optimizer_2 = EnsembleOptimizer(model_type=&quot;MLP&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;    ei_2, next_pick_2 = optimizer_2.step_within_data(df_train_updated, df_explore_updated)&#10;    print(f&quot;Second pick - EI max: {ei_2.max():.6f}, Next pick index: {next_pick_2}&quot;)&#10;&#10;    print(&quot;✅ MLP optimizer API test completed&quot;)&#10;    return True&#10;&#10;&#10;def test_phmodel_optimizer():&#10;    &quot;&quot;&quot;Test the PhModel ensemble optimizer matching the original test_api.py structure.&quot;&quot;&quot;&#10;    print(&quot;\nTesting PhModel Ensemble Optimizer API...&quot;)&#10;&#10;    # Load data&#10;    _, _, df = load_data()&#10;&#10;    # Create optimizer&#10;    optimizer = EnsembleOptimizer(model_type=&quot;PhModel&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;&#10;    # Split data like in original test&#10;    df_train = df.iloc[:30]&#10;    df_explore = df.iloc[31:]&#10;&#10;    print(f&quot;Training set: {len(df_train)} samples&quot;)&#10;    print(f&quot;Exploration set: {len(df_explore)} samples&quot;)&#10;&#10;    # First step&#10;    ei, next_pick = optimizer.step_within_data(df_train, df_explore)&#10;    print(f&quot;First pick - EI max: {ei.max():.6f}, Next pick index: {next_pick}&quot;)&#10;&#10;    # Add picked point to training and remove from exploration&#10;    df_new = df_explore.loc[next_pick:next_pick]  # Single row DataFrame&#10;    df_train_updated = pd.concat([df_train, df_new])&#10;    df_explore_updated = df_explore.drop(index=next_pick)&#10;&#10;    # Second step&#10;    optimizer_2 = EnsembleOptimizer(model_type=&quot;PhModel&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;    ei_2, next_pick_2 = optimizer_2.step_within_data(df_train_updated, df_explore_updated)&#10;    print(f&quot;Second pick - EI max: {ei_2.max():.6f}, Next pick index: {next_pick_2}&quot;)&#10;&#10;    print(&quot;✅ PhModel optimizer API test completed&quot;)&#10;    return True&#10;&#10;&#10;def test_comparison():&#10;    &quot;&quot;&quot;Compare MLP vs PhModel acquisition function behavior.&quot;&quot;&quot;&#10;    print(&quot;\nComparing MLP vs PhModel Acquisition Functions...&quot;)&#10;&#10;    # Load data&#10;    _, _, df = load_data()&#10;    df_train = df.iloc[:20]  # Smaller training set for comparison&#10;    df_explore = df.iloc[21:30]  # Smaller exploration set&#10;&#10;    # Test both optimizers on the same data&#10;    mlp_opt = EnsembleOptimizer(model_type=&quot;MLP&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;    ph_opt = EnsembleOptimizer(model_type=&quot;PhModel&quot;, quantity=&quot;FE (Eth)&quot;, maximize=True)&#10;&#10;    ei_mlp, pick_mlp = mlp_opt.step_within_data(df_train, df_explore)&#10;    ei_ph, pick_ph = ph_opt.step_within_data(df_train, df_explore)&#10;&#10;    print(f&quot;MLP - Best EI: {ei_mlp.max():.6f}, Pick: {pick_mlp}&quot;)&#10;    print(f&quot;PhModel - Best EI: {ei_ph.max():.6f}, Pick: {pick_ph}&quot;)&#10;    print(f&quot;Same pick: {pick_mlp == pick_ph}&quot;)&#10;&#10;    return pick_mlp, pick_ph&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    if not BOTORCH_AVAILABLE:&#10;        print(&quot;❌ BoTorch not available - cannot run acquisition function tests&quot;)&#10;        print(&quot;Install with: pip install botorch&quot;)&#10;    else:&#10;        try:&#10;            test_mlp_optimizer()&#10;            test_phmodel_optimizer()&#10;            test_comparison()&#10;            print(&quot;\n✅ All API tests completed successfully!&quot;)&#10;        except Exception as e:&#10;            print(f&quot;❌ API test failed with error: {e}&quot;)&#10;            import traceback&#10;            traceback.print_exc()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_full_mlp_ensemble.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_full_mlp_ensemble.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Test MLP ensemble with full configuration matching the outline requirements.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;from data.loader import load_data&#10;from models.mlp_ensemble import MLPEnsemble, EnsembleConfig&#10;import time&#10;&#10;&#10;def test_full_mlp_ensemble():&#10;    &quot;&quot;&quot;Test MLP ensemble with full 50-model configuration.&quot;&quot;&quot;&#10;    print(&quot;Testing MLP ensemble with FULL configuration (50 models)...&quot;)&#10;&#10;    # Load data with proper normalization&#10;    print(&quot;\n1. Loading data...&quot;)&#10;    data_tensors, norm_params, df = load_data(&#10;        normalize_features=True,  # Features normalized as per old implementation&#10;        normalize_targets=False   # Targets stay as decimals 0-1&#10;    )&#10;    print(f&quot;Data: {data_tensors.X.shape[0]} samples, {data_tensors.X.shape[1]} features&quot;)&#10;&#10;    # Create FULL ensemble configuration matching the outline&#10;    print(&quot;\n2. Creating FULL MLP ensemble (50 models)...&quot;)&#10;    full_config = EnsembleConfig(&#10;        ensemble_size=50,         # ✓ 50 models as per outline&#10;        hidden_dim=64,&#10;        dropout_rate=0.1,&#10;        learning_rate=0.001,&#10;        bootstrap_fraction=0.5    # ✓ Random subset (bootstrapping)&#10;    )&#10;    ensemble = MLPEnsemble(full_config)&#10;    print(f&quot;Created ensemble with {len(ensemble.models)} models&quot;)&#10;&#10;    # Train with proper epoch count from config&#10;    print(&quot;\n3. Training full ensemble...&quot;)&#10;    start_time = time.time()&#10;    training_stats = ensemble.train(&#10;        data_tensors.X,&#10;        data_tensors.y,&#10;        num_epochs=100,  # Reduced for testing, but matches train.py approach&#10;        verbose=False    # Reduce output for 50 models&#10;    )&#10;    train_time = time.time() - start_time&#10;    print(f&quot;Training completed in {train_time:.1f} seconds&quot;)&#10;&#10;    # Test predictions with uncertainty&#10;    print(&quot;\n4. Testing ensemble predictions...&quot;)&#10;    mean_pred, std_pred = ensemble.predict(data_tensors.X, return_std=True)&#10;&#10;    print(f&quot;Prediction shapes - Mean: {mean_pred.shape}, Std: {std_pred.shape}&quot;)&#10;    print(f&quot;Mean prediction range: [{mean_pred.min():.3f}, {mean_pred.max():.3f}]&quot;)&#10;    print(f&quot;Uncertainty range: [{std_pred.min():.3f}, {std_pred.max():.3f}]&quot;)&#10;&#10;    # Verify bootstrapping worked (models should give different predictions)&#10;    print(&quot;\n5. Verifying bootstrap diversity...&quot;)&#10;    individual_preds = []&#10;    for i, model in enumerate(ensemble.models[:5]):  # Check first 5 models&#10;        model.eval()&#10;        with torch.no_grad():&#10;            pred = model(data_tensors.X[:1])  # Single test point&#10;            individual_preds.append(pred)&#10;&#10;    # Calculate variance across individual model predictions&#10;    individual_stack = torch.stack(individual_preds, dim=0)&#10;    model_variance = individual_stack.var(dim=0).mean().item()&#10;    print(f&quot;Variance across individual models: {model_variance:.6f}&quot;)&#10;    if model_variance &gt; 1e-6:&#10;        print(&quot;✅ Bootstrap sampling created diverse models&quot;)&#10;    else:&#10;        print(&quot;❌ Models are too similar - bootstrap may not be working&quot;)&#10;&#10;    # Performance metrics&#10;    print(&quot;\n6. Performance evaluation...&quot;)&#10;    train_mse = torch.mean((mean_pred - data_tensors.y)**2).item()&#10;    print(f&quot;Training MSE: {train_mse:.6f}&quot;)&#10;&#10;    # Show sample predictions&#10;    print(f&quot;\nSample predictions (first 3 points):&quot;)&#10;    for i in range(3):&#10;        true_eth, true_co = data_tensors.y[i]&#10;        pred_eth, pred_co = mean_pred[i]&#10;        std_eth, std_co = std_pred[i]&#10;        print(f&quot;  Point {i}: True=[{true_eth:.3f}, {true_co:.3f}], &quot;&#10;              f&quot;Pred=[{pred_eth:.3f}±{std_eth:.3f}, {pred_co:.3f}±{std_co:.3f}]&quot;)&#10;&#10;    # Test BoTorch compatibility&#10;    print(&quot;\n7. Testing BoTorch compatibility...&quot;)&#10;    samples = ensemble.get_prediction_samples(data_tensors.X[:3], n_samples=100)&#10;    print(f&quot;Sample tensor shape: {samples.shape}&quot;)&#10;&#10;    print(&quot;\n✅ Full MLP ensemble test completed successfully!&quot;)&#10;    print(f&quot;Summary:&quot;)&#10;    print(f&quot;  - Ensemble size: {len(ensemble.models)} models&quot;)&#10;    print(f&quot;  - Bootstrap sampling: ✓ (with replacement)&quot;)&#10;    print(f&quot;  - Uncertainty estimation: ✓ (std dev across ensemble)&quot;)&#10;    print(f&quot;  - BoTorch compatible: ✓ (sampling interface)&quot;)&#10;    print(f&quot;  - Training time: {train_time:.1f}s&quot;)&#10;&#10;    return ensemble, train_mse, model_variance&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    try:&#10;        test_full_mlp_ensemble()&#10;    except Exception as e:&#10;        print(f&quot;❌ Test failed with error: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Test MLP ensemble with full configuration matching the outline requirements.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;from data.loader import load_data&#10;from models.mlp_ensemble import MLPEnsemble, EnsembleConfig&#10;import time&#10;&#10;&#10;def test_full_mlp_ensemble():&#10;    &quot;&quot;&quot;Test MLP ensemble with full 50-model configuration.&quot;&quot;&quot;&#10;    print(&quot;Testing MLP ensemble with FULL configuration (50 models)...&quot;)&#10;&#10;    # Load data with proper normalization&#10;    print(&quot;\n1. Loading data...&quot;)&#10;    data_tensors, norm_params, df = load_data(&#10;        normalize_features=True,  # Features normalized as per old implementation&#10;        normalize_targets=False   # Targets stay as decimals 0-1&#10;    )&#10;    print(f&quot;Data: {data_tensors.X.shape[0]} samples, {data_tensors.X.shape[1]} features&quot;)&#10;&#10;    # Create FULL ensemble configuration matching the outline&#10;    print(&quot;\n2. Creating FULL MLP ensemble (50 models)...&quot;)&#10;    full_config = EnsembleConfig(&#10;        ensemble_size=50,         # ✓ 50 models as per outline&#10;        hidden_dim=64,&#10;        dropout_rate=0.1,&#10;        learning_rate=0.001,&#10;        bootstrap_fraction=0.5    # ✓ Random subset (bootstrapping)&#10;    )&#10;    ensemble = MLPEnsemble(full_config)&#10;    print(f&quot;Created ensemble with {len(ensemble.models)} models&quot;)&#10;&#10;    # Train with proper epoch count from config&#10;    print(&quot;\n3. Training full ensemble...&quot;)&#10;    start_time = time.time()&#10;    training_stats = ensemble.train(&#10;        data_tensors.X,&#10;        data_tensors.y,&#10;        num_epochs=100,  # Reduced for testing, but matches train.py approach&#10;        verbose=False    # Reduce output for 50 models&#10;    )&#10;    train_time = time.time() - start_time&#10;    print(f&quot;Training completed in {train_time:.1f} seconds&quot;)&#10;&#10;    # Test predictions with uncertainty&#10;    print(&quot;\n4. Testing ensemble predictions...&quot;)&#10;    mean_pred, std_pred = ensemble.predict(data_tensors.X, return_std=True)&#10;&#10;    print(f&quot;Prediction shapes - Mean: {mean_pred.shape}, Std: {std_pred.shape}&quot;)&#10;    print(f&quot;Mean prediction range: [{mean_pred.min():.3f}, {mean_pred.max():.3f}]&quot;)&#10;    print(f&quot;Uncertainty range: [{std_pred.min():.3f}, {std_pred.max():.3f}]&quot;)&#10;&#10;    # Verify bootstrapping worked (models should give different predictions)&#10;    print(&quot;\n5. Verifying bootstrap diversity...&quot;)&#10;    individual_preds = []&#10;    for i, model in enumerate(ensemble.models[:5]):  # Check first 5 models&#10;        model.eval()&#10;        with torch.no_grad():&#10;            pred = model(data_tensors.X[:1])  # Single test point&#10;            individual_preds.append(pred)&#10;&#10;    # Calculate variance across individual model predictions&#10;    individual_stack = torch.stack(individual_preds, dim=0)&#10;    model_variance = individual_stack.var(dim=0).mean().item()&#10;    print(f&quot;Variance across individual models: {model_variance:.6f}&quot;)&#10;    if model_variance &gt; 1e-6:&#10;        print(&quot;✅ Bootstrap sampling created diverse models&quot;)&#10;    else:&#10;        print(&quot;❌ Models are too similar - bootstrap may not be working&quot;)&#10;&#10;    # Performance metrics&#10;    print(&quot;\n6. Performance evaluation...&quot;)&#10;    train_mse = torch.mean((mean_pred - data_tensors.y)**2).item()&#10;    print(f&quot;Training MSE: {train_mse:.6f}&quot;)&#10;&#10;    # Show sample predictions&#10;    print(f&quot;\nSample predictions (first 3 points):&quot;)&#10;    for i in range(3):&#10;        true_eth, true_co = data_tensors.y[i]&#10;        pred_eth, pred_co = mean_pred[i]&#10;        std_eth, std_co = std_pred[i]&#10;        print(f&quot;  Point {i}: True=[{true_eth:.3f}, {true_co:.3f}], &quot;&#10;              f&quot;Pred=[{pred_eth:.3f}±{std_eth:.3f}, {pred_co:.3f}±{std_co:.3f}]&quot;)&#10;&#10;    # Test BoTorch compatibility&#10;    print(&quot;\n7. Testing BoTorch compatibility...&quot;)&#10;    &#10;    # Test old method (should show wrong format)&#10;    samples_old = ensemble.get_prediction_samples(data_tensors.X[:3], n_samples=100)&#10;    print(f&quot;Old sample format: {samples_old.shape} (n_samples, batch_size, outputs)&quot;)&#10;    &#10;    # Test new BoTorch-compatible method&#10;    try:&#10;        samples_botorch = ensemble.get_prediction_samples_botorch(data_tensors.X[:3], n_samples=100)&#10;        print(f&quot;BoTorch sample format: {samples_botorch.shape} (n_samples, batch_size, 1, outputs) ✅&quot;)&#10;        &#10;        # Test posterior&#10;        posterior = ensemble.get_botorch_posterior(data_tensors.X[:3])&#10;        print(f&quot;BoTorch posterior created successfully ✅&quot;)&#10;        print(f&quot;Posterior mean shape: {posterior.mean.shape}&quot;)&#10;        &#10;    except ImportError as e:&#10;        print(f&quot;BoTorch not available: {e}&quot;)&#10;        print(&quot;Install with: pip install botorch&quot;)&#10;    except Exception as e:&#10;        print(f&quot;BoTorch integration error: {e}&quot;)&#10;&#10;    print(&quot;\n✅ Full MLP ensemble test completed successfully!&quot;)&#10;    print(f&quot;Summary:&quot;)&#10;    print(f&quot;  - Ensemble size: {len(ensemble.models)} models&quot;)&#10;    print(f&quot;  - Bootstrap sampling: ✓ (with replacement)&quot;)&#10;    print(f&quot;  - Uncertainty estimation: ✓ (std dev across ensemble)&quot;)&#10;    print(f&quot;  - BoTorch compatible: ✓ (sampling interface)&quot;)&#10;    print(f&quot;  - Training time: {train_time:.1f}s&quot;)&#10;&#10;    return ensemble, train_mse, model_variance&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    try:&#10;        test_full_mlp_ensemble()&#10;    except Exception as e:&#10;        print(f&quot;❌ Test failed with error: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_mlp_ensemble.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_mlp_ensemble.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Test script for MLP ensemble model.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;from data.loader import load_data&#10;from models.mlp_ensemble import MLPEnsemble, EnsembleConfig&#10;from config import default_config&#10;&#10;&#10;def test_mlp_ensemble():&#10;    &quot;&quot;&quot;Test the MLP ensemble training and prediction.&quot;&quot;&quot;&#10;    print(&quot;Testing MLP ensemble...&quot;)&#10;&#10;    # Load data&#10;    print(&quot;\n1. Loading data...&quot;)&#10;    data_tensors, norm_params, df = load_data(&#10;        normalize_features=default_config['normalize'],&#10;        normalize_targets=False&#10;    )&#10;    print(f&quot;Data loaded: X shape {data_tensors.X.shape}, y shape {data_tensors.y.shape}&quot;)&#10;    print(f&quot;Features normalized: {norm_params is not None}&quot;)&#10;    print(f&quot;Target range: [{data_tensors.y.min():.3f}, {data_tensors.y.max():.3f}]&quot;)&#10;&#10;    # Create small ensemble for testing&#10;    print(&quot;\n2. Creating MLP ensemble...&quot;)&#10;    test_config = EnsembleConfig(&#10;        ensemble_size=5,  # Small for testing&#10;        hidden_dim=32,    # Smaller for faster training&#10;        dropout_rate=0.1,&#10;        learning_rate=0.001,&#10;        bootstrap_fraction=0.5&#10;    )&#10;    ensemble = MLPEnsemble(test_config)&#10;    print(f&quot;Created ensemble with {len(ensemble.models)} models&quot;)&#10;&#10;    # Train ensemble&#10;    print(&quot;\n3. Training ensemble...&quot;)&#10;    training_stats = ensemble.train(&#10;        data_tensors.X,&#10;        data_tensors.y,&#10;        num_epochs=50,  # Quick training for test&#10;        verbose=True&#10;    )&#10;&#10;    # Test predictions&#10;    print(&quot;\n4. Testing predictions...&quot;)&#10;    mean_pred, std_pred = ensemble.predict(data_tensors.X, return_std=True)&#10;    print(f&quot;Prediction shapes - Mean: {mean_pred.shape}, Std: {std_pred.shape}&quot;)&#10;    print(f&quot;Mean prediction range: [{mean_pred.min():.3f}, {mean_pred.max():.3f}]&quot;)&#10;    print(f&quot;Uncertainty range: [{std_pred.min():.3f}, {std_pred.max():.3f}]&quot;)&#10;&#10;    # Test sampling for BoTorch compatibility&#10;    print(&quot;\n5. Testing BoTorch compatibility...&quot;)&#10;    samples = ensemble.get_prediction_samples(data_tensors.X[:5], n_samples=100)&#10;    print(f&quot;Sample shape: {samples.shape}&quot;)&#10;    sample_mean = samples.mean(dim=0)&#10;    sample_std = samples.std(dim=0)&#10;    print(f&quot;Sample statistics match predictions: Mean diff {torch.abs(sample_mean - mean_pred[:5]).max():.3e}&quot;)&#10;&#10;    # Calculate training error&#10;    print(&quot;\n6. Training performance...&quot;)&#10;    train_error = torch.mean((mean_pred - data_tensors.y)**2).item()&#10;    print(f&quot;Final training MSE: {train_error:.6f}&quot;)&#10;&#10;    # Test individual outputs&#10;    print(f&quot;\nSample predictions for first 3 points:&quot;)&#10;    for i in range(3):&#10;        true_eth, true_co = data_tensors.y[i]&#10;        pred_eth, pred_co = mean_pred[i]&#10;        std_eth, std_co = std_pred[i]&#10;        print(f&quot;  Point {i}: True=[{true_eth:.3f}, {true_co:.3f}], &quot;&#10;              f&quot;Pred=[{pred_eth:.3f}±{std_eth:.3f}, {pred_co:.3f}±{std_co:.3f}]&quot;)&#10;&#10;    print(&quot;\n✅ MLP ensemble test completed successfully!&quot;)&#10;    return ensemble, data_tensors, norm_params&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    try:&#10;        test_mlp_ensemble()&#10;    except Exception as e:&#10;        print(f&quot;❌ Test failed with error: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Test script for MLP ensemble model.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;from data.loader import load_data&#10;from models.mlp_ensemble import MLPEnsemble, EnsembleConfig&#10;from config import default_config&#10;&#10;&#10;def test_mlp_ensemble():&#10;    &quot;&quot;&quot;Test the MLP ensemble training and prediction.&quot;&quot;&quot;&#10;    print(&quot;Testing MLP ensemble...&quot;)&#10;&#10;    # Load data&#10;    print(&quot;\n1. Loading data...&quot;)&#10;    data_tensors, norm_params, df = load_data(&#10;        normalize_features=True,  # Always normalize features for MLP training&#10;        normalize_targets=False&#10;    )&#10;    print(f&quot;Data loaded: X shape {data_tensors.X.shape}, y shape {data_tensors.y.shape}&quot;)&#10;    print(f&quot;Features normalized: {norm_params is not None}&quot;)&#10;    print(f&quot;Target range: [{data_tensors.y.min():.3f}, {data_tensors.y.max():.3f}]&quot;)&#10;&#10;    # Create small ensemble for testing&#10;    print(&quot;\n2. Creating MLP ensemble...&quot;)&#10;    test_config = EnsembleConfig(&#10;        ensemble_size=5,  # Small for testing&#10;        hidden_dim=32,    # Smaller for faster training&#10;        dropout_rate=0.1,&#10;        learning_rate=0.001,&#10;        bootstrap_fraction=0.5&#10;    )&#10;    ensemble = MLPEnsemble(test_config)&#10;    print(f&quot;Created ensemble with {len(ensemble.models)} models&quot;)&#10;&#10;    # Train ensemble&#10;    print(&quot;\n3. Training ensemble...&quot;)&#10;    training_stats = ensemble.train(&#10;        data_tensors.X,&#10;        data_tensors.y,&#10;        num_epochs=50,  # Quick training for test&#10;        verbose=True&#10;    )&#10;&#10;    # Test predictions&#10;    print(&quot;\n4. Testing predictions...&quot;)&#10;    mean_pred, std_pred = ensemble.predict(data_tensors.X, return_std=True)&#10;    print(f&quot;Prediction shapes - Mean: {mean_pred.shape}, Std: {std_pred.shape}&quot;)&#10;    print(f&quot;Mean prediction range: [{mean_pred.min():.3f}, {mean_pred.max():.3f}]&quot;)&#10;    print(f&quot;Uncertainty range: [{std_pred.min():.3f}, {std_pred.max():.3f}]&quot;)&#10;&#10;    # Test sampling for BoTorch compatibility&#10;    print(&quot;\n5. Testing BoTorch compatibility...&quot;)&#10;    samples = ensemble.get_prediction_samples(data_tensors.X[:5], n_samples=100)&#10;    print(f&quot;Sample shape: {samples.shape}&quot;)&#10;    sample_mean = samples.mean(dim=0)&#10;    sample_std = samples.std(dim=0)&#10;    print(f&quot;Sample statistics match predictions: Mean diff {torch.abs(sample_mean - mean_pred[:5]).max():.3e}&quot;)&#10;&#10;    # Calculate training error&#10;    print(&quot;\n6. Training performance...&quot;)&#10;    train_error = torch.mean((mean_pred - data_tensors.y)**2).item()&#10;    print(f&quot;Final training MSE: {train_error:.6f}&quot;)&#10;&#10;    # Test individual outputs&#10;    print(f&quot;\nSample predictions for first 3 points:&quot;)&#10;    for i in range(3):&#10;        true_eth, true_co = data_tensors.y[i]&#10;        pred_eth, pred_co = mean_pred[i]&#10;        std_eth, std_co = std_pred[i]&#10;        print(f&quot;  Point {i}: True=[{true_eth:.3f}, {true_co:.3f}], &quot;&#10;              f&quot;Pred=[{pred_eth:.3f}±{std_eth:.3f}, {pred_co:.3f}±{std_co:.3f}]&quot;)&#10;&#10;    print(&quot;\n✅ MLP ensemble test completed successfully!&quot;)&#10;    return ensemble, data_tensors, norm_params&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    try:&#10;        test_mlp_ensemble()&#10;    except Exception as e:&#10;        print(f&quot;❌ Test failed with error: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_mlp_unnormalized.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_mlp_unnormalized.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Test MLP ensemble with non-normalized inputs to check robustness.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;from data.loader import load_data&#10;from models.mlp_ensemble import MLPEnsemble, EnsembleConfig&#10;&#10;&#10;def test_mlp_with_unnormalized_inputs():&#10;    &quot;&quot;&quot;Test if MLP can work with non-normalized inputs.&quot;&quot;&quot;&#10;    print(&quot;Testing MLP ensemble with non-normalized inputs...&quot;)&#10;    &#10;    # Load data without normalization&#10;    print(&quot;\n1. Loading data without normalization...&quot;)&#10;    data_tensors, norm_params, df = load_data(&#10;        normalize_features=False,  # Test without normalization&#10;        normalize_targets=False&#10;    )&#10;    print(f&quot;Data loaded: X shape {data_tensors.X.shape}, y shape {data_tensors.y.shape}&quot;)&#10;    print(f&quot;Features normalized: {norm_params is not None}&quot;)&#10;    print(f&quot;X range: [{data_tensors.X.min():.3f}, {data_tensors.X.max():.3f}]&quot;)&#10;    print(f&quot;Sample X values:\n{data_tensors.X[:3]}&quot;)&#10;    &#10;    # Create small ensemble for testing&#10;    print(&quot;\n2. Creating MLP ensemble...&quot;)&#10;    test_config = EnsembleConfig(&#10;        ensemble_size=3,  # Very small for quick test&#10;        hidden_dim=32,&#10;        dropout_rate=0.1,&#10;        learning_rate=0.01,  # Higher learning rate for unnormalized data&#10;        bootstrap_fraction=0.5&#10;    )&#10;    ensemble = MLPEnsemble(test_config)&#10;    &#10;    # Train ensemble&#10;    print(&quot;\n3. Training ensemble with unnormalized inputs...&quot;)&#10;    try:&#10;        training_stats = ensemble.train(&#10;            data_tensors.X, &#10;            data_tensors.y, &#10;            num_epochs=100,  # More epochs for harder training&#10;            verbose=True&#10;        )&#10;        &#10;        # Test predictions&#10;        mean_pred, std_pred = ensemble.predict(data_tensors.X, return_std=True)&#10;        print(f&quot;\nPrediction range: [{mean_pred.min():.3f}, {mean_pred.max():.3f}]&quot;)&#10;        &#10;        # Calculate training error&#10;        train_error = torch.mean((mean_pred - data_tensors.y)**2).item()&#10;        print(f&quot;Final training MSE: {train_error:.6f}&quot;)&#10;        &#10;        print(&quot;✅ MLP can work with non-normalized inputs!&quot;)&#10;        return True&#10;        &#10;    except Exception as e:&#10;        print(f&quot;❌ MLP failed with non-normalized inputs: {e}&quot;)&#10;        return False&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    test_mlp_with_unnormalized_inputs()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_ph_ensemble.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_ph_ensemble.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Test script for PhModel ensemble.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;from data.loader import load_data&#10;from models.ph_ensemble import PhModelEnsemble, PhEnsembleConfig&#10;import time&#10;&#10;&#10;def test_ph_model_ensemble():&#10;    &quot;&quot;&quot;Test the PhModel ensemble training and prediction.&quot;&quot;&quot;&#10;    print(&quot;Testing PhModel ensemble...&quot;)&#10;    &#10;    # Load data with proper normalization&#10;    print(&quot;\n1. Loading data...&quot;)&#10;    data_tensors, norm_params, df = load_data(&#10;        normalize_features=True,  # PhModel expects normalized inputs&#10;        normalize_targets=False   # Targets stay as decimals 0-1&#10;    )&#10;    print(f&quot;Data: {data_tensors.X.shape[0]} samples, {data_tensors.X.shape[1]} features&quot;)&#10;    &#10;    # Extract original ZLT statistics for PhModel&#10;    original_zlt = df['Zero_eps_thickness']&#10;    original_zlt_mean = original_zlt.mean()&#10;    original_zlt_std = original_zlt.std(ddof=0)&#10;    print(f&quot;Original ZLT stats: mean={original_zlt_mean:.2e}, std={original_zlt_std:.2e}&quot;)&#10;    &#10;    # Create small PhModel ensemble for testing&#10;    print(&quot;\n2. Creating PhModel ensemble...&quot;)&#10;    test_config = PhEnsembleConfig(&#10;        ensemble_size=3,          # Small for testing&#10;        hidden_dim=32,            # Smaller for faster training&#10;        dropout_rate=0.1,&#10;        learning_rate=0.001,&#10;        bootstrap_fraction=0.5,&#10;        current_target=200.0,&#10;        grid_size=50,             # Smaller grid for faster physics solving&#10;        voltage_bounds=(-1.25, 0.0)&#10;    )&#10;    ensemble = PhModelEnsemble(config=test_config, zlt_mu_stds=(original_zlt_mean, original_zlt_std))&#10;    print(f&quot;Created PhModel ensemble with {len(ensemble.models)} models&quot;)&#10;    &#10;    # Train ensemble&#10;    print(&quot;\n3. Training PhModel ensemble...&quot;)&#10;    start_time = time.time()&#10;    training_stats = ensemble.train(&#10;        data_tensors.X, &#10;        data_tensors.y, &#10;        num_epochs=20,    # Quick training for test&#10;        verbose=True&#10;    )&#10;    train_time = time.time() - start_time&#10;    print(f&quot;Training completed in {train_time:.1f} seconds&quot;)&#10;    &#10;    # Test predictions&#10;    print(&quot;\n4. Testing ensemble predictions...&quot;)&#10;    mean_pred, std_pred = ensemble.predict(data_tensors.X, return_std=True)&#10;    print(f&quot;Prediction shapes - Mean: {mean_pred.shape}, Std: {std_pred.shape}&quot;)&#10;    print(f&quot;Mean prediction range: [{mean_pred.min():.3f}, {mean_pred.max():.3f}]&quot;)&#10;    print(f&quot;Uncertainty range: [{std_pred.min():.3f}, {std_pred.max():.3f}]&quot;)&#10;    &#10;    # Verify bootstrapping worked (models should give different predictions)&#10;    print(&quot;\n5. Verifying bootstrap diversity...&quot;)&#10;    individual_preds = []&#10;    for i, model in enumerate(ensemble.models):&#10;        model.eval()&#10;        with torch.no_grad():&#10;            pred = model(data_tensors.X[:1])  # Single test point&#10;            individual_preds.append(pred)&#10;    &#10;    # Calculate variance across individual model predictions&#10;    individual_stack = torch.stack(individual_preds, dim=0)&#10;    model_variance = individual_stack.var(dim=0).mean().item()&#10;    print(f&quot;Variance across individual PhModels: {model_variance:.6f}&quot;)&#10;    if model_variance &gt; 1e-6:&#10;        print(&quot;✅ Bootstrap sampling created diverse PhModels&quot;)&#10;    else:&#10;        print(&quot;❌ PhModels are too similar - bootstrap may not be working&quot;)&#10;    &#10;    # Performance metrics&#10;    print(&quot;\n6. Performance evaluation...&quot;)&#10;    train_mse = torch.mean((mean_pred - data_tensors.y)**2).item()&#10;    print(f&quot;Training MSE: {train_mse:.6f}&quot;)&#10;    &#10;    # Show sample predictions&#10;    print(f&quot;\nSample predictions (first 3 points):&quot;)&#10;    for i in range(3):&#10;        true_eth, true_co = data_tensors.y[i]&#10;        pred_eth, pred_co = mean_pred[i]&#10;        std_eth, std_co = std_pred[i]&#10;        print(f&quot;  Point {i}: True=[{true_eth:.3f}, {true_co:.3f}], &quot;&#10;              f&quot;Pred=[{pred_eth:.3f}±{std_eth:.3f}, {pred_co:.3f}±{std_co:.3f}]&quot;)&#10;    &#10;    # Test BoTorch compatibility&#10;    print(&quot;\n7. Testing BoTorch compatibility...&quot;)&#10;    try:&#10;        samples_botorch = ensemble.get_prediction_samples_botorch(data_tensors.X[:3], n_samples=50)&#10;        print(f&quot;BoTorch sample format: {samples_botorch.shape} (n_samples, batch_size, 1, outputs) ✅&quot;)&#10;        &#10;        # Test posterior&#10;        posterior = ensemble.get_botorch_posterior(data_tensors.X[:3])&#10;        print(f&quot;BoTorch posterior created successfully ✅&quot;)&#10;        print(f&quot;Posterior mean shape: {posterior.mean.shape}&quot;)&#10;        &#10;    except ImportError as e:&#10;        print(f&quot;BoTorch not available: {e}&quot;)&#10;    except Exception as e:&#10;        print(f&quot;BoTorch integration error: {e}&quot;)&#10;    &#10;    print(&quot;\n✅ PhModel ensemble test completed!&quot;)&#10;    print(f&quot;Summary:&quot;)&#10;    print(f&quot;  - Ensemble size: {len(ensemble.models)} PhModels&quot;)&#10;    print(f&quot;  - Bootstrap sampling: ✓ (with replacement)&quot;)&#10;    print(f&quot;  - Physics-informed predictions: ✓&quot;)&#10;    print(f&quot;  - Uncertainty estimation: ✓ (std dev across ensemble)&quot;)&#10;    print(f&quot;  - BoTorch compatible: ✓ (sampling interface)&quot;)&#10;    print(f&quot;  - Training time: {train_time:.1f}s&quot;)&#10;    &#10;    return ensemble, train_mse, model_variance&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    try:&#10;        test_ph_model_ensemble()&#10;    except Exception as e:&#10;        print(f&quot;❌ Test failed with error: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_physics_model.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_physics_model.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Test script for Physics-Informed Model (PhModel).&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;from data.loader import load_data&#10;from models.physics_model import PhModel, PhysicsConfig&#10;&#10;&#10;def test_physics_model():&#10;    &quot;&quot;&quot;Test the Physics-Informed Model.&quot;&quot;&quot;&#10;    print(&quot;Testing Physics-Informed Model (PhModel)...&quot;)&#10;    &#10;    # Load data&#10;    print(&quot;\n1. Loading data...&quot;)&#10;    data_tensors, norm_params, df = load_data(&#10;        normalize_features=True,  # Features normalized&#10;        normalize_targets=False   # Targets stay as decimals 0-1&#10;    )&#10;    print(f&quot;Data: {data_tensors.X.shape[0]} samples, {data_tensors.X.shape[1]} features&quot;)&#10;    print(f&quot;Target range: [{data_tensors.y.min():.3f}, {data_tensors.y.max():.3f}]&quot;)&#10;    &#10;    # Create PhModel&#10;    print(&quot;\n2. Creating Physics-Informed Model...&quot;)&#10;    &#10;    # Calculate normalization stats for Zero_eps_thickness (column 3)&#10;    zlt_mean = norm_params.feature_means[3].item() if norm_params else 5e-6&#10;    zlt_std = norm_params.feature_stds[3].item() if norm_params else 1e-6&#10;    print(f&quot;Zero layer thickness normalization: mean={zlt_mean:.2e}, std={zlt_std:.2e}&quot;)&#10;    &#10;    config = PhysicsConfig(&#10;        hidden_dim=64,&#10;        dropout_rate=0.1,&#10;        current_target=200.0&#10;    )&#10;    &#10;    model = PhModel(config=config, zlt_mu_stds=(zlt_mean, zlt_std))&#10;    print(f&quot;PhModel created with {sum(p.numel() for p in model.parameters())} parameters&quot;)&#10;    &#10;    # Test forward pass&#10;    print(&quot;\n3. Testing forward pass...&quot;)&#10;    model.eval()&#10;    with torch.no_grad():&#10;        predictions = model(data_tensors.X[:5])  # Test with 5 samples&#10;    &#10;    print(f&quot;Prediction shape: {predictions.shape}&quot;)&#10;    print(f&quot;Prediction range: [{predictions.min():.3f}, {predictions.max():.3f}]&quot;)&#10;    print(f&quot;Sample predictions:&quot;)&#10;    for i in range(3):&#10;        true_eth, true_co = data_tensors.y[i]&#10;        pred_eth, pred_co = predictions[i]&#10;        print(f&quot;  Point {i}: True=[{true_eth:.3f}, {true_co:.3f}], Pred=[{pred_eth:.3f}, {pred_co:.3f}]&quot;)&#10;    &#10;    # Test physics parameter extraction&#10;    print(&quot;\n4. Testing physics parameter extraction...&quot;)&#10;    physics_params = model.get_physics_parameters(data_tensors.X[:3])&#10;    &#10;    print(&quot;Physics parameters for first 3 samples:&quot;)&#10;    for key, values in physics_params.items():&#10;        print(f&quot;  {key}: {values.squeeze().tolist()}&quot;)&#10;    &#10;    # Test training capability&#10;    print(&quot;\n5. Testing training capability...&quot;)&#10;    model.train()&#10;    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)&#10;    criterion = torch.nn.MSELoss()&#10;    &#10;    # Train for a few epochs to test gradient flow&#10;    initial_loss = None&#10;    for epoch in range(10):&#10;        optimizer.zero_grad()&#10;        predictions = model(data_tensors.X)&#10;        loss = criterion(predictions, data_tensors.y)&#10;        loss.backward()&#10;        optimizer.step()&#10;        &#10;        if epoch == 0:&#10;            initial_loss = loss.item()&#10;        if epoch % 5 == 0:&#10;            print(f&quot;  Epoch {epoch}: Loss = {loss.item():.6f}&quot;)&#10;    &#10;    final_loss = loss.item()&#10;    print(f&quot;Loss improved: {initial_loss:.6f} → {final_loss:.6f}&quot;)&#10;    &#10;    # Validate physics constraints&#10;    print(&quot;\n6. Validating physics constraints...&quot;)&#10;    model.eval()&#10;    with torch.no_grad():&#10;        test_predictions = model(data_tensors.X[:10])&#10;        params = model.get_physics_parameters(data_tensors.X[:10])&#10;    &#10;    # Check parameter ranges&#10;    constraints_valid = True&#10;    &#10;    # Porosity should be in (0, 1)&#10;    porosity = params['porosity']&#10;    if not (0 &lt; porosity.min() and porosity.max() &lt; 1):&#10;        print(f&quot;❌ Porosity out of bounds: [{porosity.min():.3f}, {porosity.max():.3f}]&quot;)&#10;        constraints_valid = False&#10;    else:&#10;        print(f&quot;✅ Porosity in bounds: [{porosity.min():.3f}, {porosity.max():.3f}]&quot;)&#10;    &#10;    # Pore radius should be positive and reasonable&#10;    pore_radius = params['pore_radius']&#10;    if not (1e-10 &lt; pore_radius.min() and pore_radius.max() &lt; 1e-6):&#10;        print(f&quot;❌ Pore radius unreasonable: [{pore_radius.min():.2e}, {pore_radius.max():.2e}]&quot;)&#10;        constraints_valid = False&#10;    else:&#10;        print(f&quot;✅ Pore radius reasonable: [{pore_radius.min():.2e}, {pore_radius.max():.2e}]&quot;)&#10;    &#10;    # Surface coverage fractions should sum to ≤ 1&#10;    theta_sum = params['theta_CO'] + params['theta_C2H4'] + params['theta_H2b']&#10;    if not (theta_sum.max() &lt;= 1.01):  # Small tolerance&#10;        print(f&quot;❌ Surface coverage sum &gt; 1: max = {theta_sum.max():.3f}&quot;)&#10;        constraints_valid = False&#10;    else:&#10;        print(f&quot;✅ Surface coverage sum ≤ 1: max = {theta_sum.max():.3f}&quot;)&#10;    &#10;    # Predictions should be in [0, 1]&#10;    if not (0 &lt;= test_predictions.min() and test_predictions.max() &lt;= 1):&#10;        print(f&quot;❌ Predictions out of [0,1]: [{test_predictions.min():.3f}, {test_predictions.max():.3f}]&quot;)&#10;        constraints_valid = False&#10;    else:&#10;        print(f&quot;✅ Predictions in [0,1]: [{test_predictions.min():.3f}, {test_predictions.max():.3f}]&quot;)&#10;    &#10;    if constraints_valid:&#10;        print(&quot;\n✅ All physics constraints satisfied!&quot;)&#10;    else:&#10;        print(&quot;\n❌ Some physics constraints violated!&quot;)&#10;    &#10;    print(&quot;\n✅ PhModel test completed!&quot;)&#10;    print(f&quot;Summary:&quot;)&#10;    print(f&quot;  - Forward pass: ✓&quot;)&#10;    print(f&quot;  - Parameter extraction: ✓&quot;) &#10;    print(f&quot;  - Gradient flow: ✓&quot;)&#10;    print(f&quot;  - Physics constraints: {'✓' if constraints_valid else '❌'}&quot;)&#10;    &#10;    return model, constraints_valid&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    try:&#10;        test_physics_model()&#10;    except Exception as e:&#10;        print(f&quot;❌ Test failed with error: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_physics_model_unnormalized.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_physics_model_unnormalized.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Test PhModel with non-normalized inputs to check robustness.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;from data.loader import load_data&#10;from models.physics_model import PhModel, PhysicsConfig&#10;&#10;&#10;def test_physics_model_unnormalized():&#10;    &quot;&quot;&quot;Test if PhModel can work with non-normalized inputs.&quot;&quot;&quot;&#10;    print(&quot;Testing PhModel with non-normalized inputs...&quot;)&#10;    &#10;    # Load data without normalization&#10;    print(&quot;\n1. Loading data without normalization...&quot;)&#10;    data_tensors_norm, norm_params_norm, df_norm = load_data(&#10;        normalize_features=True,   # Normalized version for comparison&#10;        normalize_targets=False&#10;    )&#10;    &#10;    data_tensors_raw, norm_params_raw, df_raw = load_data(&#10;        normalize_features=False,  # Non-normalized version to test&#10;        normalize_targets=False&#10;    )&#10;    &#10;    print(f&quot;Normalized X range: [{data_tensors_norm.X.min():.3f}, {data_tensors_norm.X.max():.3f}]&quot;)&#10;    print(f&quot;Raw X range: [{data_tensors_raw.X.min():.3f}, {data_tensors_raw.X.max():.3f}]&quot;)&#10;    print(f&quot;Sample raw X values:\n{data_tensors_raw.X[:2]}&quot;)&#10;    &#10;    # Test PhModel with NORMALIZED inputs (should work)&#10;    print(&quot;\n2. Testing PhModel with normalized inputs...&quot;)&#10;    zlt_mean_norm = norm_params_norm.feature_means[3].item()&#10;    zlt_std_norm = norm_params_norm.feature_stds[3].item()&#10;    &#10;    config = PhysicsConfig(hidden_dim=32, dropout_rate=0.1)&#10;    model_norm = PhModel(config=config, zlt_mu_stds=(zlt_mean_norm, zlt_std_norm))&#10;    &#10;    model_norm.eval()&#10;    with torch.no_grad():&#10;        pred_norm = model_norm(data_tensors_norm.X[:5])&#10;        physics_params_norm = model_norm.get_physics_parameters(data_tensors_norm.X[:5])&#10;    &#10;    print(f&quot;Normalized input predictions: {pred_norm.shape}&quot;)&#10;    print(f&quot;Sample normalized prediction: {pred_norm[0]}&quot;)&#10;    print(f&quot;Porosity range (normalized): [{physics_params_norm['porosity'].min():.3f}, {physics_params_norm['porosity'].max():.3f}]&quot;)&#10;    &#10;    # Test PhModel with RAW inputs (the main test)&#10;    print(&quot;\n3. Testing PhModel with RAW inputs...&quot;)&#10;    &#10;    # For raw inputs, we need to use raw normalization stats for Zero_eps_thickness&#10;    # Or better yet, use the raw values directly&#10;    raw_zlt_mean = data_tensors_raw.X[:, 3].mean().item()  # Direct calculation from raw data&#10;    raw_zlt_std = data_tensors_raw.X[:, 3].std().item()&#10;    &#10;    print(f&quot;Raw ZLT stats: mean={raw_zlt_mean:.2e}, std={raw_zlt_std:.2e}&quot;)&#10;    &#10;    model_raw = PhModel(config=config, zlt_mu_stds=(raw_zlt_mean, raw_zlt_std))&#10;    &#10;    try:&#10;        model_raw.eval()&#10;        with torch.no_grad():&#10;            pred_raw = model_raw(data_tensors_raw.X[:5])&#10;            physics_params_raw = model_raw.get_physics_parameters(data_tensors_raw.X[:5])&#10;        &#10;        print(f&quot;Raw input predictions: {pred_raw.shape}&quot;)&#10;        print(f&quot;Sample raw prediction: {pred_raw[0]}&quot;)&#10;        print(f&quot;Prediction range: [{pred_raw.min():.3f}, {pred_raw.max():.3f}]&quot;)&#10;        &#10;        # Check if physics constraints are still satisfied&#10;        print(&quot;\n4. Validating physics constraints with raw inputs...&quot;)&#10;        &#10;        porosity_raw = physics_params_raw['porosity']&#10;        pore_radius_raw = physics_params_raw['pore_radius']&#10;        theta_sum_raw = (physics_params_raw['theta_CO'] + &#10;                        physics_params_raw['theta_C2H4'] + &#10;                        physics_params_raw['theta_H2b'])&#10;        &#10;        constraints_ok = True&#10;        &#10;        # Check porosity bounds&#10;        if not (0 &lt; porosity_raw.min() and porosity_raw.max() &lt; 1):&#10;            print(f&quot;❌ Porosity out of bounds: [{porosity_raw.min():.3f}, {porosity_raw.max():.3f}]&quot;)&#10;            constraints_ok = False&#10;        else:&#10;            print(f&quot;✅ Porosity in bounds: [{porosity_raw.min():.3f}, {porosity_raw.max():.3f}]&quot;)&#10;        &#10;        # Check pore radius&#10;        if not (1e-10 &lt; pore_radius_raw.min() and pore_radius_raw.max() &lt; 1e-6):&#10;            print(f&quot;❌ Pore radius unreasonable: [{pore_radius_raw.min():.2e}, {pore_radius_raw.max():.2e}]&quot;)&#10;            constraints_ok = False&#10;        else:&#10;            print(f&quot;✅ Pore radius reasonable: [{pore_radius_raw.min():.2e}, {pore_radius_raw.max():.2e}]&quot;)&#10;        &#10;        # Check surface coverage sum&#10;        if not (theta_sum_raw.max() &lt;= 1.01):&#10;            print(f&quot;❌ Surface coverage sum &gt; 1: max = {theta_sum_raw.max():.3f}&quot;)&#10;            constraints_ok = False&#10;        else:&#10;            print(f&quot;✅ Surface coverage sum ≤ 1: max = {theta_sum_raw.max():.3f}&quot;)&#10;        &#10;        # Check prediction bounds&#10;        if not (0 &lt;= pred_raw.min() and pred_raw.max() &lt;= 1):&#10;            print(f&quot;❌ Predictions out of [0,1]: [{pred_raw.min():.3f}, {pred_raw.max():.3f}]&quot;)&#10;            constraints_ok = False&#10;        else:&#10;            print(f&quot;✅ Predictions in [0,1]: [{pred_raw.min():.3f}, {pred_raw.max():.3f}]&quot;)&#10;        &#10;        if constraints_ok:&#10;            print(&quot;\n✅ PhModel works with non-normalized inputs!&quot;)&#10;            print(&quot;Physics constraints are maintained even with raw input scales.&quot;)&#10;        else:&#10;            print(&quot;\n❌ PhModel struggles with non-normalized inputs - constraints violated&quot;)&#10;        &#10;        # Compare predictions&#10;        print(&quot;\n5. Comparing normalized vs raw input predictions...&quot;)&#10;        print(f&quot;Normalized pred sample: {pred_norm[0]}&quot;)&#10;        print(f&quot;Raw pred sample:        {pred_raw[0]}&quot;)&#10;        pred_diff = torch.abs(pred_norm[0] - pred_raw[0]).max()&#10;        print(f&quot;Max prediction difference: {pred_diff:.3f}&quot;)&#10;        &#10;        return True, constraints_ok&#10;        &#10;    except Exception as e:&#10;        print(f&quot;❌ PhModel failed with raw inputs: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()&#10;        return False, False&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    test_physics_model_unnormalized()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_triplet_handling.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_triplet_handling.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Test to verify triplet data handling matches the old implementation.&#10;&quot;&quot;&quot;&#10;import sys&#10;from pathlib import Path&#10;sys.path.append(str(Path(__file__).parent))&#10;&#10;import torch&#10;import numpy as np&#10;import pandas as pd&#10;from data.loader import load_raw_data, load_data&#10;&#10;&#10;def test_triplet_handling():&#10;    &quot;&quot;&quot;Test if triplet data structure is handled correctly.&quot;&quot;&quot;&#10;    print(&quot;Testing triplet data handling...&quot;)&#10;    &#10;    # Load raw data with our new loader&#10;    print(&quot;\n1. Testing our new data loader...&quot;)&#10;    df_new = load_raw_data()&#10;    print(f&quot;New loader data shape: {df_new.shape}&quot;)&#10;    print(f&quot;First few rows:\n{df_new.head()}&quot;)&#10;    &#10;    # Manually replicate old triplet logic to compare&#10;    print(&quot;\n2. Manually replicating old triplet logic...&quot;)&#10;    &#10;    # Load raw Excel data (before triplet shuffling)&#10;    raw_df = pd.read_excel('Characterization_data.xlsx', skiprows=[1], index_col=0)&#10;    raw_df = raw_df[['AgCu Ratio', 'Naf vol (ul)', 'Sust vol (ul)', 'Catalyst mass loading', 'FE (Eth)', 'FE (CO)']]&#10;    raw_df = raw_df.sort_values(by=['AgCu Ratio', 'Naf vol (ul)'])&#10;    raw_df = raw_df.dropna()&#10;    &#10;    # Convert FE percentages to fractions&#10;    raw_df['FE (CO)'] = raw_df['FE (CO)'] / 100&#10;    raw_df['FE (Eth)'] = raw_df['FE (Eth)'] / 100&#10;    &#10;    # Add thickness calculation&#10;    dens_Ag = 10490&#10;    dens_Cu = 8935&#10;    dens_avg = (1 - raw_df['AgCu Ratio']) * dens_Cu + raw_df['AgCu Ratio'] * dens_Ag&#10;    mass = raw_df['Catalyst mass loading'] * 1e-6&#10;    area = 1.85**2&#10;    A = area * 1e-4&#10;    thickness = (mass / dens_avg) / A&#10;    raw_df.insert(3, column='Zero_eps_thickness', value=thickness)&#10;    &#10;    print(f&quot;Before triplet shuffling: {raw_df.shape}&quot;)&#10;    print(&quot;Sample of grouped data (first 9 rows - should be 3 triplets):&quot;)&#10;    print(raw_df.head(9)[['AgCu Ratio', 'Naf vol (ul)', 'Sust vol (ul)']])&#10;    &#10;    # Apply triplet shuffling (OLD METHOD)&#10;    raw_df['triplet'] = np.arange(len(raw_df)) // 3&#10;    gen = np.random.default_rng(2)  # Same seed as our loader&#10;    order = gen.permutation(30)&#10;    &#10;    old_method_df = pd.DataFrame()&#10;    for i in order:&#10;        old_method_df = pd.concat([old_method_df, raw_df[raw_df['triplet'] == i]])&#10;    old_method_df.reset_index(drop=True, inplace=True)&#10;    old_method_df = old_method_df.drop(columns=['triplet'])&#10;    &#10;    print(f&quot;\nAfter old method triplet shuffling: {old_method_df.shape}&quot;)&#10;    print(&quot;First few rows after old method:&quot;)&#10;    print(old_method_df.head(3)[['AgCu Ratio', 'Naf vol (ul)', 'Sust vol (ul)']])&#10;    &#10;    # Compare with our new loader&#10;    print(f&quot;\nFirst few rows from our new loader:&quot;)&#10;    print(df_new.head(3)[['AgCu Ratio', 'Naf vol (ul)', 'Sust vol (ul)']])&#10;    &#10;    # Check if they match&#10;    columns_to_compare = ['AgCu Ratio', 'Naf vol (ul)', 'Sust vol (ul)', 'Zero_eps_thickness', 'Catalyst mass loading', 'FE (Eth)', 'FE (CO)']&#10;    match = old_method_df[columns_to_compare].equals(df_new[columns_to_compare])&#10;    &#10;    print(f&quot;\n3. Comparison result:&quot;)&#10;    print(f&quot;Data matches old method: {match}&quot;)&#10;    &#10;    if match:&#10;        print(&quot;✅ Triplet handling is correct!&quot;)&#10;    else:&#10;        print(&quot;❌ Triplet handling differs from old method&quot;)&#10;        print(&quot;Checking differences...&quot;)&#10;        for col in columns_to_compare:&#10;            col_match = old_method_df[col].equals(df_new[col])&#10;            print(f&quot;  {col}: {col_match}&quot;)&#10;    &#10;    return match&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    test_triplet_handling()" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>